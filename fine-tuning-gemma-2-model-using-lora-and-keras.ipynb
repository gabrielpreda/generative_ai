{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras?scriptVersionId=195844539\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"1cab7fa7","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.011826,"end_time":"2024-09-08T19:02:38.24943","exception":false,"start_time":"2024-09-08T19:02:38.237604","status":"completed"},"tags":[]},"source":["<center><h1>Fine-tuning Gemma 2 model using LoRA and Keras</h1></center>\n","\n","<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n","\n","\n","# Introduction\n","\n","This notebook will demonstrate three things:\n","\n","1. How to fine-tune Gemma model using LoRA\n","2. Creation of a specialised class to query about Kaggle features\n","3. Some results of querying about Kaggle Docs\n","\n","This work is largely based on previous work. Here I list the sources:\n","\n","1. Gemma 2 Model Card, Kaggle Models,https://www.kaggle.com/models/google/gemma-2/\n","2. Kaggle QA with Gemma - KerasNLP Starter, Kaggle Code, https://www.kaggle.com/code/awsaf49/kaggle-qa-with-gemma-kerasnlp-starter (Version 11)  \n","3. Fine-tune Gemma models in Keras using LoRA, Kaggle Code, https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora (Version 1) \n","4. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, LoRA: Low-Rank Adaptation of Large Language Models, ArXiv, https://arxiv.org/pdf/2106.09685.pdf\n","5. Abheesht Sharma, Matthew Watson, Parameter-efficient fine-tuning of GPT-2 with LoRA, https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/\n","6. Keras 3 API documentation / KerasNLP / Models / Gemma, https://keras.io/api/keras_nlp/models/gemma/\n","7. Unlock the Power of Gemma 2: Prompt it like a Pro, https://www.kaggle.com/code/gpreda/unlock-the-power-of-gemma-2-prompt-it-like-a-pro  \n","8. Fine-tune Gemma using LoRA and Keras, https://www.kaggle.com/code/gpreda/fine-tune-gemma-using-lora-and-keras\n","9. Fine-tunning Gemma model with Kaggle Docs data, https://www.kaggle.com/code/gpreda/fine-tunning-gemma-model-with-kaggle-docs-data\n","10. Kaggle Docs, Kaggle Dataset, https://www.kaggle.com/datasets/awsaf49/kaggle-docs  \n","\n","\n","**Let's go**!\n"]},{"cell_type":"markdown","id":"d54f3ae0","metadata":{"papermill":{"duration":0.010988,"end_time":"2024-09-08T19:02:38.271901","exception":false,"start_time":"2024-09-08T19:02:38.260913","status":"completed"},"tags":[]},"source":["# What is Gemma 2?\n","\n","Gemma is a collection of lightweight, advanced open models developed by Google, leveraging the same research and technology behind the Gemini models. These models are text-to-text, decoder-only large language models available in English, with open weights provided for both pre-trained and instruction-tuned versions. Gemma models excel in a range of text generation tasks, such as question answering, summarization, and reasoning. Their compact size allows for deployment in resource-constrained environments like laptops, desktops, or personal cloud infrastructure, making state-of-the-art AI models more accessible and encouraging innovation for all. \n","\n","Gemma 2 represent the 2nd generation of Gemma models. These models were trained on a dataset of text data that includes a wide variety of sources. The **27B** model was trained with **13 trillion** tokens, the **9B** model was trained with **8 trillion tokens**, and **2B** model was trained with **2 trillion** tokens. Here is a summary of their key components: \n","* **Web Documents**: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content.\n","* **Code**: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions.\n","* **Mathematics**: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.\n","\n","To learn more about Gemma 2, follow this link: [Gemma 2 Model Card](https://www.kaggle.com/models/google/gemma-2).\n","\n","\n"]},{"cell_type":"markdown","id":"1dc5ff79","metadata":{"papermill":{"duration":0.011215,"end_time":"2024-09-08T19:02:38.294702","exception":false,"start_time":"2024-09-08T19:02:38.283487","status":"completed"},"tags":[]},"source":["# What is LoRA?  \n","\n","**LoRA** stands for **Low-Rank Adaptation**. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to **LoRA** paper, this number decreases **10,000 times**, and the computational resources size decreases 3 times. "]},{"cell_type":"markdown","id":"e5be045e","metadata":{"papermill":{"duration":0.011119,"end_time":"2024-09-08T19:02:38.318243","exception":false,"start_time":"2024-09-08T19:02:38.307124","status":"completed"},"tags":[]},"source":["# How we proceed?\n","\n","For fine-tunning with LoRA, we will follow the steps:\n","\n","1. Install prerequisites\n","2. Load and process the data for fine-tuning\n","3. Initialize the code for Gemma causal language model (Gemma Causal LM)\n","4. Perform fine-tuning\n","5. Test the fine-tunned model with questions from the data used for fine-tuning and with aditional questions"]},{"cell_type":"markdown","id":"89d83620","metadata":{"papermill":{"duration":0.010938,"end_time":"2024-09-08T19:02:38.340341","exception":false,"start_time":"2024-09-08T19:02:38.329403","status":"completed"},"tags":[]},"source":["# Prerequisites\n","\n","\n","## Install packages\n","\n","We start by installing `keras-nlp` and `keras` packages."]},{"cell_type":"code","execution_count":1,"id":"1f657af2","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-09-08T19:02:38.364189Z","iopub.status.busy":"2024-09-08T19:02:38.363812Z","iopub.status.idle":"2024-09-08T19:03:21.658112Z","shell.execute_reply":"2024-09-08T19:03:21.656919Z"},"papermill":{"duration":43.308914,"end_time":"2024-09-08T19:03:21.660418","exception":false,"start_time":"2024-09-08T19:02:38.351504","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\r\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.5.0 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","keras-cv 0.8.2 requires keras-core, which is not installed.\u001b[0m\u001b[31m\r\n","\u001b[0m"]}],"source":["# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n","!pip install -q -U keras-nlp\n","!pip install -q -U keras>=3\n","!pip install -q -U kagglehub --upgrade"]},{"cell_type":"markdown","id":"45f3045e","metadata":{"papermill":{"duration":0.011377,"end_time":"2024-09-08T19:03:21.683798","exception":false,"start_time":"2024-09-08T19:03:21.672421","status":"completed"},"tags":[]},"source":["## Import packages\n","\n","Now we can import the packages we just installed. We will also install `os`, so that we can set the environment variables needed for keras backend. We will use `jax` as `KERAS_BACKEND`.\n","\n","Because we want to publish the Model from the Notebook, we also include `kagglehub` and import secrets from `Kaggle App`."]},{"cell_type":"code","execution_count":2,"id":"da4b621a","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-09-08T19:03:21.708467Z","iopub.status.busy":"2024-09-08T19:03:21.708135Z","iopub.status.idle":"2024-09-08T19:03:36.831241Z","shell.execute_reply":"2024-09-08T19:03:36.830252Z"},"papermill":{"duration":15.138394,"end_time":"2024-09-08T19:03:36.833744","exception":false,"start_time":"2024-09-08T19:03:21.69535","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-09-08 19:03:26.117528: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-09-08 19:03:26.117650: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-09-08 19:03:26.264307: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import os\n","os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n","os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n","os.environ[\"JAX_PLATFORMS\"] = \"\"\n","import keras\n","import keras_nlp\n","import kagglehub\n","\n","\n","from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","os.environ[\"KAGGLE_USERNAME\"] = user_secrets.get_secret(\"kaggle_username\")\n","os.environ[\"KAGGLE_KEY\"] = user_secrets.get_secret(\"kaggle_key\")\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","tqdm.pandas() # progress bar for pandas\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from IPython.display import display, Markdown"]},{"cell_type":"markdown","id":"91a761ec","metadata":{"papermill":{"duration":0.01144,"end_time":"2024-09-08T19:03:36.857313","exception":false,"start_time":"2024-09-08T19:03:36.845873","status":"completed"},"tags":[]},"source":["## Configurations\n","\n","\n","We use a `Config` class to group the information needed to control the fine-tuning process:\n","* random seed \n","* dataset path\n","* preset - name of pretrained Gemma 2\n","* sequence length - this is the maximum size of input sequence for training\n","* batch size - size of the input batch in training, x 2 as two GPUs\n","* lora rank - rank for LoRA, higher means more trainable parameters \n","* learning rate used in the train\n","* epochs - number of epochs for train"]},{"cell_type":"code","execution_count":3,"id":"cd916449","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:03:36.882165Z","iopub.status.busy":"2024-09-08T19:03:36.881279Z","iopub.status.idle":"2024-09-08T19:03:36.886618Z","shell.execute_reply":"2024-09-08T19:03:36.885736Z"},"papermill":{"duration":0.019805,"end_time":"2024-09-08T19:03:36.888572","exception":false,"start_time":"2024-09-08T19:03:36.868767","status":"completed"},"tags":[]},"outputs":[],"source":["class Config:\n","    seed = 42\n","    dataset_path = \"/kaggle/input/kaggle-docs/questions_answers\"\n","    preset = \"gemma2_2b_en\" # name of pretrained Gemma 2\n","    sequence_length = 512 # max size of input sequence for training\n","    batch_size = 1 # size of the input batch in training\n","    lora_rank = 4 # rank for LoRA, higher means more trainable parameters\n","    learning_rate=8e-5 # learning rate used in train\n","    epochs = 10 # number of epochs to train"]},{"cell_type":"markdown","id":"c40a05bf","metadata":{"papermill":{"duration":0.01129,"end_time":"2024-09-08T19:03:36.911415","exception":false,"start_time":"2024-09-08T19:03:36.900125","status":"completed"},"tags":[]},"source":["Set a random seed for results reproducibility."]},{"cell_type":"code","execution_count":4,"id":"eec69bf7","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:03:36.935742Z","iopub.status.busy":"2024-09-08T19:03:36.935478Z","iopub.status.idle":"2024-09-08T19:03:36.939811Z","shell.execute_reply":"2024-09-08T19:03:36.938978Z"},"papermill":{"duration":0.018723,"end_time":"2024-09-08T19:03:36.94167","exception":false,"start_time":"2024-09-08T19:03:36.922947","status":"completed"},"tags":[]},"outputs":[],"source":["keras.utils.set_random_seed(Config.seed)"]},{"cell_type":"markdown","id":"c8f57627","metadata":{"papermill":{"duration":0.011284,"end_time":"2024-09-08T19:03:36.964427","exception":false,"start_time":"2024-09-08T19:03:36.953143","status":"completed"},"tags":[]},"source":["# Load the data\n","\n","\n","We load the data we will use for fine-tunining."]},{"cell_type":"code","execution_count":5,"id":"08354e2c","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:03:36.988658Z","iopub.status.busy":"2024-09-08T19:03:36.988117Z","iopub.status.idle":"2024-09-08T19:03:37.031138Z","shell.execute_reply":"2024-09-08T19:03:37.030296Z"},"papermill":{"duration":0.05737,"end_time":"2024-09-08T19:03:37.033234","exception":false,"start_time":"2024-09-08T19:03:36.975864","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","      <th>Category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What are the different types of competitions a...</td>\n","      <td># Types of Competitions\\n\\nKaggle Competitions...</td>\n","      <td>competition</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What are the different competition formats on ...</td>\n","      <td>There are handful of different formats competi...</td>\n","      <td>competition</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>How to join a competition?</td>\n","      <td>Before you start, navigate to the [Competition...</td>\n","      <td>competition</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>How to form, manage, and disband teams in a co...</td>\n","      <td>Everyone that competes in a Competition does s...</td>\n","      <td>competition</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>How do I make a submission in a competition?</td>\n","      <td>You will need to submit your model predictions...</td>\n","      <td>competition</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Question  \\\n","0  What are the different types of competitions a...   \n","1  What are the different competition formats on ...   \n","2                         How to join a competition?   \n","3  How to form, manage, and disband teams in a co...   \n","4       How do I make a submission in a competition?   \n","\n","                                              Answer     Category  \n","0  # Types of Competitions\\n\\nKaggle Competitions...  competition  \n","1  There are handful of different formats competi...  competition  \n","2  Before you start, navigate to the [Competition...  competition  \n","3  Everyone that competes in a Competition does s...  competition  \n","4  You will need to submit your model predictions...  competition  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(f\"{Config.dataset_path}/data.csv\")\n","df.head()"]},{"cell_type":"markdown","id":"1f37018d","metadata":{"papermill":{"duration":0.015876,"end_time":"2024-09-08T19:03:37.061689","exception":false,"start_time":"2024-09-08T19:03:37.045813","status":"completed"},"tags":[]},"source":["Let's check the total number of rows in this dataset."]},{"cell_type":"code","execution_count":6,"id":"b36c6c71","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:03:37.095749Z","iopub.status.busy":"2024-09-08T19:03:37.094986Z","iopub.status.idle":"2024-09-08T19:03:37.100813Z","shell.execute_reply":"2024-09-08T19:03:37.099911Z"},"papermill":{"duration":0.021463,"end_time":"2024-09-08T19:03:37.10281","exception":false,"start_time":"2024-09-08T19:03:37.081347","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["60"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df.shape[0]"]},{"cell_type":"markdown","id":"271dcd84","metadata":{"papermill":{"duration":0.011832,"end_time":"2024-09-08T19:03:37.126735","exception":false,"start_time":"2024-09-08T19:03:37.114903","status":"completed"},"tags":[]},"source":["For easiness, we will create the following template for QA: "]},{"cell_type":"code","execution_count":7,"id":"964247ab","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:03:37.15209Z","iopub.status.busy":"2024-09-08T19:03:37.151524Z","iopub.status.idle":"2024-09-08T19:03:37.160952Z","shell.execute_reply":"2024-09-08T19:03:37.160116Z"},"papermill":{"duration":0.024118,"end_time":"2024-09-08T19:03:37.162923","exception":false,"start_time":"2024-09-08T19:03:37.138805","status":"completed"},"tags":[]},"outputs":[],"source":["template = \"\\n\\nCategory:\\nkaggle-{Category}\\n\\nQuestion:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\n","df[\"prompt\"] = df.apply(lambda row: template.format(Category=row.Category,\n","                                                             Question=row.Question,\n","                                                             Answer=row.Answer), axis=1)\n","data = df.prompt.tolist()"]},{"cell_type":"markdown","id":"b50223cc","metadata":{"papermill":{"duration":0.011607,"end_time":"2024-09-08T19:03:37.186342","exception":false,"start_time":"2024-09-08T19:03:37.174735","status":"completed"},"tags":[]},"source":["## Template utility function"]},{"cell_type":"code","execution_count":8,"id":"54525b80","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:03:37.211283Z","iopub.status.busy":"2024-09-08T19:03:37.21103Z","iopub.status.idle":"2024-09-08T19:03:37.215691Z","shell.execute_reply":"2024-09-08T19:03:37.214879Z"},"papermill":{"duration":0.019312,"end_time":"2024-09-08T19:03:37.217583","exception":false,"start_time":"2024-09-08T19:03:37.198271","status":"completed"},"tags":[]},"outputs":[],"source":["def colorize_text(text):\n","    for word, color in zip([\"Category\", \"Question\", \"Answer\"], [\"blue\", \"red\", \"green\"]):\n","        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n","    return text"]},{"cell_type":"markdown","id":"df8028fb","metadata":{"papermill":{"duration":0.011729,"end_time":"2024-09-08T19:03:37.241478","exception":false,"start_time":"2024-09-08T19:03:37.229749","status":"completed"},"tags":[]},"source":["# Specialized class to query Gemma\n","\n","\n","We define a specialized class to query Gemma. But first, we need to initialize an object of GemmaCausalLM class."]},{"cell_type":"markdown","id":"a7f02e79","metadata":{"papermill":{"duration":0.012167,"end_time":"2024-09-08T19:03:37.307658","exception":false,"start_time":"2024-09-08T19:03:37.295491","status":"completed"},"tags":[]},"source":["## Initialize the code for Gemma Causal LM"]},{"cell_type":"code","execution_count":9,"id":"f4f7e76f","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:03:37.335144Z","iopub.status.busy":"2024-09-08T19:03:37.334264Z","iopub.status.idle":"2024-09-08T19:05:06.927732Z","shell.execute_reply":"2024-09-08T19:05:06.926807Z"},"papermill":{"duration":89.608853,"end_time":"2024-09-08T19:05:06.929773","exception":false,"start_time":"2024-09-08T19:03:37.32092","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n","└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["gemma_causal_lm = keras_nlp.models.GemmaCausalLM.from_preset(Config.preset)\n","gemma_causal_lm.summary()"]},{"cell_type":"markdown","id":"8e7c7705","metadata":{"papermill":{"duration":0.013201,"end_time":"2024-09-08T19:05:06.956653","exception":false,"start_time":"2024-09-08T19:05:06.943452","status":"completed"},"tags":[]},"source":["## Define the specialized class\n","\n","Here we define the special class `GemmaQA`. \n","in the `__init__` we pass the `GemmaCausalLM` object created before.\n","The `query` member function uses `GemmaCausalLM` member function `generate` to generate the answer, based on a prompt that includes the category and the question."]},{"cell_type":"code","execution_count":10,"id":"2ac0eaa5","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:05:06.984598Z","iopub.status.busy":"2024-09-08T19:05:06.984241Z","iopub.status.idle":"2024-09-08T19:05:06.99047Z","shell.execute_reply":"2024-09-08T19:05:06.989452Z"},"papermill":{"duration":0.022563,"end_time":"2024-09-08T19:05:06.992425","exception":false,"start_time":"2024-09-08T19:05:06.969862","status":"completed"},"tags":[]},"outputs":[],"source":["class GemmaQA:\n","    def __init__(self, max_length=512):\n","        self.max_length = max_length\n","        self.prompt = template\n","        self.gemma_causal_lm = gemma_causal_lm\n","        \n","    def query(self, category, question):\n","        response = self.gemma_causal_lm.generate(\n","            self.prompt.format(\n","                Category=category,\n","                Question=question,\n","                Answer=\"\"), \n","            max_length=self.max_length)\n","        display(Markdown(colorize_text(response)))\n","        "]},{"cell_type":"markdown","id":"96020494","metadata":{"papermill":{"duration":0.013316,"end_time":"2024-09-08T19:05:07.019141","exception":false,"start_time":"2024-09-08T19:05:07.005825","status":"completed"},"tags":[]},"source":["## Gemma preprocessor\n","\n","\n","This preprocessing layer will take in batches of strings, and return outputs in a ```(x, y, sample_weight)``` format, where the y label is the next token id in the x sequence.\n","\n","From the code below, we can see that, after the preprocessor, the data shape is ```(num_samples, sequence_length)```."]},{"cell_type":"code","execution_count":11,"id":"660aa081","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:05:07.047197Z","iopub.status.busy":"2024-09-08T19:05:07.046869Z","iopub.status.idle":"2024-09-08T19:05:07.387076Z","shell.execute_reply":"2024-09-08T19:05:07.386223Z"},"papermill":{"duration":0.356818,"end_time":"2024-09-08T19:05:07.389399","exception":false,"start_time":"2024-09-08T19:05:07.032581","status":"completed"},"tags":[]},"outputs":[],"source":["x, y, sample_weight = gemma_causal_lm.preprocessor(data[0:2])"]},{"cell_type":"code","execution_count":12,"id":"65ecfcdf","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:05:07.417974Z","iopub.status.busy":"2024-09-08T19:05:07.417629Z","iopub.status.idle":"2024-09-08T19:05:07.423839Z","shell.execute_reply":"2024-09-08T19:05:07.423004Z"},"papermill":{"duration":0.022593,"end_time":"2024-09-08T19:05:07.425786","exception":false,"start_time":"2024-09-08T19:05:07.403193","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["{'token_ids': Array([[   2,  109, 8606, ...,    0,    0,    0],\n","       [   2,  109, 8606, ...,    0,    0,    0]], dtype=int32), 'padding_mask': Array([[ True,  True,  True, ..., False, False, False],\n","       [ True,  True,  True, ..., False, False, False]], dtype=bool)} [[   109   8606 235292 ...      0      0      0]\n"," [   109   8606 235292 ...      0      0      0]]\n"]}],"source":["print(x, y)"]},{"cell_type":"markdown","id":"bc88571c","metadata":{"papermill":{"duration":0.013338,"end_time":"2024-09-08T19:05:07.452537","exception":false,"start_time":"2024-09-08T19:05:07.439199","status":"completed"},"tags":[]},"source":["# Perform fine-tuning with LoRA"]},{"cell_type":"markdown","id":"639d1f48","metadata":{"papermill":{"duration":0.013158,"end_time":"2024-09-08T19:05:07.479145","exception":false,"start_time":"2024-09-08T19:05:07.465987","status":"completed"},"tags":[]},"source":["## Enable LoRA for the model\n","\n","LoRA rank is setting the number of trainable parameters. A larger rank will result in a larger number of parameters to train."]},{"cell_type":"code","execution_count":13,"id":"59cddfe7","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:05:07.50711Z","iopub.status.busy":"2024-09-08T19:05:07.506784Z","iopub.status.idle":"2024-09-08T19:05:08.231784Z","shell.execute_reply":"2024-09-08T19:05:08.230885Z"},"papermill":{"duration":0.741284,"end_time":"2024-09-08T19:05:08.233786","exception":false,"start_time":"2024-09-08T19:05:07.492502","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n","└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,617,270,528\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> (9.75 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,617,270,528\u001b[0m (9.75 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,928,640</span> (11.17 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,928,640\u001b[0m (11.17 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"]},"metadata":{},"output_type":"display_data"}],"source":["# Enable LoRA for the model and set the LoRA rank to the lora_rank as set in Config (4).\n","gemma_causal_lm.backbone.enable_lora(rank=Config.lora_rank)\n","gemma_causal_lm.summary()"]},{"cell_type":"markdown","id":"b5381763","metadata":{"papermill":{"duration":0.01452,"end_time":"2024-09-08T19:05:08.263408","exception":false,"start_time":"2024-09-08T19:05:08.248888","status":"completed"},"tags":[]},"source":["We see that only a small part of the parameters are trainable. 2.6 billions parameters total, and only 2.9 Millions parameters trainable."]},{"cell_type":"markdown","id":"92fcdec6","metadata":{"papermill":{"duration":0.01466,"end_time":"2024-09-08T19:05:08.292935","exception":false,"start_time":"2024-09-08T19:05:08.278275","status":"completed"},"tags":[]},"source":["## Run the training sequence\n","\n","We set the `sequence_length` for the `GemmaCausalLM` (from configuration, will be 512).\n","We compile the model, with the loss, optimizer and metric.\n","For the metric, it is used `SparseCategoricalAccuracy`. This metric calculates how often predictions match integer labels."]},{"cell_type":"code","execution_count":14,"id":"10735960","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:05:08.324399Z","iopub.status.busy":"2024-09-08T19:05:08.323692Z","iopub.status.idle":"2024-09-08T19:14:12.32856Z","shell.execute_reply":"2024-09-08T19:14:12.327605Z"},"papermill":{"duration":544.022484,"end_time":"2024-09-08T19:14:12.33053","exception":false,"start_time":"2024-09-08T19:05:08.308046","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 842ms/step - loss: 1.6851 - sparse_categorical_accuracy: 0.5356\n","Epoch 2/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 835ms/step - loss: 1.6007 - sparse_categorical_accuracy: 0.5475\n","Epoch 3/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 835ms/step - loss: 1.5246 - sparse_categorical_accuracy: 0.5575\n","Epoch 4/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 835ms/step - loss: 1.4837 - sparse_categorical_accuracy: 0.5646\n","Epoch 5/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 835ms/step - loss: 1.4438 - sparse_categorical_accuracy: 0.5767\n","Epoch 6/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 835ms/step - loss: 1.4002 - sparse_categorical_accuracy: 0.5853\n","Epoch 7/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 835ms/step - loss: 1.3505 - sparse_categorical_accuracy: 0.5938\n","Epoch 8/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 835ms/step - loss: 1.2935 - sparse_categorical_accuracy: 0.6077\n","Epoch 9/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 835ms/step - loss: 1.2293 - sparse_categorical_accuracy: 0.6223\n","Epoch 10/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 836ms/step - loss: 1.1578 - sparse_categorical_accuracy: 0.6386\n"]},{"data":{"text/plain":["<keras.src.callbacks.history.History at 0x79ecc429aec0>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["#set sequence length cf. config (512)\n","gemma_causal_lm.preprocessor.sequence_length = Config.sequence_length \n","\n","# Compile the model with loss, optimizer, and metric\n","gemma_causal_lm.compile(\n","    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer=keras.optimizers.Adam(learning_rate=Config.learning_rate),\n","    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",")\n","\n","# Train model\n","gemma_causal_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)"]},{"cell_type":"markdown","id":"08eae893","metadata":{"papermill":{"duration":0.062707,"end_time":"2024-09-08T19:14:12.457638","exception":false,"start_time":"2024-09-08T19:14:12.394931","status":"completed"},"tags":[]},"source":["# Test the fine-tuned model\n","\n","We instantiate an object of class GemmaQA. Because `gemma_causal_lm` was fine-tuned using LoRA, `gemma_qa` defined here will use the fine-tuned model."]},{"cell_type":"code","execution_count":15,"id":"04150da3","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:14:12.58702Z","iopub.status.busy":"2024-09-08T19:14:12.586274Z","iopub.status.idle":"2024-09-08T19:14:12.590756Z","shell.execute_reply":"2024-09-08T19:14:12.589822Z"},"papermill":{"duration":0.071885,"end_time":"2024-09-08T19:14:12.592606","exception":false,"start_time":"2024-09-08T19:14:12.520721","status":"completed"},"tags":[]},"outputs":[],"source":["gemma_qa = GemmaQA()"]},{"cell_type":"markdown","id":"e799f7d8","metadata":{"papermill":{"duration":0.062697,"end_time":"2024-09-08T19:14:12.71849","exception":false,"start_time":"2024-09-08T19:14:12.655793","status":"completed"},"tags":[]},"source":["For start, we are testing the model with some of the data from the training set itself."]},{"cell_type":"markdown","id":"da68ec9a","metadata":{"papermill":{"duration":0.062608,"end_time":"2024-09-08T19:14:12.844951","exception":false,"start_time":"2024-09-08T19:14:12.782343","status":"completed"},"tags":[]},"source":["## Sample 1"]},{"cell_type":"code","execution_count":16,"id":"21dbb38a","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:14:12.973764Z","iopub.status.busy":"2024-09-08T19:14:12.972914Z","iopub.status.idle":"2024-09-08T19:14:54.037394Z","shell.execute_reply":"2024-09-08T19:14:54.036378Z"},"papermill":{"duration":41.195917,"end_time":"2024-09-08T19:14:54.103331","exception":false,"start_time":"2024-09-08T19:14:12.907414","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-competition\n","\n","**<font color='red'>Question:</font>**\n","What are the different types of competitions available on Kaggle?\n","\n","**<font color='green'>Answer:</font>**\n","Kaggle competitions are a great way to get started with machine learning. There are two main types of competitions: public and private.\n","\n","## Public Competitions\n","\n","Public competitions are open to anyone and are a great way to get started with Kaggle. These competitions are a great way to learn new skills, meet other data scientists, and collaborate on interesting projects.\n","\n","Public competitions are typically hosted by Kaggle’s data science team. These competitions are a great way to get a taste of what Kaggle competitions are like.\n","\n","## Private Competitions\n","\n","Private competitions are closed competitions hosted by companies or organizations. These competitions are typically invite-only and require a special invitation to participate.\n","\n","Private competitions are a great way for companies and organizations to host closed competitions. This can be a great way to source new data science talent or to test new machine learning models in a controlled environment.\n","\n","Some private competitions may be open source. This means that the data and models used in the competition will be made publicly available. This can be a great way to learn new skills and techniques.\n","\n","Some private competitions may not be open source. This means that the data and models used in the competition will not be made publicly available. This can be a great way to keep the competition fair and to protect the intellectual property of the company or organization hosting the competition.\n","\n","Some private competitions may be a mix of open source and closed source. This is the most common type of private competition.\n","\n","Some private competitions may be a mix of open source and closed source. This is the most common type of private competition.\n","\n","Some private competitions may be closed source. This means that the data and models used in the competition will not be made publicly available. This can be a great way to keep the competition fair and to protect the intellectual property of the company or organization hosting the competition.\n","\n","Some private competitions may be closed source. This means that the data and models used in the competition will not be made publicly available. This can be a great way to keep the competition fair and to protect the intellectual property of the company or organization hosting the competition.\n","\n","Some private competitions may be closed source. This means that the data and models used in the competition will not be made publicly available. This can be a great way to keep the competition fair and to protect the intellectual property of the company or organization hosting the competition.\n","\n","Some private competitions may be"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["row = df.iloc[0]\n","gemma_qa.query(row.Category,row.Question)"]},{"cell_type":"markdown","id":"eaa689ae","metadata":{"papermill":{"duration":0.063818,"end_time":"2024-09-08T19:14:54.231555","exception":false,"start_time":"2024-09-08T19:14:54.167737","status":"completed"},"tags":[]},"source":["## Sample 2"]},{"cell_type":"code","execution_count":17,"id":"34800640","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:14:54.362008Z","iopub.status.busy":"2024-09-08T19:14:54.3616Z","iopub.status.idle":"2024-09-08T19:15:13.777533Z","shell.execute_reply":"2024-09-08T19:15:13.776508Z"},"papermill":{"duration":19.48381,"end_time":"2024-09-08T19:15:13.77964","exception":false,"start_time":"2024-09-08T19:14:54.29583","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-tpu\n","\n","**<font color='red'>Question:</font>**\n","How to load and save model on TPU?\n","\n","**<font color='green'>Answer:</font>**\n","When using TPU, you can load and save models using the same syntax as on CPU. However, there are a few differences:\n","\n","- TPU models must be saved in the `keras.models.load_model` format.\n","- TPU models must be saved in the `keras.models.save_model` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format.\n","- TPU models must be saved in the `h5` format"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["row = df.iloc[15]\n","gemma_qa.query(row.Category,row.Question)"]},{"cell_type":"markdown","id":"0900f6b3","metadata":{"papermill":{"duration":0.06417,"end_time":"2024-09-08T19:15:13.910887","exception":false,"start_time":"2024-09-08T19:15:13.846717","status":"completed"},"tags":[]},"source":["## Sample 3"]},{"cell_type":"code","execution_count":18,"id":"070c58e9","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:15:14.045175Z","iopub.status.busy":"2024-09-08T19:15:14.044387Z","iopub.status.idle":"2024-09-08T19:15:33.29729Z","shell.execute_reply":"2024-09-08T19:15:33.296106Z"},"papermill":{"duration":19.322329,"end_time":"2024-09-08T19:15:33.29939","exception":false,"start_time":"2024-09-08T19:15:13.977061","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-noteboook\n","\n","**<font color='red'>Question:</font>**\n","What are the different types of notebooks available on Kaggle?\n","\n","**<font color='green'>Answer:</font>**\n","Kaggle Notebooks are a powerful tool for data science. They allow you to write code, run analyses, and share your findings in a reproducible and collaborative way. There are two types of Notebooks available on Kaggle: Public Notebooks and Private Notebooks.\n","\n","## Public Notebooks\n","\n","Public Notebooks are a great way to share your work with the community. Anyone can view and run your code, but only you can make changes to the notebook.\n","\n","Public Notebooks are a great way to share your work with the community. Anyone can view and run your code, but only you can make changes to the notebook.\n","\n","Public Notebooks are a great way to share your work with the community. Anyone can view and run your code, but only you can make changes to the notebook.\n","\n","Public Notebooks are a great way to share your work with the community. Anyone can view and run your code, but only you can make changes to the notebook.\n","\n","Public Notebooks are a great way to share your work with the community. Anyone can view and run your code, but only you can make changes to the notebook.\n","\n","Public Notebooks are a great way to share your work with the community. Anyone can view and run your code, but only you can make changes to the notebook.\n","\n","Public Notebooks are a great way to share your work with the community. Anyone can view and run your code, but only you can make changes to the notebook.\n","\n","Public Notebooks are a great way to share your work with the community. Anyone can view and run your code, but only you can make changes to the notebook.\n","\n","Public Notebooks are a great way to share your work with the community. Anyone can view and run your code, but only you can make changes to the notebook.\n","\n","Public Notebooks are a great way to share your work with the community. Anyone can view and run your code, but only you can make changes to the notebook.\n","\n","Public Notebooks are a great way to share your work with the community. Anyone can view and run your code, but only you can make changes to the notebook.\n","\n","Public Notebooks are a great way to share your work with the community. Anyone can view and run your code, but only you can make changes to the notebook.\n","\n","Public Notebooks are a great way to share your work with the community. Anyone can view and run your code, but only you can make changes to the notebook"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["row = df.iloc[25]\n","gemma_qa.query(row.Category,row.Question)"]},{"cell_type":"markdown","id":"66f7b759","metadata":{"papermill":{"duration":0.064195,"end_time":"2024-09-08T19:15:33.4285","exception":false,"start_time":"2024-09-08T19:15:33.364305","status":"completed"},"tags":[]},"source":["## Not seen question(s)"]},{"cell_type":"code","execution_count":19,"id":"554ec346","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:15:33.5592Z","iopub.status.busy":"2024-09-08T19:15:33.558431Z","iopub.status.idle":"2024-09-08T19:15:53.12487Z","shell.execute_reply":"2024-09-08T19:15:53.123789Z"},"papermill":{"duration":19.634139,"end_time":"2024-09-08T19:15:53.126814","exception":false,"start_time":"2024-09-08T19:15:33.492675","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-notebook\n","\n","**<font color='red'>Question:</font>**\n","How to run a notebook?\n","\n","**<font color='green'>Answer:</font>**\n","You can run a notebook in two ways:\n","\n","- Directly in the browser: click on the \"Run\" button (or press alt + R on your keyboard) to execute a single cell or right-click on the notebook and select \"Run to <n> to <n> (or Run Selected to <n> to <n>) to execute a batch of cells.\n","- Locally on your computer: download the notebook as a .ipynb file and open it in Jupiter Notebook on your local machine.\n","\n","## Running Notebook Code\n","\n","When you run code cells in a Notebook, the kernel will execute that code, evaluate the output, and display it in the Notebook pane. You can execute code in a cell one cell at a time or multiple cells at once by selecting multiple cells and running all of them at once (Ctrl/Cmd-E-R).\n","\n","You can also run a Notebook in debug mode. Debug mode is useful when you are trying to track down an error in your code. When in debug mode, the Notebook will stop at any syntax errors in the code. You can enable debug mode by clicking on the \"Debug\" button on the toolbar.\n","\n","## Running Notebook Code Locally\n","\n","You can download a Notebook as a .ipynb file and run it locally on your computer. This can be useful for editing and saving your work, or for sharing a Notebook with collaborators.\n","\n","To download a Notebook locally, click on the \"Download\" button on the top bar. This will save a copy of the Notebook in your Downloads folder.\n","\n","To open the Notebook locally, you will need to have Jupiter Notebook installed on your computer. You can download it for free from https://github.com/rfast/r-help/issues#start-here.\n","\n","Once you have Jupiter Notebook installed, open a terminal or command prompt and navigate to the folder where you saved your .ipynb file. Then, run the command \"jupyter notebook\". This will open the Notebook editor and load the Notebook you downloaded locally.\n","\n","## Running Notebooks in Parallel\n","\n","You can run multiple instances of the kernel simultaneously to execute multiple cells in parallel. This can be useful for speeding up the execution of a Notebook with a large number of cells.\n","\n","To run multiple instances of the kernel, click on the \"Run\" button on the top bar and select \"Run to <n> to <n> (or Run Selected to"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["category = \"notebook\"\n","question = \"How to run a notebook?\"\n","gemma_qa.query(category,question)"]},{"cell_type":"code","execution_count":20,"id":"e0bc2f0a","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:15:53.260025Z","iopub.status.busy":"2024-09-08T19:15:53.259178Z","iopub.status.idle":"2024-09-08T19:16:12.784043Z","shell.execute_reply":"2024-09-08T19:16:12.783061Z"},"papermill":{"duration":19.592173,"end_time":"2024-09-08T19:16:12.786151","exception":false,"start_time":"2024-09-08T19:15:53.193978","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-discussions\n","\n","**<font color='red'>Question:</font>**\n","How to create a discussion topic?\n","\n","**<font color='green'>Answer:</font>**\n","Discussions are a great way to ask questions, share ideas, and learn from others in the Kaggle community.\n","\n","To start a discussion, click on the \"Discussions\" tab in the menu bar at the top of the page. Then click on the \"New Discussion\" button.\n","\n","You will be prompted to enter a title and description for your discussion.\n","\n","## Title\n","\n","Your title should be descriptive and clear. It should also be unique. If you’re starting a discussion about a specific dataset, for example, make sure the title is not the same as a previous discussion about the same dataset.\n","\n","## Description\n","\n","Your description is a great place to provide more context about your discussion. You can also use the description to link to a relevant notebook or dataset.\n","\n","## Tags\n","\n","Tags are keywords that help other users find discussions that are relevant to their interests. You can add up to 5 tags to your discussion.\n","\n","To add tags, click on the \"Tags\" field and start typing. You will see a list of suggested tags. Click and drag to select the tags you want.\n","\n","## Participants\n","\n","Discussions are public forums. Anyone can view and contribute to a discussion.\n","\n","However, you can also choose to make the discussion private. This way only people with a link to the discussion will be able to see or contribute to it.\n","\n","To make a discussion private, click on the \"Participants\" button and select \"Private\".\n","\n","## Creating a Discussion\n","\n","Now you’re ready to start a discussion!\n","\n","Click on the “Discussions” tab in the menu bar at the top of the page. Then click on the “New Discussion” button.\n","\n","You will be prompted to enter a title and description for your discussion.\n","\n","## Title\n","\n","Your title should be descriptive and clear. It should also be unique. If you’re starting a discussion about a specific dataset, for example, make sure the title is not the same as a previous discussion about the same dataset.\n","\n","## Description\n","\n","Your description is a great place to provide more context about your discussion. You can also use the description to link to a relevant notebook or dataset.\n","\n","## Tags\n","\n","Tags are keywords that help other users find discussions that are relevant to their interests. You can add up to 5 tags to your discussion.\n","\n","To add tags, click on the “Tags” field and start typing. You will see a list of"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["category = \"discussions\"\n","question = \"How to create a discussion topic?\"\n","gemma_qa.query(category,question)"]},{"cell_type":"code","execution_count":21,"id":"453100b5","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:16:12.917094Z","iopub.status.busy":"2024-09-08T19:16:12.916481Z","iopub.status.idle":"2024-09-08T19:16:32.439504Z","shell.execute_reply":"2024-09-08T19:16:32.438562Z"},"papermill":{"duration":19.590241,"end_time":"2024-09-08T19:16:32.441622","exception":false,"start_time":"2024-09-08T19:16:12.851381","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-competitions\n","\n","**<font color='red'>Question:</font>**\n","What is a code competition?\n","\n","**<font color='green'>Answer:</font>**\n","Code competitions are a great way to practice your skills and learn new techniques. They are also a great way to get your work in front of the Kaggle community.\n","\n","There are two types of code competitions:\n","1. Kaggle Notebooks Competitions: These competitions are run on Kaggle Notebooks. You will be provided with a dataset and a set of tasks to complete.\n","2. Code Challenges: These competitions are run on Kaggle’s public Datasets. You will be provided with a dataset and a set of tasks to complete.\n","\n","Kaggle Notebooks Competitions are a great way to get started with code competitions. They are relatively easy to understand and follow, and the datasets are often well-suited for beginners.\n","\n","Code Challenges are a more advanced type of code competition. You will need to be familiar with more advanced techniques, such as using machine learning models, to succeed.\n","\n","## Kaggle Notebooks Competitions\n","\n","Kaggle Notebooks Competitions are a great way to get started with code competitions. They are relatively easy to understand and follow, and the datasets are often well-suited for beginners.\n","\n","There are two types of Kaggle Notebooks Competitions:\n","\n","- **Public Competitions**: Competitions that are open to the public.\n","- **Invite-Only Competitions**: Competitions that are only accessible to people who have been invited by the competition organizer.\n","\n","Invite-Only Competitions are typically more challenging than Public Competitions, as they are designed for a more advanced audience.\n","\n","If you’re new to code competitions, we recommend starting with a Public Competition.\n","\n","## Code Challenges\n","\n","Code Challenges are a more advanced type of code competition. You will need to be familiar with more advanced techniques, such as using machine learning models, to succeed.\n","\n","There are two types of Code Challenges:\n","\n","- **Public Competitions**: Competitions that are open to the public.\n","- **Invite-Only Competitions**: Competitions that are only accessible to people who have been invited by the competition organizer.\n","\n","Invite-Only Competitions are typically more challenging than Public Competitions, as they are designed for a more advanced audience.\n","\n","If you’re new to code competitions, we recommend starting with a Public Competition.\n","\n","## How to Win\n","\n","There are a few key things to keep in mind if you want to win a code competition:\n","\n","- **Read the instructions carefully**: The competition organizers will provide instructions on how to complete the tasks. Make sure you understand them before starting.\n"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["category = \"competitions\"\n","question = \"What is a code competition?\"\n","gemma_qa.query(category,question)"]},{"cell_type":"code","execution_count":22,"id":"3443c4ef","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:16:32.572659Z","iopub.status.busy":"2024-09-08T19:16:32.571892Z","iopub.status.idle":"2024-09-08T19:16:51.937872Z","shell.execute_reply":"2024-09-08T19:16:51.936918Z"},"papermill":{"duration":19.4334,"end_time":"2024-09-08T19:16:51.9402","exception":false,"start_time":"2024-09-08T19:16:32.5068","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-datasets\n","\n","**<font color='red'>Question:</font>**\n","What are the steps to create a Kaggle dataset?\n","\n","**<font color='green'>Answer:</font>**\n","To create a Kaggle dataset, follow these steps:\n","\n","1. Navigate to https://www.kaggle.com/datasets.\n","2. Click on the \"New Dataset\" button.\n","3. Fill out the form with the following information:\n","\n","- **Dataset Name**: The name of your dataset.\n","- **Description**: A description of your dataset.\n","- **License**: A license for your dataset.\n","- **File Structure**: The file structure of your dataset.\n","- **Data Files**: The files that make up your dataset.\n","- **Thumbnail**: A preview image for your dataset.\n","\n","1. Upload your files.\n","2. Review your dataset.\n","3. Publish your dataset.\n","4. Share your dataset!\n","\n","You can also create a dataset programmatically using the Kaggle API.\n","\n","## Creating a Dataset Programmatically\n","\n","You can create a Kaggle dataset programmatically using the Kaggle API.\n","\n","To get started, create a new file called `create_dataset.py` and add the following code:\n","\n","```python\n","import kaggle as kag\n","\n","# Replace with your Kaggle API token\n","kag.Dataset.create(\n","    \"YourDatasetName\",\n","    \"YourDatasetDescription\",\n","    \"YourFileStructure\",\n","    \"YourFileUrls\",\n","    \"YourLicense\",\n","    \"YourThumbnail\",\n",")\n","```\n","\n","For more information and examples, see the [API documentation](https://www.kaggle.com/docs/reference/api/datasets/).\n","\n","## Creating a Dataset from Scratch\n","\n","If you don't have a file structure or files to upload, you can create a dataset from scratch.\n","\n","1. Fill out the form with the following information:\n","\n","- **Dataset Name**: The name of your dataset.\n","- **Description**: A description of your dataset.\n","- **License**: A license for your dataset.\n","- **File Structure**: The file structure of your dataset.\n","- **Data Files**: The files that make up your dataset.\n","- **Thumbnail**: A preview image for your dataset.\n","\n","1. Click on the \"Create Dataset\" button.\n","2. You will be redirected to a new page where you can create the dataset.\n","3. Review your dataset.\n","4. Publish your dataset!\n","\n","## Creating a Dataset from Another Dataset\n","\n","If you have a"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["category = \"datasets\"\n","question = \"What are the steps to create a Kaggle dataset?\"\n","gemma_qa.query(category,question)"]},{"cell_type":"markdown","id":"98248301","metadata":{"papermill":{"duration":0.066199,"end_time":"2024-09-08T19:16:52.074521","exception":false,"start_time":"2024-09-08T19:16:52.008322","status":"completed"},"tags":[]},"source":["# Save the model"]},{"cell_type":"code","execution_count":23,"id":"c342ace8","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:16:52.211005Z","iopub.status.busy":"2024-09-08T19:16:52.210131Z","iopub.status.idle":"2024-09-08T19:17:30.979115Z","shell.execute_reply":"2024-09-08T19:17:30.977875Z"},"papermill":{"duration":38.837122,"end_time":"2024-09-08T19:17:30.981841","exception":false,"start_time":"2024-09-08T19:16:52.144719","status":"completed"},"tags":[]},"outputs":[],"source":["preset_dir = \".\\gemma2_2b_en_kaggle_docs\"\n","gemma_causal_lm.save_to_preset(preset_dir)"]},{"cell_type":"markdown","id":"b19eb7e7","metadata":{"papermill":{"duration":0.066518,"end_time":"2024-09-08T19:17:31.116533","exception":false,"start_time":"2024-09-08T19:17:31.050015","status":"completed"},"tags":[]},"source":["# Publish Model on Kaggle as a Kaggle Model\n","\n","We are publishing now the saved model as a Kaggle Model."]},{"cell_type":"code","execution_count":24,"id":"17cd02d8","metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:17:31.249463Z","iopub.status.busy":"2024-09-08T19:17:31.248593Z","iopub.status.idle":"2024-09-08T19:20:18.115562Z","shell.execute_reply":"2024-09-08T19:20:18.114445Z"},"papermill":{"duration":166.935908,"end_time":"2024-09-08T19:20:18.117893","exception":false,"start_time":"2024-09-08T19:17:31.181985","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Uploading Model https://www.kaggle.com/models/gpreda/gemma2-kaggle-docs/keras/gemma2_2b_en_kaggle_docs ...\n","Model 'gemma2-kaggle-docs' does not exist or access is forbidden for user 'gpreda'. Creating or handling Model...\n","Model 'gemma2-kaggle-docs' Created.\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/task.json\n"]},{"name":"stderr","output_type":"stream","text":["Uploading: 100%|██████████| 2.78k/2.78k [00:00<00:00, 4.35kB/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/task.json (3KB)\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/model.weights.h5\n"]},{"name":"stderr","output_type":"stream","text":["\n","Uploading: 100%|██████████| 10.5G/10.5G [02:37<00:00, 66.4MB/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/model.weights.h5 (10GB)\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/preprocessor.json\n"]},{"name":"stderr","output_type":"stream","text":["\n","Uploading: 100%|██████████| 1.25k/1.25k [00:00<00:00, 2.08kB/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/preprocessor.json (1KB)\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/config.json\n"]},{"name":"stderr","output_type":"stream","text":["\n","Uploading: 100%|██████████| 782/782 [00:00<00:00, 1.24kB/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/config.json (782B)\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/tokenizer.json\n"]},{"name":"stderr","output_type":"stream","text":["\n","Uploading: 100%|██████████| 498/498 [00:00<00:00, 743B/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/tokenizer.json (498B)\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/metadata.json\n"]},{"name":"stderr","output_type":"stream","text":["\n","Uploading: 100%|██████████| 143/143 [00:00<00:00, 236B/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/metadata.json (143B)\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/assets/tokenizer/vocabulary.spm\n"]},{"name":"stderr","output_type":"stream","text":["\n","Uploading: 100%|██████████| 4.24M/4.24M [00:00<00:00, 5.99MB/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/assets/tokenizer/vocabulary.spm (4MB)\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Your model instance has been created.\n","Files are being processed...\n","See at: https://www.kaggle.com/models/gpreda/gemma2-kaggle-docs/keras/gemma2_2b_en_kaggle_docs\n"]}],"source":["kaggle_username = os.environ[\"KAGGLE_USERNAME\"]\n","\n","kaggle_uri = f\"kaggle://{kaggle_username}/gemma2-kaggle-docs/keras/gemma2_2b_en_kaggle_docs\"\n","keras_nlp.upload_preset(kaggle_uri, preset_dir)"]},{"cell_type":"markdown","id":"9f644c02","metadata":{"papermill":{"duration":0.201449,"end_time":"2024-09-08T19:20:18.470483","exception":false,"start_time":"2024-09-08T19:20:18.269034","status":"completed"},"tags":[]},"source":["# Conclusions\n","\n"]},{"cell_type":"markdown","id":"7785e8ac","metadata":{"papermill":{"duration":0.148773,"end_time":"2024-09-08T19:20:18.767997","exception":false,"start_time":"2024-09-08T19:20:18.619224","status":"completed"},"tags":[]},"source":["We demonstated how to fine-tune a **Gemma 2** model using LoRA.   \n","We also created a class to run queries to the **Gemma 2** model and tested it with some examples from the existing training data but also with some new, not seen questions.   \n","We also saved the models as a Keras model. \n","Then we published the model as a Kaggle Model on Kaggle Models platform."]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4484051,"sourceId":7711309,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":78150,"modelInstanceId":72244,"sourceId":85984,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":1067.779252,"end_time":"2024-09-08T19:20:23.212032","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-09-08T19:02:35.43278","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}