{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras?scriptVersionId=195839219\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"891bda00","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.012643,"end_time":"2024-09-08T17:53:18.824135","exception":false,"start_time":"2024-09-08T17:53:18.811492","status":"completed"},"tags":[]},"source":["<center><h1>Fine-tuning Gemma 2 model using LoRA and Keras</h1></center>\n","\n","<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n","\n","\n","# Introduction\n","\n","This notebook will demonstrate three things:\n","\n","1. How to fine-tune Gemma model using LoRA\n","2. Creation of a specialised class to query about Kaggle features\n","3. Some results of querying about Kaggle Docs\n","\n","This work is largely based on previous work. Here I list the sources:\n","\n","1. Gemma 2 Model Card, Kaggle Models,https://www.kaggle.com/models/google/gemma-2/\n","2. Kaggle QA with Gemma - KerasNLP Starter, Kaggle Code, https://www.kaggle.com/code/awsaf49/kaggle-qa-with-gemma-kerasnlp-starter (Version 11)  \n","3. Fine-tune Gemma models in Keras using LoRA, Kaggle Code, https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora (Version 1) \n","4. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, LoRA: Low-Rank Adaptation of Large Language Models, ArXiv, https://arxiv.org/pdf/2106.09685.pdf\n","5. Abheesht Sharma, Matthew Watson, Parameter-efficient fine-tuning of GPT-2 with LoRA, https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/\n","6. Keras 3 API documentation / KerasNLP / Models / Gemma, https://keras.io/api/keras_nlp/models/gemma/\n","7. Unlock the Power of Gemma 2: Prompt it like a Pro, https://www.kaggle.com/code/gpreda/unlock-the-power-of-gemma-2-prompt-it-like-a-pro  \n","8. Fine-tune Gemma using LoRA and Keras, https://www.kaggle.com/code/gpreda/fine-tune-gemma-using-lora-and-keras\n","9. Fine-tunning Gemma model with Kaggle Docs data, https://www.kaggle.com/code/gpreda/fine-tunning-gemma-model-with-kaggle-docs-data\n","10. Kaggle Docs, Kaggle Dataset, https://www.kaggle.com/datasets/awsaf49/kaggle-docs  \n","\n","\n","**Let's go**!\n"]},{"cell_type":"markdown","id":"d6177009","metadata":{"papermill":{"duration":0.0114,"end_time":"2024-09-08T17:53:18.848327","exception":false,"start_time":"2024-09-08T17:53:18.836927","status":"completed"},"tags":[]},"source":["# What is Gemma 2?\n","\n","Gemma is a collection of lightweight, advanced open models developed by Google, leveraging the same research and technology behind the Gemini models. These models are text-to-text, decoder-only large language models available in English, with open weights provided for both pre-trained and instruction-tuned versions. Gemma models excel in a range of text generation tasks, such as question answering, summarization, and reasoning. Their compact size allows for deployment in resource-constrained environments like laptops, desktops, or personal cloud infrastructure, making state-of-the-art AI models more accessible and encouraging innovation for all. \n","\n","Gemma 2 represent the 2nd generation of Gemma models. These models were trained on a dataset of text data that includes a wide variety of sources. The **27B** model was trained with **13 trillion** tokens, the **9B** model was trained with **8 trillion tokens**, and **2B** model was trained with **2 trillion** tokens. Here is a summary of their key components: \n","* **Web Documents**: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content.\n","* **Code**: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions.\n","* **Mathematics**: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.\n","\n","To learn more about Gemma 2, follow this link: [Gemma 2 Model Card](https://www.kaggle.com/models/google/gemma-2).\n","\n","\n"]},{"cell_type":"markdown","id":"5fdda71d","metadata":{"papermill":{"duration":0.011271,"end_time":"2024-09-08T17:53:18.870919","exception":false,"start_time":"2024-09-08T17:53:18.859648","status":"completed"},"tags":[]},"source":["# What is LoRA?  \n","\n","**LoRA** stands for **Low-Rank Adaptation**. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to **LoRA** paper, this number decreases **10,000 times**, and the computational resources size decreases 3 times. "]},{"cell_type":"markdown","id":"4870e5d5","metadata":{"papermill":{"duration":0.011152,"end_time":"2024-09-08T17:53:18.894439","exception":false,"start_time":"2024-09-08T17:53:18.883287","status":"completed"},"tags":[]},"source":["# How we proceed?\n","\n","For fine-tunning with LoRA, we will follow the steps:\n","\n","1. Install prerequisites\n","2. Load and process the data for fine-tuning\n","3. Initialize the code for Gemma causal language model (Gemma Causal LM)\n","4. Perform fine-tuning\n","5. Test the fine-tunned model with questions from the data used for fine-tuning and with aditional questions"]},{"cell_type":"markdown","id":"09cff78e","metadata":{"papermill":{"duration":0.011598,"end_time":"2024-09-08T17:53:18.91792","exception":false,"start_time":"2024-09-08T17:53:18.906322","status":"completed"},"tags":[]},"source":["# Prerequisites\n","\n","\n","## Install packages\n","\n","We start by installing `keras-nlp` and `keras` packages."]},{"cell_type":"code","execution_count":1,"id":"da430259","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-09-08T17:53:18.943655Z","iopub.status.busy":"2024-09-08T17:53:18.943246Z","iopub.status.idle":"2024-09-08T17:54:02.349323Z","shell.execute_reply":"2024-09-08T17:54:02.348031Z"},"papermill":{"duration":43.422545,"end_time":"2024-09-08T17:54:02.352218","exception":false,"start_time":"2024-09-08T17:53:18.929673","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\r\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.5.0 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","keras-cv 0.8.2 requires keras-core, which is not installed.\u001b[0m\u001b[31m\r\n","\u001b[0m"]}],"source":["# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n","!pip install -q -U keras-nlp\n","!pip install -q -U keras>=3\n","!pip install -q -U kagglehub --upgrade"]},{"cell_type":"markdown","id":"7edf7f67","metadata":{"papermill":{"duration":0.011389,"end_time":"2024-09-08T17:54:02.37542","exception":false,"start_time":"2024-09-08T17:54:02.364031","status":"completed"},"tags":[]},"source":["## Import packages\n","\n","Now we can import the packages we just installed. We will also install `os`, so that we can set the environment variables needed for keras backend. We will use `jax` as `KERAS_BACKEND`.\n","\n","Because we want to publish the Model from the Notebook, we also include `kagglehub` and import secrets from `Kaggle App`."]},{"cell_type":"code","execution_count":2,"id":"f604dcaf","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-09-08T17:54:02.400386Z","iopub.status.busy":"2024-09-08T17:54:02.399694Z","iopub.status.idle":"2024-09-08T17:54:17.20565Z","shell.execute_reply":"2024-09-08T17:54:17.204648Z"},"papermill":{"duration":14.821569,"end_time":"2024-09-08T17:54:17.208531","exception":false,"start_time":"2024-09-08T17:54:02.386962","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-09-08 17:54:06.836889: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-09-08 17:54:06.836993: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-09-08 17:54:06.973862: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import os\n","os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n","os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n","os.environ[\"JAX_PLATFORMS\"] = \"\"\n","import keras\n","import keras_nlp\n","import kagglehub\n","\n","\n","from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","os.environ[\"KAGGLE_USERNAME\"] = user_secrets.get_secret(\"kaggle_username\")\n","os.environ[\"KAGGLE_KEY\"] = user_secrets.get_secret(\"kaggle_key\")\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","tqdm.pandas() # progress bar for pandas\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from IPython.display import display, Markdown"]},{"cell_type":"markdown","id":"90103d04","metadata":{"papermill":{"duration":0.012013,"end_time":"2024-09-08T17:54:17.235837","exception":false,"start_time":"2024-09-08T17:54:17.223824","status":"completed"},"tags":[]},"source":["## Configurations\n","\n","\n","We use a `Config` class to group the information needed to control the fine-tuning process:\n","* random seed \n","* dataset path\n","* preset - name of pretrained Gemma 2\n","* sequence length - this is the maximum size of input sequence for training\n","* batch size - size of the input batch in training, x 2 as two GPUs\n","* lora rank - rank for LoRA, higher means more trainable parameters \n","* learning rate used in the train\n","* epochs - number of epochs for train"]},{"cell_type":"code","execution_count":3,"id":"9d332f78","metadata":{"execution":{"iopub.execute_input":"2024-09-08T17:54:17.260959Z","iopub.status.busy":"2024-09-08T17:54:17.260465Z","iopub.status.idle":"2024-09-08T17:54:17.26578Z","shell.execute_reply":"2024-09-08T17:54:17.264883Z"},"papermill":{"duration":0.019889,"end_time":"2024-09-08T17:54:17.267846","exception":false,"start_time":"2024-09-08T17:54:17.247957","status":"completed"},"tags":[]},"outputs":[],"source":["class Config:\n","    seed = 42\n","    dataset_path = \"/kaggle/input/kaggle-docs/questions_answers\"\n","    preset = \"gemma2_2b_en\" # name of pretrained Gemma 2\n","    sequence_length = 512 # max size of input sequence for training\n","    batch_size = 1 # size of the input batch in training\n","    lora_rank = 3 # rank for LoRA, higher means more trainable parameters\n","    learning_rate=8e-5 # learning rate used in train\n","    epochs = 10 # number of epochs to train"]},{"cell_type":"markdown","id":"2bb7d458","metadata":{"papermill":{"duration":0.011276,"end_time":"2024-09-08T17:54:17.290674","exception":false,"start_time":"2024-09-08T17:54:17.279398","status":"completed"},"tags":[]},"source":["Set a random seed for results reproducibility."]},{"cell_type":"code","execution_count":4,"id":"52ec2faf","metadata":{"execution":{"iopub.execute_input":"2024-09-08T17:54:17.315056Z","iopub.status.busy":"2024-09-08T17:54:17.314765Z","iopub.status.idle":"2024-09-08T17:54:17.319275Z","shell.execute_reply":"2024-09-08T17:54:17.318352Z"},"papermill":{"duration":0.019519,"end_time":"2024-09-08T17:54:17.321812","exception":false,"start_time":"2024-09-08T17:54:17.302293","status":"completed"},"tags":[]},"outputs":[],"source":["keras.utils.set_random_seed(Config.seed)"]},{"cell_type":"markdown","id":"7a151ceb","metadata":{"papermill":{"duration":0.011341,"end_time":"2024-09-08T17:54:17.348313","exception":false,"start_time":"2024-09-08T17:54:17.336972","status":"completed"},"tags":[]},"source":["# Load the data\n","\n","\n","We load the data we will use for fine-tunining."]},{"cell_type":"code","execution_count":5,"id":"63fc249a","metadata":{"execution":{"iopub.execute_input":"2024-09-08T17:54:17.372691Z","iopub.status.busy":"2024-09-08T17:54:17.372396Z","iopub.status.idle":"2024-09-08T17:54:17.409608Z","shell.execute_reply":"2024-09-08T17:54:17.408669Z"},"papermill":{"duration":0.052144,"end_time":"2024-09-08T17:54:17.41202","exception":false,"start_time":"2024-09-08T17:54:17.359876","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","      <th>Category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What are the different types of competitions a...</td>\n","      <td># Types of Competitions\\n\\nKaggle Competitions...</td>\n","      <td>competition</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What are the different competition formats on ...</td>\n","      <td>There are handful of different formats competi...</td>\n","      <td>competition</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>How to join a competition?</td>\n","      <td>Before you start, navigate to the [Competition...</td>\n","      <td>competition</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>How to form, manage, and disband teams in a co...</td>\n","      <td>Everyone that competes in a Competition does s...</td>\n","      <td>competition</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>How do I make a submission in a competition?</td>\n","      <td>You will need to submit your model predictions...</td>\n","      <td>competition</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Question  \\\n","0  What are the different types of competitions a...   \n","1  What are the different competition formats on ...   \n","2                         How to join a competition?   \n","3  How to form, manage, and disband teams in a co...   \n","4       How do I make a submission in a competition?   \n","\n","                                              Answer     Category  \n","0  # Types of Competitions\\n\\nKaggle Competitions...  competition  \n","1  There are handful of different formats competi...  competition  \n","2  Before you start, navigate to the [Competition...  competition  \n","3  Everyone that competes in a Competition does s...  competition  \n","4  You will need to submit your model predictions...  competition  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(f\"{Config.dataset_path}/data.csv\")\n","df.head()"]},{"cell_type":"markdown","id":"37ab0e97","metadata":{"papermill":{"duration":0.011521,"end_time":"2024-09-08T17:54:17.44002","exception":false,"start_time":"2024-09-08T17:54:17.428499","status":"completed"},"tags":[]},"source":["Let's check the total number of rows in this dataset."]},{"cell_type":"code","execution_count":6,"id":"085f5ce4","metadata":{"execution":{"iopub.execute_input":"2024-09-08T17:54:17.464613Z","iopub.status.busy":"2024-09-08T17:54:17.46433Z","iopub.status.idle":"2024-09-08T17:54:17.470079Z","shell.execute_reply":"2024-09-08T17:54:17.469126Z"},"papermill":{"duration":0.020387,"end_time":"2024-09-08T17:54:17.472157","exception":false,"start_time":"2024-09-08T17:54:17.45177","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["60"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df.shape[0]"]},{"cell_type":"markdown","id":"7d0a740f","metadata":{"papermill":{"duration":0.013415,"end_time":"2024-09-08T17:54:17.497463","exception":false,"start_time":"2024-09-08T17:54:17.484048","status":"completed"},"tags":[]},"source":["For easiness, we will create the following template for QA: "]},{"cell_type":"code","execution_count":7,"id":"59e6f6d9","metadata":{"execution":{"iopub.execute_input":"2024-09-08T17:54:17.522637Z","iopub.status.busy":"2024-09-08T17:54:17.522384Z","iopub.status.idle":"2024-09-08T17:54:17.531687Z","shell.execute_reply":"2024-09-08T17:54:17.530906Z"},"papermill":{"duration":0.024157,"end_time":"2024-09-08T17:54:17.53348","exception":false,"start_time":"2024-09-08T17:54:17.509323","status":"completed"},"tags":[]},"outputs":[],"source":["template = \"\\n\\nCategory:\\nkaggle-{Category}\\n\\nQuestion:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\n","df[\"prompt\"] = df.apply(lambda row: template.format(Category=row.Category,\n","                                                             Question=row.Question,\n","                                                             Answer=row.Answer), axis=1)\n","data = df.prompt.tolist()"]},{"cell_type":"markdown","id":"0451e4d3","metadata":{"papermill":{"duration":0.011617,"end_time":"2024-09-08T17:54:17.556941","exception":false,"start_time":"2024-09-08T17:54:17.545324","status":"completed"},"tags":[]},"source":["## Template utility function"]},{"cell_type":"code","execution_count":8,"id":"92e03d98","metadata":{"execution":{"iopub.execute_input":"2024-09-08T17:54:17.581633Z","iopub.status.busy":"2024-09-08T17:54:17.581339Z","iopub.status.idle":"2024-09-08T17:54:17.585908Z","shell.execute_reply":"2024-09-08T17:54:17.585145Z"},"papermill":{"duration":0.0191,"end_time":"2024-09-08T17:54:17.587807","exception":false,"start_time":"2024-09-08T17:54:17.568707","status":"completed"},"tags":[]},"outputs":[],"source":["def colorize_text(text):\n","    for word, color in zip([\"Category\", \"Question\", \"Answer\"], [\"blue\", \"red\", \"green\"]):\n","        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n","    return text"]},{"cell_type":"markdown","id":"358a7e6f","metadata":{"papermill":{"duration":0.011967,"end_time":"2024-09-08T17:54:17.611707","exception":false,"start_time":"2024-09-08T17:54:17.59974","status":"completed"},"tags":[]},"source":["# Specialized class to query Gemma\n","\n","\n","We define a specialized class to query Gemma. But first, we need to initialize an object of GemmaCausalLM class."]},{"cell_type":"markdown","id":"498dcf8f","metadata":{"papermill":{"duration":0.012104,"end_time":"2024-09-08T17:54:17.676401","exception":false,"start_time":"2024-09-08T17:54:17.664297","status":"completed"},"tags":[]},"source":["## Initialize the code for Gemma Causal LM"]},{"cell_type":"code","execution_count":9,"id":"e4100cfe","metadata":{"execution":{"iopub.execute_input":"2024-09-08T17:54:17.713622Z","iopub.status.busy":"2024-09-08T17:54:17.713287Z","iopub.status.idle":"2024-09-08T17:55:16.794657Z","shell.execute_reply":"2024-09-08T17:55:16.793795Z"},"papermill":{"duration":59.096391,"end_time":"2024-09-08T17:55:16.796603","exception":false,"start_time":"2024-09-08T17:54:17.700212","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n","└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["gemma_causal_lm = keras_nlp.models.GemmaCausalLM.from_preset(Config.preset)\n","gemma_causal_lm.summary()"]},{"cell_type":"markdown","id":"abb6e52f","metadata":{"papermill":{"duration":0.013221,"end_time":"2024-09-08T17:55:16.823302","exception":false,"start_time":"2024-09-08T17:55:16.810081","status":"completed"},"tags":[]},"source":["## Define the specialized class\n","\n","Here we define the special class `GemmaQA`. \n","in the `__init__` we pass the `GemmaCausalLM` object created before.\n","The `query` member function uses `GemmaCausalLM` member function `generate` to generate the answer, based on a prompt that includes the category and the question."]},{"cell_type":"code","execution_count":10,"id":"63cdd49b","metadata":{"execution":{"iopub.execute_input":"2024-09-08T17:55:16.850985Z","iopub.status.busy":"2024-09-08T17:55:16.850668Z","iopub.status.idle":"2024-09-08T17:55:16.856766Z","shell.execute_reply":"2024-09-08T17:55:16.856063Z"},"papermill":{"duration":0.022117,"end_time":"2024-09-08T17:55:16.85864","exception":false,"start_time":"2024-09-08T17:55:16.836523","status":"completed"},"tags":[]},"outputs":[],"source":["class GemmaQA:\n","    def __init__(self, max_length=512):\n","        self.max_length = max_length\n","        self.prompt = template\n","        self.gemma_causal_lm = gemma_causal_lm\n","        \n","    def query(self, category, question):\n","        response = self.gemma_causal_lm.generate(\n","            self.prompt.format(\n","                Category=category,\n","                Question=question,\n","                Answer=\"\"), \n","            max_length=self.max_length)\n","        display(Markdown(colorize_text(response)))\n","        "]},{"cell_type":"markdown","id":"da788dd9","metadata":{"papermill":{"duration":0.012973,"end_time":"2024-09-08T17:55:16.885297","exception":false,"start_time":"2024-09-08T17:55:16.872324","status":"completed"},"tags":[]},"source":["## Gemma preprocessor\n","\n","\n","This preprocessing layer will take in batches of strings, and return outputs in a ```(x, y, sample_weight)``` format, where the y label is the next token id in the x sequence.\n","\n","From the code below, we can see that, after the preprocessor, the data shape is ```(num_samples, sequence_length)```."]},{"cell_type":"code","execution_count":11,"id":"d64d94c0","metadata":{"execution":{"iopub.execute_input":"2024-09-08T17:55:16.912848Z","iopub.status.busy":"2024-09-08T17:55:16.912562Z","iopub.status.idle":"2024-09-08T17:55:17.257734Z","shell.execute_reply":"2024-09-08T17:55:17.256959Z"},"papermill":{"duration":0.36167,"end_time":"2024-09-08T17:55:17.260128","exception":false,"start_time":"2024-09-08T17:55:16.898458","status":"completed"},"tags":[]},"outputs":[],"source":["x, y, sample_weight = gemma_causal_lm.preprocessor(data[0:2])"]},{"cell_type":"code","execution_count":12,"id":"baafb400","metadata":{"execution":{"iopub.execute_input":"2024-09-08T17:55:17.289264Z","iopub.status.busy":"2024-09-08T17:55:17.288638Z","iopub.status.idle":"2024-09-08T17:55:17.294688Z","shell.execute_reply":"2024-09-08T17:55:17.293715Z"},"papermill":{"duration":0.022382,"end_time":"2024-09-08T17:55:17.296837","exception":false,"start_time":"2024-09-08T17:55:17.274455","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["{'token_ids': Array([[   2,  109, 8606, ...,    0,    0,    0],\n","       [   2,  109, 8606, ...,    0,    0,    0]], dtype=int32), 'padding_mask': Array([[ True,  True,  True, ..., False, False, False],\n","       [ True,  True,  True, ..., False, False, False]], dtype=bool)} [[   109   8606 235292 ...      0      0      0]\n"," [   109   8606 235292 ...      0      0      0]]\n"]}],"source":["print(x, y)"]},{"cell_type":"markdown","id":"039130fa","metadata":{"papermill":{"duration":0.013334,"end_time":"2024-09-08T17:55:17.325035","exception":false,"start_time":"2024-09-08T17:55:17.311701","status":"completed"},"tags":[]},"source":["# Perform fine-tuning with LoRA"]},{"cell_type":"markdown","id":"5e5ff6f8","metadata":{"papermill":{"duration":0.013128,"end_time":"2024-09-08T17:55:17.351684","exception":false,"start_time":"2024-09-08T17:55:17.338556","status":"completed"},"tags":[]},"source":["## Enable LoRA for the model\n","\n","LoRA rank is setting the number of trainable parameters. A larger rank will result in a larger number of parameters to train."]},{"cell_type":"code","execution_count":13,"id":"f14e09ba","metadata":{"execution":{"iopub.execute_input":"2024-09-08T17:55:17.380155Z","iopub.status.busy":"2024-09-08T17:55:17.379633Z","iopub.status.idle":"2024-09-08T17:55:18.094662Z","shell.execute_reply":"2024-09-08T17:55:18.093786Z"},"papermill":{"duration":0.73139,"end_time":"2024-09-08T17:55:18.096657","exception":false,"start_time":"2024-09-08T17:55:17.365267","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n","└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,616,538,368</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,616,538,368\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,616,538,368</span> (9.75 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,616,538,368\u001b[0m (9.75 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,196,480</span> (8.38 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,196,480\u001b[0m (8.38 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"]},"metadata":{},"output_type":"display_data"}],"source":["# Enable LoRA for the model and set the LoRA rank to the lora_rank as set in Config (4).\n","gemma_causal_lm.backbone.enable_lora(rank=Config.lora_rank)\n","gemma_causal_lm.summary()"]},{"cell_type":"markdown","id":"6b1e9dc4","metadata":{"papermill":{"duration":0.014682,"end_time":"2024-09-08T17:55:18.127735","exception":false,"start_time":"2024-09-08T17:55:18.113053","status":"completed"},"tags":[]},"source":["We see that only a small part of the parameters are trainable. 2.6 billions parameters total, and only 2.9 Millions parameters trainable."]},{"cell_type":"markdown","id":"c3759bc4","metadata":{"papermill":{"duration":0.014291,"end_time":"2024-09-08T17:55:18.156534","exception":false,"start_time":"2024-09-08T17:55:18.142243","status":"completed"},"tags":[]},"source":["## Run the training sequence\n","\n","We set the `sequence_length` for the `GemmaCausalLM` (from configuration, will be 512).\n","We compile the model, with the loss, optimizer and metric.\n","For the metric, it is used `SparseCategoricalAccuracy`. This metric calculates how often predictions match integer labels."]},{"cell_type":"code","execution_count":14,"id":"107e99a9","metadata":{"execution":{"iopub.execute_input":"2024-09-08T17:55:18.187162Z","iopub.status.busy":"2024-09-08T17:55:18.186831Z","iopub.status.idle":"2024-09-08T18:04:20.890976Z","shell.execute_reply":"2024-09-08T18:04:20.890078Z"},"papermill":{"duration":542.721714,"end_time":"2024-09-08T18:04:20.893007","exception":false,"start_time":"2024-09-08T17:55:18.171293","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 841ms/step - loss: 1.6867 - sparse_categorical_accuracy: 0.5355\n","Epoch 2/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 833ms/step - loss: 1.6137 - sparse_categorical_accuracy: 0.5458\n","Epoch 3/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 834ms/step - loss: 1.5368 - sparse_categorical_accuracy: 0.5551\n","Epoch 4/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 834ms/step - loss: 1.4978 - sparse_categorical_accuracy: 0.5637\n","Epoch 5/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 833ms/step - loss: 1.4636 - sparse_categorical_accuracy: 0.5702\n","Epoch 6/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 833ms/step - loss: 1.4269 - sparse_categorical_accuracy: 0.5774\n","Epoch 7/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 833ms/step - loss: 1.3854 - sparse_categorical_accuracy: 0.5874\n","Epoch 8/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 833ms/step - loss: 1.3379 - sparse_categorical_accuracy: 0.5967\n","Epoch 9/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 833ms/step - loss: 1.2832 - sparse_categorical_accuracy: 0.6068\n","Epoch 10/10\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 833ms/step - loss: 1.2206 - sparse_categorical_accuracy: 0.6224\n"]},{"data":{"text/plain":["<keras.src.callbacks.history.History at 0x79223410be20>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["#set sequence length cf. config (512)\n","gemma_causal_lm.preprocessor.sequence_length = Config.sequence_length \n","\n","# Compile the model with loss, optimizer, and metric\n","gemma_causal_lm.compile(\n","    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer=keras.optimizers.Adam(learning_rate=Config.learning_rate),\n","    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",")\n","\n","# Train model\n","gemma_causal_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)"]},{"cell_type":"markdown","id":"d3334e17","metadata":{"papermill":{"duration":0.063651,"end_time":"2024-09-08T18:04:21.019555","exception":false,"start_time":"2024-09-08T18:04:20.955904","status":"completed"},"tags":[]},"source":["# Test the fine-tuned model\n","\n","We instantiate an object of class GemmaQA. Because `gemma_causal_lm` was fine-tuned using LoRA, `gemma_qa` defined here will use the fine-tuned model."]},{"cell_type":"code","execution_count":15,"id":"d3010d92","metadata":{"execution":{"iopub.execute_input":"2024-09-08T18:04:21.146821Z","iopub.status.busy":"2024-09-08T18:04:21.146474Z","iopub.status.idle":"2024-09-08T18:04:21.15077Z","shell.execute_reply":"2024-09-08T18:04:21.14986Z"},"papermill":{"duration":0.070509,"end_time":"2024-09-08T18:04:21.152646","exception":false,"start_time":"2024-09-08T18:04:21.082137","status":"completed"},"tags":[]},"outputs":[],"source":["gemma_qa = GemmaQA()"]},{"cell_type":"markdown","id":"3e85c602","metadata":{"papermill":{"duration":0.061823,"end_time":"2024-09-08T18:04:21.277076","exception":false,"start_time":"2024-09-08T18:04:21.215253","status":"completed"},"tags":[]},"source":["For start, we are testing the model with some of the data from the training set itself."]},{"cell_type":"markdown","id":"e72159e6","metadata":{"papermill":{"duration":0.061895,"end_time":"2024-09-08T18:04:21.401706","exception":false,"start_time":"2024-09-08T18:04:21.339811","status":"completed"},"tags":[]},"source":["## Sample 1"]},{"cell_type":"code","execution_count":16,"id":"cb893547","metadata":{"execution":{"iopub.execute_input":"2024-09-08T18:04:21.527584Z","iopub.status.busy":"2024-09-08T18:04:21.526856Z","iopub.status.idle":"2024-09-08T18:05:02.003671Z","shell.execute_reply":"2024-09-08T18:05:02.002742Z"},"papermill":{"duration":40.605796,"end_time":"2024-09-08T18:05:02.069262","exception":false,"start_time":"2024-09-08T18:04:21.463466","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-competition\n","\n","**<font color='red'>Question:</font>**\n","What are the different types of competitions available on Kaggle?\n","\n","**<font color='green'>Answer:</font>**\n","Kaggle competitions are a great way to get started with machine learning. They are a structured way to learn new skills and techniques, and they are a great way to meet other data scientists.\n","\n","There are two types of competitions on Kaggle: public and private.\n","\n","## Public Competitions\n","\n","Public competitions are open to anyone. They are a great way to learn new skills and techniques, and they are a great way to meet other data scientists.\n","\n","Public competitions are a great way to get started with Kaggle competitions. They are a structured way to learn new skills and techniques, and they are a great way to meet other data scientists.\n","\n","Public competitions are a great way to get started with Kaggle competitions. They are a structured way to learn new skills and techniques, and they are a great way to meet other data scientists.\n","\n","Public competitions are a great way to get started with Kaggle competitions. They are a structured way to learn new skills and techniques, and they are a great way to meet other data scientists.\n","\n","Public competitions are a great way to get started with Kaggle competitions. They are a structured way to learn new skills and techniques, and they are a great way to meet other data scientists.\n","\n","Public competitions are a great way to get started with Kaggle competitions. They are a structured way to learn new skills and techniques, and they are a great way to meet other data scientists.\n","\n","Public competitions are a great way to get started with Kaggle competitions. They are a structured way to learn new skills and techniques, and they are a great way to meet other data scientists.\n","\n","Public competitions are a great way to get started with Kaggle competitions. They are a structured way to learn new skills and techniques, and they are a great way to meet other data scientists.\n","\n","Public competitions are a great way to get started with Kaggle competitions. They are a structured way to learn new skills and techniques, and they are a great way to meet other data scientists.\n","\n","Public competitions are a great way to get started with Kaggle competitions. They are a structured way to learn new skills and techniques, and they are a great way to meet other data scientists.\n","\n","Public competitions are a great way to get started with Kaggle competitions. They are a structured way to learn new skills and techniques, and they are a great way to meet other data scientists.\n","\n","Public competitions are"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["row = df.iloc[0]\n","gemma_qa.query(row.Category,row.Question)"]},{"cell_type":"markdown","id":"6f6fec8f","metadata":{"papermill":{"duration":0.063111,"end_time":"2024-09-08T18:05:02.196481","exception":false,"start_time":"2024-09-08T18:05:02.13337","status":"completed"},"tags":[]},"source":["## Sample 2"]},{"cell_type":"code","execution_count":17,"id":"c2a874f5","metadata":{"execution":{"iopub.execute_input":"2024-09-08T18:05:02.32334Z","iopub.status.busy":"2024-09-08T18:05:02.322914Z","iopub.status.idle":"2024-09-08T18:05:21.647777Z","shell.execute_reply":"2024-09-08T18:05:21.646862Z"},"papermill":{"duration":19.390584,"end_time":"2024-09-08T18:05:21.650258","exception":false,"start_time":"2024-09-08T18:05:02.259674","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-tpu\n","\n","**<font color='red'>Question:</font>**\n","How to load and save model on TPU?\n","\n","**<font color='green'>Answer:</font>**\n","You can save and load models on TPU using the same syntax as on CPU.\n","\n","## Saving a model\n","\n","To save a model on TPU, use the `save_weights` method of the model object. This method takes a `tf.io.TFRecord` object as an argument.\n","\n","```python\n","# Load the model from disk\n","model = tf.keras.models.load_model(\"saved_model.h5\")\n","\n","# Save the model to disk\n","model.save_weights(\"saved_model.h5\")\n","```\n","\n","## Loading a model\n","\n","To load a model on TPU, use the `model_from_json` and `load_weights` methods of the model object.\n","\n","```python\n","# Load the model from disk\n","model_json = tf.io.read_file(\"saved_model.json\")\n","model_json = tf.io.gfile.GFile(model_json, \"rb\").read()\n","model = tf.keras.models.model_from_json(model_json)\n","\n","# Load the weights from disk\n","model.load_weights(\"saved_model.h5\")\n","```\n","\n","## Saving and loading a model with multiple layers\n","\n","If you have a model with multiple layers, you can save and load the model using the same syntax.\n","\n","```python\n","# Load the model from disk\n","model = tf.keras.models.load_model(\"saved_model.h5\")\n","\n","# Save the model to disk\n","model.save_weights(\"saved_model.h5\")\n","```\n","\n","## Saving and loading a model with multiple models\n","\n","If you have a model with multiple models, you can save and load the model using the same syntax.\n","\n","```python\n","# Load the model from disk\n","model_1 = tf.keras.models.load_model(\"saved_model_1.h5\")\n","model_2 = tf.keras.models.load_model(\"saved_model_2.h5\")\n","\n","# Save the model to disk\n","model_1.save_weights(\"saved_model.h5\")\n","model_2.save_weights(\"saved_model.h5\")\n","```\n","\n","## Saving and loading a model with multiple models and layers\n","\n","If you have a model with multiple models and multiple layers, you can save"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["row = df.iloc[15]\n","gemma_qa.query(row.Category,row.Question)"]},{"cell_type":"markdown","id":"2b810b66","metadata":{"papermill":{"duration":0.062798,"end_time":"2024-09-08T18:05:21.779735","exception":false,"start_time":"2024-09-08T18:05:21.716937","status":"completed"},"tags":[]},"source":["## Sample 3"]},{"cell_type":"code","execution_count":18,"id":"8491f500","metadata":{"execution":{"iopub.execute_input":"2024-09-08T18:05:21.907162Z","iopub.status.busy":"2024-09-08T18:05:21.906817Z","iopub.status.idle":"2024-09-08T18:05:41.071932Z","shell.execute_reply":"2024-09-08T18:05:41.071047Z"},"papermill":{"duration":19.230818,"end_time":"2024-09-08T18:05:41.074009","exception":false,"start_time":"2024-09-08T18:05:21.843191","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-noteboook\n","\n","**<font color='red'>Question:</font>**\n","What are the different types of notebooks available on Kaggle?\n","\n","**<font color='green'>Answer:</font>**\n","Kaggle Notebooks are a powerful tool for data science. They allow you to run code and share interactive visualizations in a collaborative, version-controlled environment.\n","\n","There are two types of Notebooks available on Kaggle:\n","\n","- **Public Notebooks**: Notebooks that are publicly accessible. You can view and run the code and visualizations in the notebook.\n","- **Private Notebooks**: Notebooks that are only accessible to you. You can view and run the code and visualizations in the notebook, but other users cannot view or run it.\n","\n","You can create a new Notebook of either type by clicking on the “New Notebook” button in the top right corner of the Kaggle platform.\n","\n","You can also find and explore a wide variety of public and private Notebooks in the “Explore Notebooks” tab of the Kaggle platform.\n","\n","## Public Notebooks\n","\n","Public Notebooks are the most common type of Notebook on Kaggle. They are a great way to share your work with the community and learn new techniques from other data scientists.\n","\n","Public Notebooks are accessible to anyone with a Kaggle account.\n","\n","## Private Notebooks\n","\n","Private Notebooks are a great way to work on sensitive or confidential projects. They are only accessible to you and cannot be viewed or run by anyone else.\n","\n","Private Notebooks are accessible only to you.\n","\n","## Collaborators\n","\n","Public and Private Notebooks can be shared with collaborators. Collaborators can view and run the code and visualizations in the notebook, but they cannot edit or delete it.\n","\n","You can invite collaborators by clicking on the “Invite Collaborators” button in the top right corner of the Notebook editor. You can also add collaborators from your contact list or from a list of Kaggle users.\n","\n","## Sharing Notebooks\n","\n","You can share a Public or Private Notebook with anyone by clicking on the “Share Notebook” button in the top right corner of the Notebook editor. You can also copy the link to the Notebook and share it directly.\n","\n","## Creating a New Notebook\n","\n","You can create a new Notebook of either type by clicking on the “New Notebook” button in the top right corner of the Kaggle platform.\n","\n","## Notebook Editor\n","\n","The Notebook editor is where you write and run your code and visualizations. It is a collaborative, version-controlled environment that allows you to work with others on your projects.\n","\n","The Notebook editor is available in two modes:\n","\n","- **Code Mode**: Code Mode is the default"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["row = df.iloc[25]\n","gemma_qa.query(row.Category,row.Question)"]},{"cell_type":"markdown","id":"5063be83","metadata":{"papermill":{"duration":0.063983,"end_time":"2024-09-08T18:05:41.201999","exception":false,"start_time":"2024-09-08T18:05:41.138016","status":"completed"},"tags":[]},"source":["## Not seen question(s)"]},{"cell_type":"code","execution_count":19,"id":"2c2b3bd7","metadata":{"execution":{"iopub.execute_input":"2024-09-08T18:05:41.331124Z","iopub.status.busy":"2024-09-08T18:05:41.33079Z","iopub.status.idle":"2024-09-08T18:06:00.809781Z","shell.execute_reply":"2024-09-08T18:06:00.808731Z"},"papermill":{"duration":19.545011,"end_time":"2024-09-08T18:06:00.812014","exception":false,"start_time":"2024-09-08T18:05:41.267003","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-notebook\n","\n","**<font color='red'>Question:</font>**\n","How to run a notebook?\n","\n","**<font color='green'>Answer:</font>**\n","You can run a notebook in two ways:\n","\n","1. From the Notebook Editor\n","2. From the Command Line\n","\n","## Notebook Editor\n","\n","You can run a notebook from the Notebook Editor by clicking on the \"Run\" button in the top right corner of the editor. This will execute the code in the notebook in a new tab.\n","\n","If you have a large notebook with a lot of code, you may want to break it up into smaller chunks and run each chunk separately. To do this, you can click on the \"Run\" button in the top right corner of the editor for each chunk of code you want to run.\n","\n","## Command Line\n","\n","You can also run a notebook from the command line. To do this, open a terminal and navigate to the directory where your notebook is located. Then, run the command:\n","\n","```bash\n","kaggle run notebook-name.ipynb\n","```\n","\n","This will open a new tab in your browser and allow you to run the notebook from the command line.\n","\n","If you encounter any errors or need to debug your notebook, you can also use the command line to interact with the notebook editor. For example, you can open a new tab in your browser and navigate to the notebook editor by running the command:\n","\n","```bash\n","kaggle editor notebook-name.ipynb\n","```\n","\n","This will open a new tab in your browser and allow you to edit and run your notebook from the command line.\n","\n","## Running a Notebook in Parallel\n","\n","If you have a large dataset or a lot of code, you may want to run your notebook in parallel. This will allow you to run multiple chunks of code at the same time, which can speed up the execution of your notebook.\n","\n","To run a notebook in parallel, you can use the `kaggle run --parallel` command line flag. This will open a new tab in your browser and allow you to run the notebook from the command line.\n","\n","## Running a Notebook in a Docker Container\n","\n","If you are using a GPU, you can also run your notebook in a Docker container. This will allow you to run your notebook in a virtual machine with all the necessary hardware and software installed.\n","\n","To run a notebook in a Docker container, you can use the `kaggle run --docker` command line flag. This will open a new tab in your browser and allow you to run the notebook from the command line.\n","\n","##"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["category = \"notebook\"\n","question = \"How to run a notebook?\"\n","gemma_qa.query(category,question)"]},{"cell_type":"code","execution_count":20,"id":"9b867c65","metadata":{"execution":{"iopub.execute_input":"2024-09-08T18:06:00.945219Z","iopub.status.busy":"2024-09-08T18:06:00.944469Z","iopub.status.idle":"2024-09-08T18:06:20.3878Z","shell.execute_reply":"2024-09-08T18:06:20.386871Z"},"papermill":{"duration":19.510712,"end_time":"2024-09-08T18:06:20.390293","exception":false,"start_time":"2024-09-08T18:06:00.879581","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-discussions\n","\n","**<font color='red'>Question:</font>**\n","How to create a discussion topic?\n","\n","**<font color='green'>Answer:</font>**\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","## Creating a new discussion topic\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions are a great way to ask questions, share ideas, and get feedback on your work.\n","\n","Discussions"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["category = \"discussions\"\n","question = \"How to create a discussion topic?\"\n","gemma_qa.query(category,question)"]},{"cell_type":"code","execution_count":21,"id":"3b74da32","metadata":{"execution":{"iopub.execute_input":"2024-09-08T18:06:20.528707Z","iopub.status.busy":"2024-09-08T18:06:20.527913Z","iopub.status.idle":"2024-09-08T18:06:30.16644Z","shell.execute_reply":"2024-09-08T18:06:30.165469Z"},"papermill":{"duration":9.706937,"end_time":"2024-09-08T18:06:30.168469","exception":false,"start_time":"2024-09-08T18:06:20.461532","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-competitions\n","\n","**<font color='red'>Question:</font>**\n","What is a code competition?\n","\n","**<font color='green'>Answer:</font>**\n","Code competitions are a great way to practice your coding skills. They are also a great way to learn new skills and techniques.\n","\n","There are many different types of code competitions, but they all share some common features. First, they are usually held online. Second, they are usually timed. Third, they are usually scored based on how well you solve the problems.\n","\n","Some code competitions are open to anyone, while others are invite-only for a select group of users. If you are interested in participating in a code competition, be sure to check out the rules and requirements for the competition you are interested in.\n","\n","Once you have registered for the competition, you will be given a set of problems to solve within a certain amount of time. The problems will be scored based on how well you solve them.\n","\n","Some code competitions also have a leaderboard where you can see how you compare to other participants. This can be a great motivator to keep you going until you have solved all the problems.\n","\n","Code competitions are a great way to practice your coding skills and learn new techniques. If you are interested in participating, be sure to check out the different types of code competitions available."],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["category = \"competitions\"\n","question = \"What is a code competition?\"\n","gemma_qa.query(category,question)"]},{"cell_type":"code","execution_count":22,"id":"f9b32fd0","metadata":{"execution":{"iopub.execute_input":"2024-09-08T18:06:30.298577Z","iopub.status.busy":"2024-09-08T18:06:30.298249Z","iopub.status.idle":"2024-09-08T18:06:49.582418Z","shell.execute_reply":"2024-09-08T18:06:49.581498Z"},"papermill":{"duration":19.350992,"end_time":"2024-09-08T18:06:49.584603","exception":false,"start_time":"2024-09-08T18:06:30.233611","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-datasets\n","\n","**<font color='red'>Question:</font>**\n","What are the steps to create a Kaggle dataset?\n","\n","**<font color='green'>Answer:</font>**\n","To create a Kaggle dataset, follow these steps:\n","\n","1. Navigate to the Datasets page.\n","2. Click on the “Create Dataset” button.\n","3. Fill out the form with the following information:\n","\n","- **Name**: The name of your dataset.\n","- **Description**: A description of your dataset.\n","- **License**: The license you want to apply to your dataset.\n","- **Tags**: A list of tags that describe your dataset.\n","- **Privacy**: The privacy setting you want to apply to your dataset.\n","- **Content**: The content of your dataset.\n","\n","1. Click on the “Create Dataset” button.\n","2. Your dataset will be created and you will be redirected to the “Overview” page.\n","3. From here, you can edit your dataset at any time.\n","\n","You can also create a dataset from scratch by clicking on the “Create Dataset” button on the Datasets page. This will take you through the same steps as above.\n","\n","If you want to create a dataset from a CSV file, you can do so by clicking on the “Upload CSV” button on the Overview page of your dataset. This will open a modal where you can select the CSV file you want to upload.\n","\n","If you want to create a dataset from a URL, you can do so by clicking on the “Create Dataset from URL” button on the Datasets page. This will open a modal where you can enter the URL of the dataset you want to create a copy of.\n","\n","If you want to create a dataset from a notebook, you can do so by clicking on the “Create Dataset from Notebook” button on the Datasets page. This will open a modal where you can select the notebook you want to create a dataset from.\n","\n","If you want to create a dataset from a private repository, you can do so by clicking on the “Create Dataset from Repository” button on the Datasets page. This will open a modal where you can select the repository you want to create a dataset from.\n","\n","If you want to create a dataset from a private repository, you can do so by clicking on the “Create Dataset from Repository” button on the Datasets page. This will open a modal where you can select the repository you want to create a dataset from.\n","\n","If you want to create a dataset from a private repository, you can do so by clicking on the"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["category = \"datasets\"\n","question = \"What are the steps to create a Kaggle dataset?\"\n","gemma_qa.query(category,question)"]},{"cell_type":"markdown","id":"fdbd0808","metadata":{"papermill":{"duration":0.06505,"end_time":"2024-09-08T18:06:49.714944","exception":false,"start_time":"2024-09-08T18:06:49.649894","status":"completed"},"tags":[]},"source":["# Save the model"]},{"cell_type":"code","execution_count":23,"id":"83da4e20","metadata":{"execution":{"iopub.execute_input":"2024-09-08T18:06:49.845805Z","iopub.status.busy":"2024-09-08T18:06:49.844939Z","iopub.status.idle":"2024-09-08T18:07:28.922846Z","shell.execute_reply":"2024-09-08T18:07:28.92188Z"},"papermill":{"duration":39.145531,"end_time":"2024-09-08T18:07:28.925307","exception":false,"start_time":"2024-09-08T18:06:49.779776","status":"completed"},"tags":[]},"outputs":[],"source":["preset_dir = \".\\gemma2_2b_en_kaggle_docs\"\n","gemma_causal_lm.save_to_preset(preset_dir)"]},{"cell_type":"markdown","id":"0f94f20f","metadata":{"papermill":{"duration":0.102674,"end_time":"2024-09-08T18:07:29.870404","exception":false,"start_time":"2024-09-08T18:07:29.76773","status":"completed"},"tags":[]},"source":["# Publish Model on Kaggle as a Kaggle Model\n","\n","We are publishing now the saved model as a Kaggle Model."]},{"cell_type":"code","execution_count":24,"id":"42affe31","metadata":{"execution":{"iopub.execute_input":"2024-09-08T18:07:30.065861Z","iopub.status.busy":"2024-09-08T18:07:30.064844Z","iopub.status.idle":"2024-09-08T18:08:49.108065Z","shell.execute_reply":"2024-09-08T18:08:49.107072Z"},"papermill":{"duration":79.146667,"end_time":"2024-09-08T18:08:49.110196","exception":false,"start_time":"2024-09-08T18:07:29.963529","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Uploading Model https://www.kaggle.com/models/gpreda/gemma2-finetuned-kaggle-doc/keras/1 ...\n","Model 'gemma2-finetuned-kaggle-doc' does not exist or access is forbidden for user 'gpreda'. Creating or handling Model...\n","Model 'gemma2-finetuned-kaggle-doc' Created.\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/preprocessor.json\n"]},{"name":"stderr","output_type":"stream","text":["Uploading: 100%|██████████| 1.25k/1.25k [00:00<00:00, 6.64kB/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/preprocessor.json (1KB)\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/task.json\n"]},{"name":"stderr","output_type":"stream","text":["\n","Uploading: 100%|██████████| 2.78k/2.78k [00:00<00:00, 15.9kB/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/task.json (3KB)\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/config.json\n"]},{"name":"stderr","output_type":"stream","text":["\n","Uploading: 100%|██████████| 782/782 [00:00<00:00, 4.49kB/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/config.json (782B)\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/metadata.json\n"]},{"name":"stderr","output_type":"stream","text":["\n","Uploading: 100%|██████████| 143/143 [00:00<00:00, 724B/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/metadata.json (143B)\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/model.weights.h5\n"]},{"name":"stderr","output_type":"stream","text":["Uploading: 100%|██████████| 10.5G/10.5G [01:13<00:00, 143MB/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/model.weights.h5 (10GB)\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/tokenizer.json\n"]},{"name":"stderr","output_type":"stream","text":["\n","Uploading: 100%|██████████| 498/498 [00:00<00:00, 2.64kB/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/tokenizer.json (498B)\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/assets/tokenizer/vocabulary.spm\n"]},{"name":"stderr","output_type":"stream","text":["\n","Uploading: 100%|██████████| 4.24M/4.24M [00:00<00:00, 18.5MB/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/assets/tokenizer/vocabulary.spm (4MB)\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Your model instance has been created.\n","Files are being processed...\n","See at: https://www.kaggle.com/models/gpreda/gemma2-finetuned-kaggle-doc/keras/1\n"]}],"source":["kaggle_username = os.environ[\"KAGGLE_USERNAME\"]\n","\n","kaggle_uri = f\"kaggle://{kaggle_username}/gemma2-finetuned-kaggle-doc/keras/1\"\n","keras_nlp.upload_preset(kaggle_uri, preset_dir)"]},{"cell_type":"markdown","id":"16559946","metadata":{"papermill":{"duration":0.117232,"end_time":"2024-09-08T18:08:49.344422","exception":false,"start_time":"2024-09-08T18:08:49.22719","status":"completed"},"tags":[]},"source":["# Conclusions\n","\n"]},{"cell_type":"markdown","id":"f99d4bfb","metadata":{"papermill":{"duration":0.116372,"end_time":"2024-09-08T18:08:49.577508","exception":false,"start_time":"2024-09-08T18:08:49.461136","status":"completed"},"tags":[]},"source":["We demonstated how to fine-tune a **Gemma 2** model using LoRA.   \n","We also created a class to run queries to the **Gemma 2** model and tested it with some examples from the existing training data but also with some new, not seen questions.   \n","We also saved the models as a Keras model. \n","Then we published the model as a Kaggle Model on Kaggle Models platform."]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4484051,"sourceId":7711309,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":78150,"modelInstanceId":72244,"sourceId":85984,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":937.680179,"end_time":"2024-09-08T18:08:53.642245","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-09-08T17:53:15.962066","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}