{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7711309,"sourceType":"datasetVersion","datasetId":4484051},{"sourceId":85984,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72244,"modelId":78150}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras?scriptVersionId=193563655\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<center><h1>Fine-tuning Gemma 2 model using LoRA and Keras</h1></center>\n\n<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n\n\n# Introduction\n\nThis notebook will demonstrate three things:\n\n1. How to fine-tune Gemma model using LoRA\n2. Creation of a specialised class to query about Kaggle features\n3. Some results of querying about Kaggle Docs\n\nThis work is largely based on previous work. Here I list the sources:\n\n1. Gemma 2 Model Card, Kaggle Models,https://www.kaggle.com/models/google/gemma-2/\n2. Kaggle QA with Gemma - KerasNLP Starter, Kaggle Code, https://www.kaggle.com/code/awsaf49/kaggle-qa-with-gemma-kerasnlp-starter (Version 11)  \n3. Fine-tune Gemma models in Keras using LoRA, Kaggle Code, https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora (Version 1) \n4. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, LoRA: Low-Rank Adaptation of Large Language Models, ArXiv, https://arxiv.org/pdf/2106.09685.pdf\n5. Abheesht Sharma, Matthew Watson, Parameter-efficient fine-tuning of GPT-2 with LoRA, https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/\n6. Keras 3 API documentation / KerasNLP / Models / Gemma, https://keras.io/api/keras_nlp/models/gemma/\n7. Unlock the Power of Gemma 2: Prompt it like a Pro, https://www.kaggle.com/code/gpreda/unlock-the-power-of-gemma-2-prompt-it-like-a-pro  \n8. Fine-tune Gemma using LoRA and Keras, https://www.kaggle.com/code/gpreda/fine-tune-gemma-using-lora-and-keras\n9. Fine-tunning Gemma model with Kaggle Docs data, https://www.kaggle.com/code/gpreda/fine-tunning-gemma-model-with-kaggle-docs-data\n10. Kaggle Docs, Kaggle Dataset, https://www.kaggle.com/datasets/awsaf49/kaggle-docs  \n\n\n**Let's go**!\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# What is Gemma 2?\n\nGemma is a collection of lightweight, advanced open models developed by Google, leveraging the same research and technology behind the Gemini models. These models are text-to-text, decoder-only large language models available in English, with open weights provided for both pre-trained and instruction-tuned versions. Gemma models excel in a range of text generation tasks, such as question answering, summarization, and reasoning. Their compact size allows for deployment in resource-constrained environments like laptops, desktops, or personal cloud infrastructure, making state-of-the-art AI models more accessible and encouraging innovation for all. \n\nGemma 2 represent the 2nd generation of Gemma models. These models were trained on a dataset of text data that includes a wide variety of sources. The **27B** model was trained with **13 trillion** tokens, the **9B** model was trained with **8 trillion tokens**, and **2B** model was trained with **2 trillion** tokens. Here is a summary of their key components: \n* **Web Documents**: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content.\n* **Code**: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions.\n* **Mathematics**: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.\n\nTo learn more about Gemma 2, follow this link: [Gemma 2 Model Card](https://www.kaggle.com/models/google/gemma-2).\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# What is LoRA?  \n\n**LoRA** stands for **Low-Rank Adaptation**. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to **LoRA** paper, this number decreases **10,000 times**, and the computational resources size decreases 3 times. ","metadata":{}},{"cell_type":"markdown","source":"# How we proceed?\n\nFor fine-tunning with LoRA, we will follow the steps:\n\n1. Install prerequisites\n2. Load and process the data for fine-tuning\n3. Initialize the code for Gemma causal language model (Gemma Causal LM)\n4. Perform fine-tuning\n5. Test the fine-tunned model with questions from the data used for fine-tuning and with aditional questions","metadata":{}},{"cell_type":"markdown","source":"# Prerequisites\n\n\n## Install packages\n\nWe start by installing `keras-nlp` and `keras` packages.","metadata":{}},{"cell_type":"code","source":"# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n!pip install -q -U keras-nlp\n!pip install -q -U keras>=3","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-22T08:20:18.198867Z","iopub.execute_input":"2024-08-22T08:20:18.199223Z","iopub.status.idle":"2024-08-22T08:20:49.361234Z","shell.execute_reply.started":"2024-08-22T08:20:18.199198Z","shell.execute_reply":"2024-08-22T08:20:49.360257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import packages\n\nNow we can import the packages we just installed. We will also install `os`, so that we can set the environment variables needed for keras backend. We will use `jax` as `KERAS_BACKEND`.","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\nos.environ[\"JAX_PLATFORMS\"] = \"\"\nimport keras\nimport keras_nlp\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\ntqdm.pandas() # progress bar for pandas\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, Markdown","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-22T08:20:49.363289Z","iopub.execute_input":"2024-08-22T08:20:49.363604Z","iopub.status.idle":"2024-08-22T08:21:04.344534Z","shell.execute_reply.started":"2024-08-22T08:20:49.363578Z","shell.execute_reply":"2024-08-22T08:21:04.343551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configurations\n\n\nWe use a `Config` class to group the information needed to control the fine-tuning process:\n* random seed \n* dataset path\n* preset - name of pretrained Gemma 2\n* sequence length - this is the maximum size of input sequence for training\n* batch size - size of the input batch in training, x 2 as two GPUs\n* lora rank - rank for LoRA, higher means more trainable parameters \n* learning rate used in the train\n* epochs - number of epochs for train","metadata":{}},{"cell_type":"code","source":"class Config:\n    seed = 42\n    dataset_path = \"/kaggle/input/kaggle-docs/questions_answers\"\n    preset = \"gemma2_2b_en\" # name of pretrained Gemma 2\n    sequence_length = 512 # max size of input sequence for training\n    batch_size = 1 # size of the input batch in training\n    lora_rank = 3 # rank for LoRA, higher means more trainable parameters\n    learning_rate=8e-5 # learning rate used in train\n    epochs = 10 # number of epochs to train","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:21:04.34577Z","iopub.execute_input":"2024-08-22T08:21:04.34641Z","iopub.status.idle":"2024-08-22T08:21:04.35426Z","shell.execute_reply.started":"2024-08-22T08:21:04.346382Z","shell.execute_reply":"2024-08-22T08:21:04.3534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set a random seed for results reproducibility.","metadata":{}},{"cell_type":"code","source":"keras.utils.set_random_seed(Config.seed)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:21:04.357052Z","iopub.execute_input":"2024-08-22T08:21:04.357431Z","iopub.status.idle":"2024-08-22T08:21:04.395919Z","shell.execute_reply.started":"2024-08-22T08:21:04.357398Z","shell.execute_reply":"2024-08-22T08:21:04.395132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the data\n\n\nWe load the data we will use for fine-tunining.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(f\"{Config.dataset_path}/data.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:21:04.396974Z","iopub.execute_input":"2024-08-22T08:21:04.397272Z","iopub.status.idle":"2024-08-22T08:21:04.446928Z","shell.execute_reply.started":"2024-08-22T08:21:04.397249Z","shell.execute_reply":"2024-08-22T08:21:04.445997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the total number of rows in this dataset.","metadata":{}},{"cell_type":"code","source":"df.shape[0]","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:21:04.448598Z","iopub.execute_input":"2024-08-22T08:21:04.448888Z","iopub.status.idle":"2024-08-22T08:21:04.455278Z","shell.execute_reply.started":"2024-08-22T08:21:04.448865Z","shell.execute_reply":"2024-08-22T08:21:04.454245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For easiness, we will create the following template for QA: ","metadata":{}},{"cell_type":"code","source":"template = \"\\n\\nCategory:\\nkaggle-{Category}\\n\\nQuestion:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\ndf[\"prompt\"] = df.apply(lambda row: template.format(Category=row.Category,\n                                                             Question=row.Question,\n                                                             Answer=row.Answer), axis=1)\ndata = df.prompt.tolist()","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:21:04.456516Z","iopub.execute_input":"2024-08-22T08:21:04.456847Z","iopub.status.idle":"2024-08-22T08:21:04.469795Z","shell.execute_reply.started":"2024-08-22T08:21:04.456824Z","shell.execute_reply":"2024-08-22T08:21:04.468837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Template utility function","metadata":{}},{"cell_type":"code","source":"def colorize_text(text):\n    for word, color in zip([\"Category\", \"Question\", \"Answer\"], [\"blue\", \"red\", \"green\"]):\n        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:21:04.471012Z","iopub.execute_input":"2024-08-22T08:21:04.471393Z","iopub.status.idle":"2024-08-22T08:21:04.480467Z","shell.execute_reply.started":"2024-08-22T08:21:04.471321Z","shell.execute_reply":"2024-08-22T08:21:04.479607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Specialized class to query Gemma\n\n\nWe define a specialized class to query Gemma. But first, we need to initialize an object of GemmaCausalLM class.","metadata":{}},{"cell_type":"markdown","source":"## Initialize the code for Gemma Causal LM","metadata":{}},{"cell_type":"code","source":"gemma_causal_lm = keras_nlp.models.GemmaCausalLM.from_preset(Config.preset)\ngemma_causal_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:21:04.481879Z","iopub.execute_input":"2024-08-22T08:21:04.482177Z","iopub.status.idle":"2024-08-22T08:22:14.341001Z","shell.execute_reply.started":"2024-08-22T08:21:04.482142Z","shell.execute_reply":"2024-08-22T08:22:14.340094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the specialized class\n\nHere we define the special class `GemmaQA`. \nin the `__init__` we pass the `GemmaCausalLM` object created before.\nThe `query` member function uses `GemmaCausalLM` member function `generate` to generate the answer, based on a prompt that includes the category and the question.","metadata":{}},{"cell_type":"code","source":"class GemmaQA:\n    def __init__(self, max_length=512):\n        self.max_length = max_length\n        self.prompt = template\n        self.gemma_causal_lm = gemma_causal_lm\n        \n    def query(self, category, question):\n        response = self.gemma_causal_lm.generate(\n            self.prompt.format(\n                Category=category,\n                Question=question,\n                Answer=\"\"), \n            max_length=self.max_length)\n        display(Markdown(colorize_text(response)))\n        ","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:22:14.344026Z","iopub.execute_input":"2024-08-22T08:22:14.344373Z","iopub.status.idle":"2024-08-22T08:22:14.35032Z","shell.execute_reply.started":"2024-08-22T08:22:14.344348Z","shell.execute_reply":"2024-08-22T08:22:14.349423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gemma preprocessor\n\n\nThis preprocessing layer will take in batches of strings, and return outputs in a ```(x, y, sample_weight)``` format, where the y label is the next token id in the x sequence.\n\nFrom the code below, we can see that, after the preprocessor, the data shape is ```(num_samples, sequence_length)```.","metadata":{}},{"cell_type":"code","source":"x, y, sample_weight = gemma_causal_lm.preprocessor(data[0:2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Perform fine-tuning with LoRA","metadata":{}},{"cell_type":"markdown","source":"## Enable LoRA for the model\n\nLoRA rank is setting the number of trainable parameters. A larger rank will result in a larger number of parameters to train.","metadata":{}},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to the lora_rank as set in Config (4).\ngemma_causal_lm.backbone.enable_lora(rank=Config.lora_rank)\ngemma_causal_lm.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that only a small part of the parameters are trainable. 2.6 billions parameters total, and only 2.9 Millions parameters trainable.","metadata":{}},{"cell_type":"markdown","source":"## Run the training sequence\n\nWe set the `sequence_length` for the `GemmaCausalLM` (from configuration, will be 512).\nWe compile the model, with the loss, optimizer and metric.\nFor the metric, it is used `SparseCategoricalAccuracy`. This metric calculates how often predictions match integer labels.","metadata":{}},{"cell_type":"code","source":"#set sequence length cf. config (512)\ngemma_causal_lm.preprocessor.sequence_length = Config.sequence_length \n\n# Compile the model with loss, optimizer, and metric\ngemma_causal_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.Adam(learning_rate=Config.learning_rate),\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\n# Train model\ngemma_causal_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:23:30.434619Z","iopub.execute_input":"2024-08-22T08:23:30.435358Z","iopub.status.idle":"2024-08-22T08:23:47.078885Z","shell.execute_reply.started":"2024-08-22T08:23:30.435326Z","shell.execute_reply":"2024-08-22T08:23:47.077535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test the fine-tuned model\n\nWe instantiate an object of class GemmaQA. Because `gemma_causal_lm` was fine-tuned using LoRA, `gemma_qa` defined here will use the fine-tuned model.","metadata":{}},{"cell_type":"code","source":"gemma_qa = GemmaQA()","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:00:56.368022Z","iopub.status.idle":"2024-08-22T08:00:56.368463Z","shell.execute_reply.started":"2024-08-22T08:00:56.368234Z","shell.execute_reply":"2024-08-22T08:00:56.368252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For start, we are testing the model with some of the data from the training set itself.","metadata":{}},{"cell_type":"markdown","source":"## Sample 1","metadata":{}},{"cell_type":"code","source":"row = df.iloc[0]\ngemma_qa.query(row.Category,row.Question)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:00:56.369386Z","iopub.status.idle":"2024-08-22T08:00:56.369846Z","shell.execute_reply.started":"2024-08-22T08:00:56.369589Z","shell.execute_reply":"2024-08-22T08:00:56.369608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample 2","metadata":{}},{"cell_type":"code","source":"row = df.iloc[15]\ngemma_qa.query(row.Category,row.Question)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:00:56.371232Z","iopub.status.idle":"2024-08-22T08:00:56.371554Z","shell.execute_reply.started":"2024-08-22T08:00:56.371385Z","shell.execute_reply":"2024-08-22T08:00:56.371398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample 3","metadata":{}},{"cell_type":"code","source":"row = df.iloc[25]\ngemma_qa.query(row.Category,row.Question)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:00:56.372905Z","iopub.status.idle":"2024-08-22T08:00:56.373244Z","shell.execute_reply.started":"2024-08-22T08:00:56.373079Z","shell.execute_reply":"2024-08-22T08:00:56.373093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Not seen question(s)","metadata":{}},{"cell_type":"code","source":"category = \"notebook\"\nquestion = \"How to run a notebook?\"\ngemma_qa.query(category,question)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:00:56.374588Z","iopub.status.idle":"2024-08-22T08:00:56.374925Z","shell.execute_reply.started":"2024-08-22T08:00:56.374739Z","shell.execute_reply":"2024-08-22T08:00:56.374751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"category = \"discussions\"\nquestion = \"How to create a discussion topic?\"\ngemma_qa.query(category,question)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:00:56.376331Z","iopub.status.idle":"2024-08-22T08:00:56.3767Z","shell.execute_reply.started":"2024-08-22T08:00:56.376515Z","shell.execute_reply":"2024-08-22T08:00:56.37653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"category = \"competitions\"\nquestion = \"What is a code competition?\"\ngemma_qa.query(category,question)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:00:56.378277Z","iopub.status.idle":"2024-08-22T08:00:56.378605Z","shell.execute_reply.started":"2024-08-22T08:00:56.378443Z","shell.execute_reply":"2024-08-22T08:00:56.378456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"category = \"datasets\"\nquestion = \"What are the steps to create a Kaggle dataset?\"\ngemma_qa.query(category,question)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:00:56.379761Z","iopub.status.idle":"2024-08-22T08:00:56.380124Z","shell.execute_reply.started":"2024-08-22T08:00:56.379949Z","shell.execute_reply":"2024-08-22T08:00:56.379963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save the model","metadata":{}},{"cell_type":"code","source":"gemma_causal_lm.save(\"gemma2_2b_en_kaggle_docs.keras\")","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:00:56.381358Z","iopub.status.idle":"2024-08-22T08:00:56.381681Z","shell.execute_reply.started":"2024-08-22T08:00:56.381521Z","shell.execute_reply":"2024-08-22T08:00:56.381535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions\n\n","metadata":{}},{"cell_type":"markdown","source":"We demonstated how to fine-tune a **Gemma 2** model using LoRA.   \nWe also created a class to run queries to the **Gemma 2** model and tested it with some examples from the existing training data but also with some new, not seen questions.   \nWe also saved the models as a Keras model.","metadata":{}}]}