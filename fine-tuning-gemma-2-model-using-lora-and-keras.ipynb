{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras?scriptVersionId=205422280\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"81ce70ac","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.011995,"end_time":"2024-11-05T16:50:40.19782","exception":false,"start_time":"2024-11-05T16:50:40.185825","status":"completed"},"tags":[]},"source":["<center><h1>Fine-tuning Gemma 2 model using LoRA and Keras</h1></center>\n","\n","<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n","\n","\n","# Introduction\n","\n","This notebook will demonstrate three things:\n","\n","1. How to fine-tune Gemma model using LoRA\n","2. Creation of a specialised class to query about Kaggle features\n","3. Some results of querying about Kaggle Docs\n","\n","This work is largely based on previous work. Here I list the sources:\n","\n","1. Gemma 2 Model Card, Kaggle Models,https://www.kaggle.com/models/google/gemma-2/\n","2. Kaggle QA with Gemma - KerasNLP Starter, Kaggle Code, https://www.kaggle.com/code/awsaf49/kaggle-qa-with-gemma-kerasnlp-starter (Version 11)  \n","3. Fine-tune Gemma models in Keras using LoRA, Kaggle Code, https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora (Version 1) \n","4. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, LoRA: Low-Rank Adaptation of Large Language Models, ArXiv, https://arxiv.org/pdf/2106.09685.pdf\n","5. Abheesht Sharma, Matthew Watson, Parameter-efficient fine-tuning of GPT-2 with LoRA, https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/\n","6. Keras 3 API documentation / KerasNLP / Models / Gemma, https://keras.io/api/keras_nlp/models/gemma/\n","7. Unlock the Power of Gemma 2: Prompt it like a Pro, https://www.kaggle.com/code/gpreda/unlock-the-power-of-gemma-2-prompt-it-like-a-pro  \n","8. Fine-tune Gemma using LoRA and Keras, https://www.kaggle.com/code/gpreda/fine-tune-gemma-using-lora-and-keras\n","9. Fine-tunning Gemma model with Kaggle Docs data, https://www.kaggle.com/code/gpreda/fine-tunning-gemma-model-with-kaggle-docs-data\n","10. Kaggle Docs, Kaggle Dataset, https://www.kaggle.com/datasets/awsaf49/kaggle-docs  \n","\n","\n","**Let's go**!\n"]},{"cell_type":"markdown","id":"4e20aaeb","metadata":{"papermill":{"duration":0.010998,"end_time":"2024-11-05T16:50:40.220627","exception":false,"start_time":"2024-11-05T16:50:40.209629","status":"completed"},"tags":[]},"source":["# What is Gemma 2?\n","\n","Gemma is a collection of lightweight, advanced open models developed by Google, leveraging the same research and technology behind the Gemini models. These models are text-to-text, decoder-only large language models available in English, with open weights provided for both pre-trained and instruction-tuned versions. Gemma models excel in a range of text generation tasks, such as question answering, summarization, and reasoning. Their compact size allows for deployment in resource-constrained environments like laptops, desktops, or personal cloud infrastructure, making state-of-the-art AI models more accessible and encouraging innovation for all. \n","\n","Gemma 2 represent the 2nd generation of Gemma models. These models were trained on a dataset of text data that includes a wide variety of sources. The **27B** model was trained with **13 trillion** tokens, the **9B** model was trained with **8 trillion tokens**, and **2B** model was trained with **2 trillion** tokens. Here is a summary of their key components: \n","* **Web Documents**: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content.\n","* **Code**: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions.\n","* **Mathematics**: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.\n","\n","To learn more about Gemma 2, follow this link: [Gemma 2 Model Card](https://www.kaggle.com/models/google/gemma-2).\n","\n","\n"]},{"cell_type":"markdown","id":"e03254d2","metadata":{"papermill":{"duration":0.011307,"end_time":"2024-11-05T16:50:40.243317","exception":false,"start_time":"2024-11-05T16:50:40.23201","status":"completed"},"tags":[]},"source":["# What is LoRA?  \n","\n","**LoRA** stands for **Low-Rank Adaptation**. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to **LoRA** paper, this number decreases **10,000 times**, and the computational resources size decreases 3 times. "]},{"cell_type":"markdown","id":"ce7bd9c2","metadata":{"papermill":{"duration":0.011025,"end_time":"2024-11-05T16:50:40.266575","exception":false,"start_time":"2024-11-05T16:50:40.25555","status":"completed"},"tags":[]},"source":["# How we proceed?\n","\n","For fine-tunning with LoRA, we will follow the steps:\n","\n","1. Install prerequisites\n","2. Load and process the data for fine-tuning\n","3. Initialize the code for Gemma causal language model (Gemma Causal LM)\n","4. Perform fine-tuning\n","5. Test the fine-tunned model with questions from the data used for fine-tuning and with aditional questions"]},{"cell_type":"markdown","id":"923f4494","metadata":{"papermill":{"duration":0.011194,"end_time":"2024-11-05T16:50:40.288946","exception":false,"start_time":"2024-11-05T16:50:40.277752","status":"completed"},"tags":[]},"source":["# Prerequisites\n","\n","\n","## Install packages\n","\n","We start by installing `keras-nlp` and `keras` packages."]},{"cell_type":"code","execution_count":1,"id":"f4b39495","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-11-05T16:50:40.313458Z","iopub.status.busy":"2024-11-05T16:50:40.312591Z","iopub.status.idle":"2024-11-05T16:51:23.909794Z","shell.execute_reply":"2024-11-05T16:51:23.908604Z"},"papermill":{"duration":43.612272,"end_time":"2024-11-05T16:51:23.912314","exception":false,"start_time":"2024-11-05T16:50:40.300042","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\r\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.6.0 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","keras-cv 0.8.2 requires keras-core, which is not installed.\u001b[0m\u001b[31m\r\n","\u001b[0m"]}],"source":["# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n","!pip install -q -U keras-nlp\n","!pip install -q -U keras>=3\n","!pip install -q -U kagglehub --upgrade"]},{"cell_type":"markdown","id":"7c40923e","metadata":{"papermill":{"duration":0.011777,"end_time":"2024-11-05T16:51:23.936431","exception":false,"start_time":"2024-11-05T16:51:23.924654","status":"completed"},"tags":[]},"source":["## Import packages\n","\n","Now we can import the packages we just installed. We will also install `os`, so that we can set the environment variables needed for keras backend. We will use `jax` as `KERAS_BACKEND`.\n","\n","Because we want to publish the Model from the Notebook, we also include `kagglehub` and import secrets from `Kaggle App`."]},{"cell_type":"code","execution_count":2,"id":"8c3c04d7","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-11-05T16:51:23.961837Z","iopub.status.busy":"2024-11-05T16:51:23.96153Z","iopub.status.idle":"2024-11-05T16:51:38.812339Z","shell.execute_reply":"2024-11-05T16:51:38.811343Z"},"papermill":{"duration":14.866296,"end_time":"2024-11-05T16:51:38.814846","exception":false,"start_time":"2024-11-05T16:51:23.94855","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-11-05 16:51:28.352361: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-11-05 16:51:28.352464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-11-05 16:51:28.485385: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import os\n","os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n","os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n","os.environ[\"JAX_PLATFORMS\"] = \"\"\n","import keras\n","import keras_nlp\n","import kagglehub\n","\n","\n","from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","os.environ[\"KAGGLE_USERNAME\"] = user_secrets.get_secret(\"kaggle_username\")\n","os.environ[\"KAGGLE_KEY\"] = user_secrets.get_secret(\"kaggle_key\")\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","tqdm.pandas() # progress bar for pandas\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from IPython.display import display, Markdown"]},{"cell_type":"markdown","id":"dd23bc56","metadata":{"papermill":{"duration":0.011473,"end_time":"2024-11-05T16:51:38.838373","exception":false,"start_time":"2024-11-05T16:51:38.8269","status":"completed"},"tags":[]},"source":["## Configurations\n","\n","\n","We use a `Config` class to group the information needed to control the fine-tuning process:\n","* random seed \n","* dataset path\n","* preset - name of pretrained Gemma 2\n","* sequence length - this is the maximum size of input sequence for training\n","* batch size - size of the input batch in training, x 2 as two GPUs\n","* lora rank - rank for LoRA, higher means more trainable parameters \n","* learning rate used in the train\n","* epochs - number of epochs for train"]},{"cell_type":"code","execution_count":3,"id":"ba0ef4c2","metadata":{"execution":{"iopub.execute_input":"2024-11-05T16:51:38.863256Z","iopub.status.busy":"2024-11-05T16:51:38.862282Z","iopub.status.idle":"2024-11-05T16:51:38.867567Z","shell.execute_reply":"2024-11-05T16:51:38.866772Z"},"papermill":{"duration":0.019502,"end_time":"2024-11-05T16:51:38.869371","exception":false,"start_time":"2024-11-05T16:51:38.849869","status":"completed"},"tags":[]},"outputs":[],"source":["class Config:\n","    seed = 42\n","    dataset_path = \"/kaggle/input/kaggle-docs/questions_answers\"\n","    preset = \"gemma2_2b_en\" # name of pretrained Gemma 2\n","    sequence_length = 512 # max size of input sequence for training\n","    batch_size = 1 # size of the input batch in training\n","    lora_rank = 5 # rank for LoRA, higher means more trainable parameters\n","    learning_rate=8e-5 # learning rate used in train\n","    epochs = 15 # number of epochs to train"]},{"cell_type":"markdown","id":"479781de","metadata":{"papermill":{"duration":0.011394,"end_time":"2024-11-05T16:51:38.892201","exception":false,"start_time":"2024-11-05T16:51:38.880807","status":"completed"},"tags":[]},"source":["Set a random seed for results reproducibility."]},{"cell_type":"code","execution_count":4,"id":"5eb8d9ce","metadata":{"execution":{"iopub.execute_input":"2024-11-05T16:51:38.916212Z","iopub.status.busy":"2024-11-05T16:51:38.915963Z","iopub.status.idle":"2024-11-05T16:51:38.920089Z","shell.execute_reply":"2024-11-05T16:51:38.919295Z"},"papermill":{"duration":0.018242,"end_time":"2024-11-05T16:51:38.921941","exception":false,"start_time":"2024-11-05T16:51:38.903699","status":"completed"},"tags":[]},"outputs":[],"source":["keras.utils.set_random_seed(Config.seed)"]},{"cell_type":"markdown","id":"d7ad8e03","metadata":{"papermill":{"duration":0.011162,"end_time":"2024-11-05T16:51:38.944495","exception":false,"start_time":"2024-11-05T16:51:38.933333","status":"completed"},"tags":[]},"source":["# Load the data\n","\n","\n","We load the data we will use for fine-tunining."]},{"cell_type":"code","execution_count":5,"id":"f159b1bc","metadata":{"execution":{"iopub.execute_input":"2024-11-05T16:51:38.968447Z","iopub.status.busy":"2024-11-05T16:51:38.968167Z","iopub.status.idle":"2024-11-05T16:51:39.006913Z","shell.execute_reply":"2024-11-05T16:51:39.006072Z"},"papermill":{"duration":0.052859,"end_time":"2024-11-05T16:51:39.008784","exception":false,"start_time":"2024-11-05T16:51:38.955925","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answer</th>\n","      <th>Category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What are the different types of competitions a...</td>\n","      <td># Types of Competitions\\n\\nKaggle Competitions...</td>\n","      <td>competition</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What are the different competition formats on ...</td>\n","      <td>There are handful of different formats competi...</td>\n","      <td>competition</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>How to join a competition?</td>\n","      <td>Before you start, navigate to the [Competition...</td>\n","      <td>competition</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>How to form, manage, and disband teams in a co...</td>\n","      <td>Everyone that competes in a Competition does s...</td>\n","      <td>competition</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>How do I make a submission in a competition?</td>\n","      <td>You will need to submit your model predictions...</td>\n","      <td>competition</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Question  \\\n","0  What are the different types of competitions a...   \n","1  What are the different competition formats on ...   \n","2                         How to join a competition?   \n","3  How to form, manage, and disband teams in a co...   \n","4       How do I make a submission in a competition?   \n","\n","                                              Answer     Category  \n","0  # Types of Competitions\\n\\nKaggle Competitions...  competition  \n","1  There are handful of different formats competi...  competition  \n","2  Before you start, navigate to the [Competition...  competition  \n","3  Everyone that competes in a Competition does s...  competition  \n","4  You will need to submit your model predictions...  competition  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(f\"{Config.dataset_path}/data.csv\")\n","df.head()"]},{"cell_type":"markdown","id":"ff14f56e","metadata":{"papermill":{"duration":0.011737,"end_time":"2024-11-05T16:51:39.033001","exception":false,"start_time":"2024-11-05T16:51:39.021264","status":"completed"},"tags":[]},"source":["Let's check the total number of rows in this dataset."]},{"cell_type":"code","execution_count":6,"id":"9c991f36","metadata":{"execution":{"iopub.execute_input":"2024-11-05T16:51:39.058006Z","iopub.status.busy":"2024-11-05T16:51:39.057289Z","iopub.status.idle":"2024-11-05T16:51:39.062643Z","shell.execute_reply":"2024-11-05T16:51:39.061831Z"},"papermill":{"duration":0.01997,"end_time":"2024-11-05T16:51:39.064615","exception":false,"start_time":"2024-11-05T16:51:39.044645","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["60"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df.shape[0]"]},{"cell_type":"markdown","id":"efac0d71","metadata":{"papermill":{"duration":0.011682,"end_time":"2024-11-05T16:51:39.088702","exception":false,"start_time":"2024-11-05T16:51:39.07702","status":"completed"},"tags":[]},"source":["For easiness, we will create the following template for QA: "]},{"cell_type":"code","execution_count":7,"id":"37a30bc1","metadata":{"execution":{"iopub.execute_input":"2024-11-05T16:51:39.113962Z","iopub.status.busy":"2024-11-05T16:51:39.113683Z","iopub.status.idle":"2024-11-05T16:51:39.123177Z","shell.execute_reply":"2024-11-05T16:51:39.122478Z"},"papermill":{"duration":0.024434,"end_time":"2024-11-05T16:51:39.125023","exception":false,"start_time":"2024-11-05T16:51:39.100589","status":"completed"},"tags":[]},"outputs":[],"source":["template = \"\\n\\nCategory:\\nkaggle-{Category}\\n\\nQuestion:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\n","df[\"prompt\"] = df.apply(lambda row: template.format(Category=row.Category,\n","                                                             Question=row.Question,\n","                                                             Answer=row.Answer), axis=1)\n","data = df.prompt.tolist()"]},{"cell_type":"markdown","id":"5b7b9c89","metadata":{"papermill":{"duration":0.012073,"end_time":"2024-11-05T16:51:39.14925","exception":false,"start_time":"2024-11-05T16:51:39.137177","status":"completed"},"tags":[]},"source":["## Template utility function"]},{"cell_type":"code","execution_count":8,"id":"17ad83db","metadata":{"execution":{"iopub.execute_input":"2024-11-05T16:51:39.181693Z","iopub.status.busy":"2024-11-05T16:51:39.181152Z","iopub.status.idle":"2024-11-05T16:51:39.185944Z","shell.execute_reply":"2024-11-05T16:51:39.185095Z"},"papermill":{"duration":0.020826,"end_time":"2024-11-05T16:51:39.187818","exception":false,"start_time":"2024-11-05T16:51:39.166992","status":"completed"},"tags":[]},"outputs":[],"source":["def colorize_text(text):\n","    for word, color in zip([\"Category\", \"Question\", \"Answer\"], [\"blue\", \"red\", \"green\"]):\n","        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n","    return text"]},{"cell_type":"markdown","id":"59881fff","metadata":{"papermill":{"duration":0.011568,"end_time":"2024-11-05T16:51:39.211245","exception":false,"start_time":"2024-11-05T16:51:39.199677","status":"completed"},"tags":[]},"source":["# Specialized class to query Gemma\n","\n","\n","We define a specialized class to query Gemma. But first, we need to initialize an object of GemmaCausalLM class."]},{"cell_type":"markdown","id":"5fc4f506","metadata":{"papermill":{"duration":0.012024,"end_time":"2024-11-05T16:51:39.273877","exception":false,"start_time":"2024-11-05T16:51:39.261853","status":"completed"},"tags":[]},"source":["## Initialize the code for Gemma Causal LM"]},{"cell_type":"code","execution_count":9,"id":"8334ef58","metadata":{"execution":{"iopub.execute_input":"2024-11-05T16:51:39.299392Z","iopub.status.busy":"2024-11-05T16:51:39.298805Z","iopub.status.idle":"2024-11-05T16:52:28.578006Z","shell.execute_reply":"2024-11-05T16:52:28.577068Z"},"papermill":{"duration":49.294167,"end_time":"2024-11-05T16:52:28.580033","exception":false,"start_time":"2024-11-05T16:51:39.285866","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["gemma_causal_lm = keras_nlp.models.GemmaCausalLM.from_preset(Config.preset)\n","gemma_causal_lm.summary()"]},{"cell_type":"markdown","id":"824a643a","metadata":{"papermill":{"duration":0.013013,"end_time":"2024-11-05T16:52:28.606697","exception":false,"start_time":"2024-11-05T16:52:28.593684","status":"completed"},"tags":[]},"source":["## Define the specialized class\n","\n","Here we define the special class `GemmaQA`. \n","in the `__init__` we pass the `GemmaCausalLM` object created before.\n","The `query` member function uses `GemmaCausalLM` member function `generate` to generate the answer, based on a prompt that includes the category and the question."]},{"cell_type":"code","execution_count":10,"id":"ee26ec12","metadata":{"execution":{"iopub.execute_input":"2024-11-05T16:52:28.634287Z","iopub.status.busy":"2024-11-05T16:52:28.633994Z","iopub.status.idle":"2024-11-05T16:52:28.640145Z","shell.execute_reply":"2024-11-05T16:52:28.639215Z"},"papermill":{"duration":0.022349,"end_time":"2024-11-05T16:52:28.64213","exception":false,"start_time":"2024-11-05T16:52:28.619781","status":"completed"},"tags":[]},"outputs":[],"source":["class GemmaQA:\n","    def __init__(self, max_length=512):\n","        self.max_length = max_length\n","        self.prompt = template\n","        self.gemma_causal_lm = gemma_causal_lm\n","        \n","    def query(self, category, question):\n","        response = self.gemma_causal_lm.generate(\n","            self.prompt.format(\n","                Category=category,\n","                Question=question,\n","                Answer=\"\"), \n","            max_length=self.max_length)\n","        display(Markdown(colorize_text(response)))\n","        "]},{"cell_type":"markdown","id":"9bd06d82","metadata":{"papermill":{"duration":0.012881,"end_time":"2024-11-05T16:52:28.668857","exception":false,"start_time":"2024-11-05T16:52:28.655976","status":"completed"},"tags":[]},"source":["## Gemma preprocessor\n","\n","\n","This preprocessing layer will take in batches of strings, and return outputs in a ```(x, y, sample_weight)``` format, where the y label is the next token id in the x sequence.\n","\n","From the code below, we can see that, after the preprocessor, the data shape is ```(num_samples, sequence_length)```."]},{"cell_type":"code","execution_count":11,"id":"f54a2321","metadata":{"execution":{"iopub.execute_input":"2024-11-05T16:52:28.696214Z","iopub.status.busy":"2024-11-05T16:52:28.695894Z","iopub.status.idle":"2024-11-05T16:52:28.821844Z","shell.execute_reply":"2024-11-05T16:52:28.820895Z"},"papermill":{"duration":0.14225,"end_time":"2024-11-05T16:52:28.824209","exception":false,"start_time":"2024-11-05T16:52:28.681959","status":"completed"},"tags":[]},"outputs":[],"source":["x, y, sample_weight = gemma_causal_lm.preprocessor(data[0:2])"]},{"cell_type":"code","execution_count":12,"id":"644fe9a0","metadata":{"execution":{"iopub.execute_input":"2024-11-05T16:52:28.852672Z","iopub.status.busy":"2024-11-05T16:52:28.852155Z","iopub.status.idle":"2024-11-05T16:52:28.858073Z","shell.execute_reply":"2024-11-05T16:52:28.857208Z"},"papermill":{"duration":0.022055,"end_time":"2024-11-05T16:52:28.86","exception":false,"start_time":"2024-11-05T16:52:28.837945","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["{'token_ids': Array([[     2,    109,   8606, ...,  25688, 235290,  75676],\n","       [     2,    109,   8606, ...,    109,    688,   2299]],      dtype=int32), 'padding_mask': Array([[ True,  True,  True, ...,  True,  True,  True],\n","       [ True,  True,  True, ...,  True,  True,  True]], dtype=bool)} [[   109   8606 235292 ... 235290  75676      1]\n"," [   109   8606 235292 ...    688   2299      1]]\n"]}],"source":["print(x, y)"]},{"cell_type":"markdown","id":"1a57199a","metadata":{"papermill":{"duration":0.01309,"end_time":"2024-11-05T16:52:28.886333","exception":false,"start_time":"2024-11-05T16:52:28.873243","status":"completed"},"tags":[]},"source":["# Perform fine-tuning with LoRA"]},{"cell_type":"markdown","id":"c5076fd4","metadata":{"papermill":{"duration":0.013069,"end_time":"2024-11-05T16:52:28.91261","exception":false,"start_time":"2024-11-05T16:52:28.899541","status":"completed"},"tags":[]},"source":["## Enable LoRA for the model\n","\n","LoRA rank is setting the number of trainable parameters. A larger rank will result in a larger number of parameters to train."]},{"cell_type":"code","execution_count":13,"id":"b5da8bae","metadata":{"execution":{"iopub.execute_input":"2024-11-05T16:52:28.940433Z","iopub.status.busy":"2024-11-05T16:52:28.940189Z","iopub.status.idle":"2024-11-05T16:52:29.434607Z","shell.execute_reply":"2024-11-05T16:52:29.433703Z"},"papermill":{"duration":0.510493,"end_time":"2024-11-05T16:52:29.436492","exception":false,"start_time":"2024-11-05T16:52:28.925999","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,618,002,688</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,618,002,688\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,618,002,688</span> (9.75 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,618,002,688\u001b[0m (9.75 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,660,800</span> (13.96 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,660,800\u001b[0m (13.96 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"]},"metadata":{},"output_type":"display_data"}],"source":["# Enable LoRA for the model and set the LoRA rank to the lora_rank as set in Config (4).\n","gemma_causal_lm.backbone.enable_lora(rank=Config.lora_rank)\n","gemma_causal_lm.summary()"]},{"cell_type":"markdown","id":"86a268a4","metadata":{"papermill":{"duration":0.014422,"end_time":"2024-11-05T16:52:29.465724","exception":false,"start_time":"2024-11-05T16:52:29.451302","status":"completed"},"tags":[]},"source":["We see that only a small part of the parameters are trainable. 2.6 billions parameters total, and only 2.9 Millions parameters trainable."]},{"cell_type":"markdown","id":"ad710ca6","metadata":{"papermill":{"duration":0.014215,"end_time":"2024-11-05T16:52:29.494312","exception":false,"start_time":"2024-11-05T16:52:29.480097","status":"completed"},"tags":[]},"source":["## Run the training sequence\n","\n","We set the `sequence_length` for the `GemmaCausalLM` (from configuration, will be 512).\n","We compile the model, with the loss, optimizer and metric.\n","For the metric, it is used `SparseCategoricalAccuracy`. This metric calculates how often predictions match integer labels."]},{"cell_type":"code","execution_count":14,"id":"3eec166e","metadata":{"execution":{"iopub.execute_input":"2024-11-05T16:52:29.524357Z","iopub.status.busy":"2024-11-05T16:52:29.524075Z","iopub.status.idle":"2024-11-05T17:06:42.931486Z","shell.execute_reply":"2024-11-05T17:06:42.930541Z"},"papermill":{"duration":853.424696,"end_time":"2024-11-05T17:06:42.933474","exception":false,"start_time":"2024-11-05T16:52:29.508778","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/15\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 829ms/step - loss: 1.6834 - sparse_categorical_accuracy: 0.5354\n","Epoch 2/15\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 826ms/step - loss: 1.5878 - sparse_categorical_accuracy: 0.5486\n","Epoch 3/15\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 827ms/step - loss: 1.5177 - sparse_categorical_accuracy: 0.5575\n","Epoch 4/15\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - loss: 1.4742 - sparse_categorical_accuracy: 0.5677\n","Epoch 5/15\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 827ms/step - loss: 1.4314 - sparse_categorical_accuracy: 0.5768\n","Epoch 6/15\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 827ms/step - loss: 1.3843 - sparse_categorical_accuracy: 0.5854\n","Epoch 7/15\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - loss: 1.3279 - sparse_categorical_accuracy: 0.5973\n","Epoch 8/15\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 827ms/step - loss: 1.2613 - sparse_categorical_accuracy: 0.6115\n","Epoch 9/15\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 827ms/step - loss: 1.1836 - sparse_categorical_accuracy: 0.6291\n","Epoch 10/15\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 827ms/step - loss: 1.0991 - sparse_categorical_accuracy: 0.6519\n","Epoch 11/15\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 827ms/step - loss: 1.0191 - sparse_categorical_accuracy: 0.6757\n","Epoch 12/15\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 827ms/step - loss: 0.9281 - sparse_categorical_accuracy: 0.7005\n","Epoch 13/15\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 827ms/step - loss: 0.8552 - sparse_categorical_accuracy: 0.7195\n","Epoch 14/15\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 827ms/step - loss: 0.7910 - sparse_categorical_accuracy: 0.7420\n","Epoch 15/15\n","\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 827ms/step - loss: 0.7444 - sparse_categorical_accuracy: 0.7551\n"]},{"data":{"text/plain":["<keras.src.callbacks.history.History at 0x786a9c134430>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["#set sequence length cf. config (512)\n","gemma_causal_lm.preprocessor.sequence_length = Config.sequence_length \n","\n","# Compile the model with loss, optimizer, and metric\n","gemma_causal_lm.compile(\n","    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer=keras.optimizers.Adam(learning_rate=Config.learning_rate),\n","    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",")\n","\n","# Train model\n","gemma_causal_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)"]},{"cell_type":"markdown","id":"bd8d50cf","metadata":{"papermill":{"duration":0.088342,"end_time":"2024-11-05T17:06:43.10914","exception":false,"start_time":"2024-11-05T17:06:43.020798","status":"completed"},"tags":[]},"source":["# Test the fine-tuned model\n","\n","We instantiate an object of class GemmaQA. Because `gemma_causal_lm` was fine-tuned using LoRA, `gemma_qa` defined here will use the fine-tuned model."]},{"cell_type":"code","execution_count":15,"id":"de8e2e3b","metadata":{"execution":{"iopub.execute_input":"2024-11-05T17:06:43.287583Z","iopub.status.busy":"2024-11-05T17:06:43.286711Z","iopub.status.idle":"2024-11-05T17:06:43.291043Z","shell.execute_reply":"2024-11-05T17:06:43.290155Z"},"papermill":{"duration":0.096156,"end_time":"2024-11-05T17:06:43.292984","exception":false,"start_time":"2024-11-05T17:06:43.196828","status":"completed"},"tags":[]},"outputs":[],"source":["gemma_qa = GemmaQA()"]},{"cell_type":"markdown","id":"28d6d1e8","metadata":{"papermill":{"duration":0.085821,"end_time":"2024-11-05T17:06:43.466416","exception":false,"start_time":"2024-11-05T17:06:43.380595","status":"completed"},"tags":[]},"source":["For start, we are testing the model with some of the data from the training set itself."]},{"cell_type":"markdown","id":"348f4bac","metadata":{"papermill":{"duration":0.088706,"end_time":"2024-11-05T17:06:43.640926","exception":false,"start_time":"2024-11-05T17:06:43.55222","status":"completed"},"tags":[]},"source":["## Sample 1"]},{"cell_type":"code","execution_count":16,"id":"f00c1048","metadata":{"execution":{"iopub.execute_input":"2024-11-05T17:06:43.817047Z","iopub.status.busy":"2024-11-05T17:06:43.816687Z","iopub.status.idle":"2024-11-05T17:07:11.515423Z","shell.execute_reply":"2024-11-05T17:07:11.514535Z"},"papermill":{"duration":27.78992,"end_time":"2024-11-05T17:07:11.517567","exception":false,"start_time":"2024-11-05T17:06:43.727647","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-competition\n","\n","**<font color='red'>Question:</font>**\n","What are the different types of competitions available on Kaggle?\n","\n","**<font color='green'>Answer:</font>**\n","There are two different types of competitions available on Kaggle:\n","\n","# Data-only Competitions\n","Data-only competitions are the most common type of competition on Kaggle. They consist of a dataset and a set of metrics to evaluate. Participants are free to explore the data and use any means necessary to generate an answer. Data-only competitions are a great way to hone your skills and get a feel for the competition format.\n","\n","# Model-only Competitions\n","Model-only competitions are a relatively new type of competition on Kaggle. They consist of a dataset, a set of metrics to evaluate, and a model that participants must beat. Model-only competitions are a more direct way to measure your skills as a modeler."],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["row = df.iloc[0]\n","gemma_qa.query(row.Category,row.Question)"]},{"cell_type":"markdown","id":"d2c44c55","metadata":{"papermill":{"duration":0.086576,"end_time":"2024-11-05T17:07:11.691653","exception":false,"start_time":"2024-11-05T17:07:11.605077","status":"completed"},"tags":[]},"source":["## Sample 2"]},{"cell_type":"code","execution_count":17,"id":"571554f6","metadata":{"execution":{"iopub.execute_input":"2024-11-05T17:07:11.872339Z","iopub.status.busy":"2024-11-05T17:07:11.871689Z","iopub.status.idle":"2024-11-05T17:07:31.273433Z","shell.execute_reply":"2024-11-05T17:07:31.272546Z"},"papermill":{"duration":19.493106,"end_time":"2024-11-05T17:07:31.2754","exception":false,"start_time":"2024-11-05T17:07:11.782294","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-tpu\n","\n","**<font color='red'>Question:</font>**\n","How to load and save model on TPU?\n","\n","**<font color='green'>Answer:</font>**\n","When saving a model on TPU, the model file is saved on the local TPU device. When loading a model, the TPU SDK will automatically look for the model file in the default directory on the TPU host.\n","\n","## Saving a model on TPU\n","\n","```python\n","# Save on TPU\n","model.save('/tmp/model')\n","\n","# Load on TPU\n","print(tf.io.gfile.listdir('/tmp/model'))\n","```\n","\n","## Loading a model on TPU\n","\n","```python\n","# Save on TPU\n","model.save('/tmp/model')\n","\n","# Load on TPU\n","print(tf.io.gfile.listdir('/tmp/model'))\n","```\n","\n","## Tips & Tricks\n","\n","- TPU-based models are typically larger than CPU-based models. It is recommended to save the model on TPU only if you need to load and use the model again.\n","- TPU-based models are typically larger than CPU-based models. It is recommended to save the model on TPU only if you need to load and use the model again.\n","- TPU-based models are typically larger than CPU-based models. It is recommended to save the model on TPU only if you need to load and use the model again.\n","- TPU-based models are typically larger than CPU-based models. It is recommended to save the model on TPU only if you need to load and use the model again.\n","- TPU-based models are typically larger than CPU-based models. It is recommended to save the model on TPU only if you need to load and use the model again.\n","- TPU-based models are typically larger than CPU-based models. It is recommended to save the model on TPU only if you need to load and use the model again.\n","- TPU-based models are typically larger than CPU-based models. It is recommended to save the model on TPU only if you need to load and use the model again.\n","- TPU-based models are typically larger than CPU-based models. It is recommended to save the model on TPU only if you need to load and use the model again.\n","- TPU-based models are typically larger than CPU-based models. It is recommended to save the model on TPU only if you need to load and use the model again.\n","- TPU-based models are typically larger than CPU-based models. It is recommended to save"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["row = df.iloc[15]\n","gemma_qa.query(row.Category,row.Question)"]},{"cell_type":"markdown","id":"74bafa7a","metadata":{"papermill":{"duration":0.08763,"end_time":"2024-11-05T17:07:31.452445","exception":false,"start_time":"2024-11-05T17:07:31.364815","status":"completed"},"tags":[]},"source":["## Sample 3"]},{"cell_type":"code","execution_count":18,"id":"7cada1a2","metadata":{"execution":{"iopub.execute_input":"2024-11-05T17:07:31.630703Z","iopub.status.busy":"2024-11-05T17:07:31.629839Z","iopub.status.idle":"2024-11-05T17:07:38.510287Z","shell.execute_reply":"2024-11-05T17:07:38.509274Z"},"papermill":{"duration":6.971899,"end_time":"2024-11-05T17:07:38.512407","exception":false,"start_time":"2024-11-05T17:07:31.540508","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-noteboook\n","\n","**<font color='red'>Question:</font>**\n","What are the different types of notebooks available on Kaggle?\n","\n","**<font color='green'>Answer:</font>**\n","There are two types of notebooks available on Kaggle:\n","\n","- **Python**: A notebook written in the Python programming language. You can run code snippets and scripts, and use the Jupyter Notebook interface for data exploration and visualizations.\n","\n","- **RMarkdown**: A notebook written in the R programming language. You can run code snippets and scripts, and use the RStudio interface for data exploration and visualizations. RMarkdown notebooks can be rendered as a static HTML document, and you can customize the look and feel of the document with stylesheets and formatting commands.\n","\n","RMarkdown notebooks are the same as RStudio files, with the .Rmd extension. Python notebooks are the same as Jupyter Notebooks, with the .ipynb extension.\n","\n","Both types of notebooks are a great way to learn new skills and share your work with the community."],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["row = df.iloc[25]\n","gemma_qa.query(row.Category,row.Question)"]},{"cell_type":"markdown","id":"9fa4fedb","metadata":{"papermill":{"duration":0.087338,"end_time":"2024-11-05T17:07:38.687612","exception":false,"start_time":"2024-11-05T17:07:38.600274","status":"completed"},"tags":[]},"source":["## Not seen question(s)"]},{"cell_type":"code","execution_count":19,"id":"d869ce50","metadata":{"execution":{"iopub.execute_input":"2024-11-05T17:07:38.86502Z","iopub.status.busy":"2024-11-05T17:07:38.864156Z","iopub.status.idle":"2024-11-05T17:07:58.421053Z","shell.execute_reply":"2024-11-05T17:07:58.420076Z"},"papermill":{"duration":19.648345,"end_time":"2024-11-05T17:07:58.42317","exception":false,"start_time":"2024-11-05T17:07:38.774825","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-notebook\n","\n","**<font color='red'>Question:</font>**\n","How to run a notebook?\n","\n","**<font color='green'>Answer:</font>**\n","When you run a notebook, the code cells execute sequentially. Each line of a code cell is executed before the next one.\n","\n","When you run a notebook, the code execution is paused after each code cell execution. This allows you to inspect the output, debug your code, or execute the next code cell immediately.\n","\n","When you run a notebook, the output is displayed in the console and in the Notebook view.\n","\n","If you want to execute a notebook synchronously, i.e. block the current thread until the notebook execution completes, use the command `Kernel.stop()` at the end of the notebook.\n","\n","If you want to execute a notebook asynchronously, i.e. continue the current thread after the notebook execution completes, use the command `Kernel.flush()` at the end of the notebook.\n","\n","You can also choose between synchronous and asynchronous execution by default in the Notebook editor. Go to Settings > Editor > Execution > Async or Sync Block. This setting pauses execution after each code block, but allows you to resume execution by clicking inside the code cell.\n","\n","When you execute a Notebook, the console output is displayed in real-time. However, the output from Notebook code cells is only updated when the Notebook execution completes.\n","\n","If you want to update the Notebook view immediately, you can flush the kernel asynchronously. This is done by default when you execute a Notebook synchronously (i.e. blocking the current thread until the notebook execution completes) using the `Kernel.flush()` call.\n","\n","If you want to execute a Notebook asynchronously, i.e. continue the current thread after the notebook execution completes, use the command `Kernel.flush()` at the end of the notebook.\n","\n","If you want to execute a Notebook asynchronously and simultaneously, i.e. continue the current thread after the notebook execution completes and update the Notebook view immediately, use the setting \"Async Update\" in the Notebook editor. Go to Settings > Editor > Execution > Async Update.\n","\n","If you want to execute a Notebook asynchronously and simultaneously with a custom speed, go to Settings > Editor > Execution > Async Update > Speed and select a value.\n","\n","If you want to execute a Notebook synchronously and simultaneously with a custom speed, go to Settings > Editor > Execution > Async Block > Speed and select a value.\n","\n","If you want to execute a Notebook synchronously and simultaneously with a custom pause, go to Settings > Editor > Execution > Sync"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["category = \"notebook\"\n","question = \"How to run a notebook?\"\n","gemma_qa.query(category,question)"]},{"cell_type":"code","execution_count":20,"id":"f499f9d2","metadata":{"execution":{"iopub.execute_input":"2024-11-05T17:07:58.599524Z","iopub.status.busy":"2024-11-05T17:07:58.599176Z","iopub.status.idle":"2024-11-05T17:08:06.346158Z","shell.execute_reply":"2024-11-05T17:08:06.345089Z"},"papermill":{"duration":7.837538,"end_time":"2024-11-05T17:08:06.34836","exception":false,"start_time":"2024-11-05T17:07:58.510822","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-discussions\n","\n","**<font color='red'>Question:</font>**\n","How to create a discussion topic?\n","\n","**<font color='green'>Answer:</font>**\n","Discussions is a community forum where users can ask questions, make suggestions, and share ideas. You can create your own topics or reply to existing ones.\n","\n","Discussions are a great way to connect with other data scientists, share your work, and learn new things. They are also a great way to stay up-to-date on the latest in data science.\n","\n","Creating a discussion topic is easy. Just click on the “Start Discussion” button on any data science landing page (e.g., [this one](https://www.kaggle.com/competitions/covid19-us-election-2020)) or on your profile page, select a topic from the provided list, or type away in the “New Post” text box.\n","\n","Discussions are a great way to connect with other data scientists, share your work, and learn new things. They are also a great way to stay up-to"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["category = \"discussions\"\n","question = \"How to create a discussion topic?\"\n","gemma_qa.query(category,question)"]},{"cell_type":"code","execution_count":21,"id":"099a693c","metadata":{"execution":{"iopub.execute_input":"2024-11-05T17:08:06.525951Z","iopub.status.busy":"2024-11-05T17:08:06.525126Z","iopub.status.idle":"2024-11-05T17:08:26.045002Z","shell.execute_reply":"2024-11-05T17:08:26.044038Z"},"papermill":{"duration":19.610825,"end_time":"2024-11-05T17:08:26.047206","exception":false,"start_time":"2024-11-05T17:08:06.436381","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-competitions\n","\n","**<font color='red'>Question:</font>**\n","What is a code competition?\n","\n","**<font color='green'>Answer:</font>**\n","Code competitions are a great way to hone your skills and stay competitive in the\n","machine learning community. They typically take the form of a leaderboard where\n","participants compete to see who can get the highest score.\n","\n","Some code competitions are open to any machine learning algorithm, while others\n","focus on a specific type of algorithm or problem domain. For example, the\n","TensorFlow Object Detection Challenge is an algorithm-focused code competition\n","while the YouTube Music Lyrics Prediction Challenge is a problem-domain focused\n","code competition.\n","\n","Most code competitions are free to join, but some may require a paid subscription\n","to Kaggle Pro to participate.\n","\n","## Types of code competitions\n","\n","There are two main types of code competitions:\n","\n","### Algorithm-focused code competitions\n","\n","Algorithm-focused code competitions focus solely on the algorithm itself.\n","Participants are typically provided with a dataset and an evaluation metric and\n","the challenge is to find the best algorithm to achieve the highest possible\n","score. These types of competitions are a great way to test and improve your\n","algorithm skills.\n","\n","Some examples of algorithm-focused code competitions include:\n","\n","- TensorFlow Object Detection Challenge\n","- PyTorch Speech Recognition Challenge\n","- TensorFlow Natural Language Processing (NLP) Challenge\n","\n","### Problem-domain-focused code competitions\n","\n","Problem-domain-focused code competitions focus on a specific industry or domain\n","of machine learning. Participants are typically provided with a dataset and a\n","specific evaluation metric and the challenge is to find the best algorithm to\n","achieve the highest possible score. These types of competitions are a great way\n","to apply your algorithm skills in a real-world setting.\n","\n","Some examples of problem-domain-focused code competitions include:\n","\n","- YouTube Music Lyrics Prediction\n","- Google Smart City Challenge\n","- Kaggle for Good Competition\n","\n","## How to Win\n","\n","### Choose a Competition that Fits You\n","\n","The first step to winning a code competition is to choose one that fits you.\n","There are a variety of different types of code competitions, and each one has\n","its own set of rules and guidelines.\n","\n","Some common types of code competitions include:\n","\n","- Algorithm-focused code competitions: These competitions focus solely on the\n","  algorithm itself. Participants are typically provided with a dataset and an\n","  evaluation metric and the challenge is to find the best algorithm to achieve\n","  the highest possible score.\n","- Problem-domain-focused code competitions: These competitions focus on a\n","  specific industry"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["category = \"competitions\"\n","question = \"What is a code competition?\"\n","gemma_qa.query(category,question)"]},{"cell_type":"code","execution_count":22,"id":"8dd1c2c1","metadata":{"execution":{"iopub.execute_input":"2024-11-05T17:08:26.22715Z","iopub.status.busy":"2024-11-05T17:08:26.226773Z","iopub.status.idle":"2024-11-05T17:08:45.587585Z","shell.execute_reply":"2024-11-05T17:08:45.586705Z"},"papermill":{"duration":19.452562,"end_time":"2024-11-05T17:08:45.589579","exception":false,"start_time":"2024-11-05T17:08:26.137017","status":"completed"},"tags":[]},"outputs":[{"data":{"text/markdown":["\n","\n","**<font color='blue'>Category:</font>**\n","kaggle-datasets\n","\n","**<font color='red'>Question:</font>**\n","What are the steps to create a Kaggle dataset?\n","\n","**<font color='green'>Answer:</font>**\n","## Creating a Dataset\n","\n","1. Create a free account on Kaggle.\n","2. Navigate to the `Datasets` tab and click on the \"Create new dataset\" button.\n","3. Select the \"Public\" option for the visibility.\n","4. Enter a descriptive name for your dataset.\n","5. Choose the data format for your dataset.\n","6. Upload your data. You can upload a CSV or a JSON file. If you are using a JSON file, you will need to configure your dataset to use a `Rows` schema.\n","7. Upload your data and then upload your metadata. The data and metadata files must be in the same folder.\n","8. Upload your data and then upload your metadata. The data and metadata files must be in the same folder.\n","9. If you are using a JSON file, you will need to configure your dataset to use a `Rows` schema.\n","10. If you are using a CSV file, you can optionally upload a sample. The sample will be used by the data exploration tool to help visualize the data.\n","11. If you are using a JSON file, you will need to configure your dataset to use a `Rows` schema.\n","12. If you are using a JSON file, you can optionally configure your dataset to use a `Columns` schema.\n","13. If you are using a JSON file, you can optionally configure your dataset to use a `Rows` schema.\n","14. If you are using a JSON file, you can optionally configure your dataset to use a `Columns` schema.\n","15. If you are using a JSON file, you can optionally configure your dataset to use a `Rows` schema.\n","16. If you are using a JSON file, you can optionally configure your dataset to use a `Columns` schema.\n","17. If you are using a JSON file, you can optionally configure your dataset to use a `Rows` schema.\n","18. If you are using a JSON file, you can optionally configure your dataset to use a `Columns` schema.\n","19. If you are using a JSON file, you can optionally configure your dataset to use a `Rows` schema.\n","20. If you are using a JSON file, you can optionally configure your dataset to use a `Columns` schema.\n","21"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["category = \"datasets\"\n","question = \"What are the steps to create a Kaggle dataset?\"\n","gemma_qa.query(category,question)"]},{"cell_type":"markdown","id":"ea5d303f","metadata":{"papermill":{"duration":0.088145,"end_time":"2024-11-05T17:08:45.767247","exception":false,"start_time":"2024-11-05T17:08:45.679102","status":"completed"},"tags":[]},"source":["# Save the model"]},{"cell_type":"code","execution_count":23,"id":"39e3f4dc","metadata":{"execution":{"iopub.execute_input":"2024-11-05T17:08:45.945812Z","iopub.status.busy":"2024-11-05T17:08:45.945044Z","iopub.status.idle":"2024-11-05T17:09:25.149615Z","shell.execute_reply":"2024-11-05T17:09:25.148728Z"},"papermill":{"duration":39.296654,"end_time":"2024-11-05T17:09:25.152133","exception":false,"start_time":"2024-11-05T17:08:45.855479","status":"completed"},"tags":[]},"outputs":[],"source":["preset_dir = \".\\gemma2_2b_en_kaggle_docs\"\n","gemma_causal_lm.save_to_preset(preset_dir)"]},{"cell_type":"markdown","id":"32094a7d","metadata":{"papermill":{"duration":0.448984,"end_time":"2024-11-05T17:09:26.011836","exception":false,"start_time":"2024-11-05T17:09:25.562852","status":"completed"},"tags":[]},"source":["# Publish Model on Kaggle as a Kaggle Model\n","\n","We are publishing now the saved model as a Kaggle Model."]},{"cell_type":"code","execution_count":24,"id":"45d2e6b7","metadata":{"execution":{"iopub.execute_input":"2024-11-05T17:09:26.297697Z","iopub.status.busy":"2024-11-05T17:09:26.296532Z","iopub.status.idle":"2024-11-05T17:12:31.551109Z","shell.execute_reply":"2024-11-05T17:12:31.550351Z"},"papermill":{"duration":185.400663,"end_time":"2024-11-05T17:12:31.553242","exception":false,"start_time":"2024-11-05T17:09:26.152579","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Uploading Model https://www.kaggle.com/models/gpreda/gemma2-kaggle-docs/keras/gemma2_2b_en_kaggle_docs ...\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/task.json\n"]},{"name":"stderr","output_type":"stream","text":["Uploading: 100%|██████████| 2.98k/2.98k [00:00<00:00, 4.35kB/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/task.json (3KB)\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/config.json\n"]},{"name":"stderr","output_type":"stream","text":["\n","Uploading: 100%|██████████| 782/782 [00:00<00:00, 1.30kB/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/config.json (782B)\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/tokenizer.json\n"]},{"name":"stderr","output_type":"stream","text":["\n","Uploading: 100%|██████████| 591/591 [00:00<00:00, 965B/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/tokenizer.json (591B)\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/model.weights.h5\n"]},{"name":"stderr","output_type":"stream","text":["\n","Uploading: 100%|██████████| 10.5G/10.5G [02:55<00:00, 59.7MB/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/model.weights.h5 (10GB)\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/metadata.json\n"]},{"name":"stderr","output_type":"stream","text":["\n","Uploading: 100%|██████████| 143/143 [00:00<00:00, 232B/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/metadata.json (143B)\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/preprocessor.json\n"]},{"name":"stderr","output_type":"stream","text":["\n","Uploading: 100%|██████████| 1.41k/1.41k [00:00<00:00, 2.20kB/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/preprocessor.json (1KB)\n","Starting upload for file .\\gemma2_2b_en_kaggle_docs/assets/tokenizer/vocabulary.spm\n"]},{"name":"stderr","output_type":"stream","text":["\n","Uploading: 100%|██████████| 4.24M/4.24M [00:00<00:00, 5.58MB/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: .\\gemma2_2b_en_kaggle_docs/assets/tokenizer/vocabulary.spm (4MB)\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Your model instance version has been created.\n","Files are being processed...\n","See at: https://www.kaggle.com/models/gpreda/gemma2-kaggle-docs/keras/gemma2_2b_en_kaggle_docs\n"]}],"source":["kaggle_username = os.environ[\"KAGGLE_USERNAME\"]\n","\n","kaggle_uri = f\"kaggle://{kaggle_username}/gemma2-kaggle-docs/keras/gemma2_2b_en_kaggle_docs\"\n","keras_nlp.upload_preset(kaggle_uri, preset_dir)"]},{"cell_type":"markdown","id":"75b55096","metadata":{"papermill":{"duration":0.178821,"end_time":"2024-11-05T17:12:31.95846","exception":false,"start_time":"2024-11-05T17:12:31.779639","status":"completed"},"tags":[]},"source":["# Conclusions\n","\n"]},{"cell_type":"markdown","id":"4f514e06","metadata":{"papermill":{"duration":0.179248,"end_time":"2024-11-05T17:12:32.3162","exception":false,"start_time":"2024-11-05T17:12:32.136952","status":"completed"},"tags":[]},"source":["We demonstated how to fine-tune a **Gemma 2** model using LoRA.   \n","We also created a class to run queries to the **Gemma 2** model and tested it with some examples from the existing training data but also with some new, not seen questions.   \n","We also saved the models as a Keras model. \n","Then we published the model as a Kaggle Model on Kaggle Models platform."]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4484051,"sourceId":7711309,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":78150,"modelInstanceId":72244,"sourceId":85984,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":1318.851865,"end_time":"2024-11-05T17:12:36.227067","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-11-05T16:50:37.375202","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}