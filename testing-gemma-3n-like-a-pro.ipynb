{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e19fd4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.008151,
     "end_time": "2025-07-03T18:25:54.699066",
     "exception": false,
     "start_time": "2025-07-03T18:25:54.690915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook presents a functional prototype built with Gemma 3n, Google’s latest on-device, multimodal AI model. The goal: use compact, private, offline-ready AI to solve a real-world problem. Full demo, code, and technical details follow.\n",
    "\n",
    "## What are the key features of Gemma 3n?\n",
    "\n",
    "The key features of this new model from Google are:\n",
    "\n",
    "1. On-Device Performance\n",
    "Optimized for mobile and edge devices, Gemma 3n delivers real-time AI with minimal memory usage. The 5B and 8B models run like 2B and 4B models, thanks to innovations like Per-Layer Embeddings (PLE).\n",
    "\n",
    "2. Mix’n’Match Model Scaling\n",
    "A single model can act as multiple: the 4B version includes a 2B submodel, enabling dynamic tradeoffs between performance and efficiency. Developers can also create custom-sized submodels tailored to specific tasks.\n",
    "\n",
    "3. Privacy-First and Offline-Ready\n",
    "Gemma 3n runs entirely on-device, ensuring user data never leaves the device. This makes it ideal for privacy-sensitive applications and for use in low- or no-connectivity environments.\n",
    "\n",
    "4. Multimodal Understanding\n",
    "Supports text, image, audio, and enhanced video input, enabling powerful applications like voice interfaces, transcription, translation, visual recognition, and more—all locally.\n",
    "\n",
    "5. Multilingual Proficiency\n",
    "Strong performance across major global languages including Japanese, German, Korean, Spanish, and French, expanding access and inclusivity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc3c64",
   "metadata": {
    "papermill": {
     "duration": 0.006819,
     "end_time": "2025-07-03T18:25:54.713457",
     "exception": false,
     "start_time": "2025-07-03T18:25:54.706638",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare the model\n",
    "\n",
    "## Install prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99d40784",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-07-03T18:25:54.729416Z",
     "iopub.status.busy": "2025-07-03T18:25:54.728877Z",
     "iopub.status.idle": "2025-07-03T18:28:03.783853Z",
     "shell.execute_reply": "2025-07-03T18:28:03.782708Z"
    },
    "papermill": {
     "duration": 129.065663,
     "end_time": "2025-07-03T18:28:03.786345",
     "exception": false,
     "start_time": "2025-07-03T18:25:54.720682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\r\n",
      "Collecting timm\r\n",
      "  Downloading timm-1.0.16-py3-none-any.whl.metadata (57 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\r\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.31.1)\r\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (25.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.2)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->timm)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->timm)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->timm)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->timm)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->timm)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->timm)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->timm)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.1.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->timm) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->timm) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->timm) (2024.2.0)\r\n",
      "Downloading timm-1.0.16-py3-none-any.whl (2.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\r\n",
      "  Attempting uninstall: timm\r\n",
      "    Found existing installation: timm 1.0.15\r\n",
      "    Uninstalling timm-1.0.15:\r\n",
      "      Successfully uninstalled timm-1.0.15\r\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 timm-1.0.16\r\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\r\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\r\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.31.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\r\n",
      "Collecting git+https://github.com/huggingface/transformers.git\r\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-2z7dep20\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-2z7dep20\r\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 1168f57abffd077d7d2687087aa10ba644a76a0d\r\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (3.18.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (0.31.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (0.21.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (0.5.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (4.67.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.54.0.dev0) (2025.3.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.54.0.dev0) (4.13.2)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.54.0.dev0) (1.1.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.54.0.dev0) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.54.0.dev0) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.54.0.dev0) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.54.0.dev0) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.54.0.dev0) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.54.0.dev0) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.54.0.dev0) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.54.0.dev0) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.54.0.dev0) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.54.0.dev0) (2025.4.26)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.54.0.dev0) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.54.0.dev0) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.54.0.dev0) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.54.0.dev0) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.54.0.dev0) (2024.2.0)\r\n",
      "Building wheels for collected packages: transformers\r\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for transformers: filename=transformers-4.54.0.dev0-py3-none-any.whl size=11778139 sha256=a437c231020b897f1757aef12a581fc651b1d93ca93fe2277de5f68244ae15a5\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-k74q43ur/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\r\n",
      "Successfully built transformers\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.51.3\r\n",
      "    Uninstalling transformers-4.51.3:\r\n",
      "      Successfully uninstalled transformers-4.51.3\r\n",
      "Successfully installed transformers-4.54.0.dev0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install timm --upgrade\n",
    "!pip install accelerate\n",
    "!pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d4d46",
   "metadata": {
    "papermill": {
     "duration": 0.03382,
     "end_time": "2025-07-03T18:28:03.855523",
     "exception": false,
     "start_time": "2025-07-03T18:28:03.821703",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b9a9cf6",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-07-03T18:28:03.924808Z",
     "iopub.status.busy": "2025-07-03T18:28:03.924450Z",
     "iopub.status.idle": "2025-07-03T18:28:37.526671Z",
     "shell.execute_reply": "2025-07-03T18:28:37.525859Z"
    },
    "papermill": {
     "duration": 33.638762,
     "end_time": "2025-07-03T18:28:37.528258",
     "exception": false,
     "start_time": "2025-07-03T18:28:03.889496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 18:28:21.214451: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751567301.433251      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751567301.496922      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import kagglehub\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d08651a",
   "metadata": {
    "papermill": {
     "duration": 0.100694,
     "end_time": "2025-07-03T18:28:37.664563",
     "exception": false,
     "start_time": "2025-07-03T18:28:37.563869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5eeb7e2",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-07-03T18:28:37.735437Z",
     "iopub.status.busy": "2025-07-03T18:28:37.734396Z",
     "iopub.status.idle": "2025-07-03T18:28:52.663171Z",
     "shell.execute_reply": "2025-07-03T18:28:52.662286Z"
    },
    "papermill": {
     "duration": 14.966116,
     "end_time": "2025-07-03T18:28:52.664819",
     "exception": false,
     "start_time": "2025-07-03T18:28:37.698703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dca806c1d7844778dcf108aab730456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GEMMA_PATH = kagglehub.model_download(\"google/gemma-3n/transformers/gemma-3n-e2b-it\")\n",
    "processor = AutoProcessor.from_pretrained(GEMMA_PATH)\n",
    "model = AutoModelForImageTextToText.from_pretrained(GEMMA_PATH, torch_dtype=\"auto\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6695d618",
   "metadata": {
    "papermill": {
     "duration": 0.034093,
     "end_time": "2025-07-03T18:28:52.733403",
     "exception": false,
     "start_time": "2025-07-03T18:28:52.699310",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test the model with a simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4027e368",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:28:52.803917Z",
     "iopub.status.busy": "2025-07-03T18:28:52.803563Z",
     "iopub.status.idle": "2025-07-03T18:29:28.184930Z",
     "shell.execute_reply": "2025-07-03T18:29:28.183932Z"
    },
    "papermill": {
     "duration": 35.452909,
     "end_time": "2025-07-03T18:29:28.220648",
     "exception": false,
     "start_time": "2025-07-03T18:28:52.767739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the France capital?\n",
      "\n",
      "Paris\n",
      "Lyon\n",
      "Marseille\n",
      "Toulouse\n",
      "\n",
      "The correct answer is:\n",
      "\n",
      "**Paris**\n",
      "\n",
      "Paris is the capital and largest city of France\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"What is the France capital?\"\"\"\n",
    "input_ids = processor(text=prompt, \n",
    "                      return_tensors=\"pt\").to(model.device, \n",
    "                                              dtype=model.dtype)\n",
    "\n",
    "outputs = model.generate(**input_ids, \n",
    "                         max_new_tokens=32, \n",
    "                         disable_compile=True)\n",
    "text = processor.batch_decode(\n",
    "    outputs,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5dcc4",
   "metadata": {
    "papermill": {
     "duration": 0.034259,
     "end_time": "2025-07-03T18:29:28.289736",
     "exception": false,
     "start_time": "2025-07-03T18:29:28.255477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's wrap this inside a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9853b23e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:29:28.360193Z",
     "iopub.status.busy": "2025-07-03T18:29:28.359521Z",
     "iopub.status.idle": "2025-07-03T18:29:28.365324Z",
     "shell.execute_reply": "2025-07-03T18:29:28.364378Z"
    },
    "papermill": {
     "duration": 0.042549,
     "end_time": "2025-07-03T18:29:28.366810",
     "exception": false,
     "start_time": "2025-07-03T18:29:28.324261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_model(prompt, max_new_tokens=32):\n",
    "    start_time = time()\n",
    "    input_ids = processor(text=prompt, \n",
    "                          return_tensors=\"pt\").to(model.device, \n",
    "                                                  dtype=model.dtype)\n",
    "    \n",
    "    outputs = model.generate(**input_ids, \n",
    "                             max_new_tokens=max_new_tokens, \n",
    "                             disable_compile=True)\n",
    "    text = processor.batch_decode(\n",
    "        outputs,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    total_time = round(time() - start_time, 2)\n",
    "    response = text[0].split(prompt)[-1]\n",
    "    return response, total_time\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1beb3d47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:29:28.442495Z",
     "iopub.status.busy": "2025-07-03T18:29:28.442101Z",
     "iopub.status.idle": "2025-07-03T18:29:34.789219Z",
     "shell.execute_reply": "2025-07-03T18:29:34.788272Z"
    },
    "papermill": {
     "duration": 6.388845,
     "end_time": "2025-07-03T18:29:34.790765",
     "exception": false,
     "start_time": "2025-07-03T18:29:28.401920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 6.34\n",
      "Question: Quelle est la capitale de la France?\n",
      "Response: \n",
      "\n",
      "La capitale de la France est Paris.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Quelle est la capitale de la France?\"\n",
    "response, total_time = query_model(prompt, max_new_tokens=16)\n",
    "print(f\"Execution time: {total_time}\")\n",
    "print(f\"Question: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3278fdc4",
   "metadata": {
    "papermill": {
     "duration": 0.034052,
     "end_time": "2025-07-03T18:29:34.859860",
     "exception": false,
     "start_time": "2025-07-03T18:29:34.825808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test the model \n",
    "\n",
    "\n",
    "## Let's start with some history questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ff816e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:29:34.930424Z",
     "iopub.status.busy": "2025-07-03T18:29:34.930071Z",
     "iopub.status.idle": "2025-07-03T18:29:50.615927Z",
     "shell.execute_reply": "2025-07-03T18:29:50.614754Z"
    },
    "papermill": {
     "duration": 15.723017,
     "end_time": "2025-07-03T18:29:50.617454",
     "exception": false,
     "start_time": "2025-07-03T18:29:34.894437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 15.68\n",
      "Question: When started WW2?\n",
      "Response: \n",
      "\n",
      "WW2 began in 1939 with the invasion of Poland by Nazi Germany.\n",
      "\n",
      "Final Answer: The final answer is $\\boxed{19\n"
     ]
    }
   ],
   "source": [
    "prompt = \"When started WW2?\"\n",
    "response, total_time = query_model(prompt, max_new_tokens=32)\n",
    "print(f\"Execution time: {total_time}\")\n",
    "print(f\"Question: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72576ac",
   "metadata": {
    "papermill": {
     "duration": 0.033835,
     "end_time": "2025-07-03T18:29:50.685864",
     "exception": false,
     "start_time": "2025-07-03T18:29:50.652029",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It doesn't look too right, I would like to keep it as short as possible. Let's refine a bit the function, we will add a system prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13694ff0",
   "metadata": {
    "papermill": {
     "duration": 0.03401,
     "end_time": "2025-07-03T18:29:50.754354",
     "exception": false,
     "start_time": "2025-07-03T18:29:50.720344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Improve the query function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8172ac84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:29:50.824770Z",
     "iopub.status.busy": "2025-07-03T18:29:50.823900Z",
     "iopub.status.idle": "2025-07-03T18:29:50.831038Z",
     "shell.execute_reply": "2025-07-03T18:29:50.830185Z"
    },
    "papermill": {
     "duration": 0.04418,
     "end_time": "2025-07-03T18:29:50.832548",
     "exception": false,
     "start_time": "2025-07-03T18:29:50.788368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_model_v2(prompt, max_new_tokens=32):\n",
    "    start_time = time()\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "            You are a smart AI expert in aswering questions.\n",
    "            Just answer to the point, do not elaborate.\n",
    "            For example, if you are asked to provide a year, a name, a location,\n",
    "            return just the information, without any other words.\n",
    "            \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": system_prompt}\n",
    "            ],\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device, dtype=model.dtype)\n",
    "\n",
    "    # retrieve input length\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    \n",
    "    outputs = model.generate(**inputs, \n",
    "                             max_new_tokens=max_new_tokens, \n",
    "                             disable_compile=True)\n",
    "    text = processor.batch_decode(\n",
    "        # use input length to filter only the response from the output\n",
    "        outputs[:, input_len:],\n",
    "        # skip special tokens\n",
    "        skip_special_tokens=True,\n",
    "        # cleanup tokenization spaces\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    total_time = round(time() - start_time, 2)\n",
    "    response = text[0]\n",
    "    return response, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbcfc3d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:29:50.902122Z",
     "iopub.status.busy": "2025-07-03T18:29:50.901470Z",
     "iopub.status.idle": "2025-07-03T18:29:58.604920Z",
     "shell.execute_reply": "2025-07-03T18:29:58.603613Z"
    },
    "papermill": {
     "duration": 7.740092,
     "end_time": "2025-07-03T18:29:58.606626",
     "exception": false,
     "start_time": "2025-07-03T18:29:50.866534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 7.7\n",
      "Question: What year started WW2?\n",
      "Response: World War II started in **1939**. \n"
     ]
    }
   ],
   "source": [
    "prompt = \"What year started WW2?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=12)\n",
    "print(f\"Execution time: {total_time}\")\n",
    "print(f\"Question: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82585793",
   "metadata": {
    "papermill": {
     "duration": 0.034281,
     "end_time": "2025-07-03T18:29:58.682869",
     "exception": false,
     "start_time": "2025-07-03T18:29:58.648588",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Colorize the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b949a8b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:29:58.752906Z",
     "iopub.status.busy": "2025-07-03T18:29:58.752608Z",
     "iopub.status.idle": "2025-07-03T18:29:58.757806Z",
     "shell.execute_reply": "2025-07-03T18:29:58.756910Z"
    },
    "papermill": {
     "duration": 0.042057,
     "end_time": "2025-07-03T18:29:58.759174",
     "exception": false,
     "start_time": "2025-07-03T18:29:58.717117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Reasoning\", \"Question\", \"Response\", \"Explanation\", \"Execution time\"], [\"blue\", \"red\", \"green\", \"darkblue\",  \"magenta\"]):\n",
    "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90053f95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:29:58.829953Z",
     "iopub.status.busy": "2025-07-03T18:29:58.829627Z",
     "iopub.status.idle": "2025-07-03T18:30:12.325936Z",
     "shell.execute_reply": "2025-07-03T18:30:12.325036Z"
    },
    "papermill": {
     "duration": 13.53329,
     "end_time": "2025-07-03T18:30:12.327285",
     "exception": false,
     "start_time": "2025-07-03T18:29:58.793995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 13.49\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Between what years was Obama president?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Barack Obama was president of the United States from **2009 to 2017**.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Between what years was Obama president?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f29851b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:30:12.398612Z",
     "iopub.status.busy": "2025-07-03T18:30:12.397867Z",
     "iopub.status.idle": "2025-07-03T18:30:30.407604Z",
     "shell.execute_reply": "2025-07-03T18:30:30.406664Z"
    },
    "papermill": {
     "duration": 18.047249,
     "end_time": "2025-07-03T18:30:30.409280",
     "exception": false,
     "start_time": "2025-07-03T18:30:12.362031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 18.0\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Between what years was the 30 years war?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** The Thirty Years' War was fought between **1618 and 1648**. \n",
       "\n",
       "While it began in 1618,"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Between what years was the 30 years war?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2f8da1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:30:30.483677Z",
     "iopub.status.busy": "2025-07-03T18:30:30.482939Z",
     "iopub.status.idle": "2025-07-03T18:30:47.746937Z",
     "shell.execute_reply": "2025-07-03T18:30:47.745021Z"
    },
    "papermill": {
     "duration": 17.30569,
     "end_time": "2025-07-03T18:30:47.751423",
     "exception": false,
     "start_time": "2025-07-03T18:30:30.445733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 17.25\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Between what years was the WW1?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** World War I (also known as the Great War) took place between **1914 and 1918**. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Between what years was the WW1?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed40f227",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:30:47.845995Z",
     "iopub.status.busy": "2025-07-03T18:30:47.845099Z",
     "iopub.status.idle": "2025-07-03T18:30:58.402691Z",
     "shell.execute_reply": "2025-07-03T18:30:58.401755Z"
    },
    "papermill": {
     "duration": 10.601755,
     "end_time": "2025-07-03T18:30:58.404215",
     "exception": false,
     "start_time": "2025-07-03T18:30:47.802460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 10.55\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What year was the Lepanto battle?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** The Battle of Lepanto was fought in **1571**.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What year was the Lepanto battle?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2da09c92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:30:58.479902Z",
     "iopub.status.busy": "2025-07-03T18:30:58.479161Z",
     "iopub.status.idle": "2025-07-03T18:31:31.500380Z",
     "shell.execute_reply": "2025-07-03T18:31:31.499429Z"
    },
    "papermill": {
     "duration": 33.093321,
     "end_time": "2025-07-03T18:31:31.536111",
     "exception": false,
     "start_time": "2025-07-03T18:30:58.442790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 33.01\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What happened in 1868 in Japan?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 1868 was a pivotal year in Japanese history, marking the end of the Edo period and the beginning of the Meiji Restoration. Here's a breakdown of the key events:\n",
       "\n",
       "*   **The Meiji Restoration Begins:** This is the most significant event.  For centuries, Japan had been under the"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What happened in 1868 in Japan?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=64)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ccf18",
   "metadata": {
    "papermill": {
     "duration": 0.034684,
     "end_time": "2025-07-03T18:31:31.605725",
     "exception": false,
     "start_time": "2025-07-03T18:31:31.571041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's modify the query function to stop the generation after a maximum character number was reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74caddc",
   "metadata": {
    "papermill": {
     "duration": 0.034686,
     "end_time": "2025-07-03T18:31:31.675372",
     "exception": false,
     "start_time": "2025-07-03T18:31:31.640686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Add a custom stopping criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0572ffd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:31:31.746958Z",
     "iopub.status.busy": "2025-07-03T18:31:31.746662Z",
     "iopub.status.idle": "2025-07-03T18:31:31.756063Z",
     "shell.execute_reply": "2025-07-03T18:31:31.755279Z"
    },
    "papermill": {
     "duration": 0.04727,
     "end_time": "2025-07-03T18:31:31.757615",
     "exception": false,
     "start_time": "2025-07-03T18:31:31.710345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class MaxCharLengthCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, max_chars, input_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_chars = max_chars\n",
    "        self.input_len = input_len\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Decode only the generated part\n",
    "        gen_tokens = input_ids[:, self.input_len:]\n",
    "        text = self.tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
    "        return len(text) >= self.max_chars\n",
    "\n",
    "def query_model_v3(prompt, max_chars=128, max_new_tokens=64):\n",
    "    start_time = time()\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "            You are a smart AI expert in aswering questions.\n",
    "            Just answer to the point, do not elaborate.\n",
    "            For example, if you are asked to provide a year, a name, a location,\n",
    "            return just the information, without any other words.\n",
    "            \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": system_prompt}\n",
    "            ],\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device, dtype=model.dtype)\n",
    "\n",
    "    # retrieve input length\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    \n",
    "    stopping_criteria = StoppingCriteriaList([\n",
    "        MaxCharLengthCriteria(processor, max_chars=max_chars, input_len=input_len)\n",
    "    ])\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        disable_compile=True\n",
    "    )\n",
    "\n",
    "    text = processor.batch_decode(\n",
    "        # use input length to filter only the response from the output\n",
    "        outputs[:, input_len:],\n",
    "        # skip special tokens\n",
    "        skip_special_tokens=True,\n",
    "        # cleanup tokenization spaces\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    total_time = round(time() - start_time, 2)\n",
    "    response = text[0]\n",
    "    return response, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdd24abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:31:31.829406Z",
     "iopub.status.busy": "2025-07-03T18:31:31.829036Z",
     "iopub.status.idle": "2025-07-03T18:31:48.732291Z",
     "shell.execute_reply": "2025-07-03T18:31:48.731386Z"
    },
    "papermill": {
     "duration": 16.940986,
     "end_time": "2025-07-03T18:31:48.733895",
     "exception": false,
     "start_time": "2025-07-03T18:31:31.792909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 16.9\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What happened in 1868 in Japan?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 1868 was a pivotal year in Japanese history, marking the end of the Edo period and the beginning of the Meiji Restoration. Here'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What happened in 1868 in Japan?\"\n",
    "response, total_time = query_model_v3(prompt, max_chars=128, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a520753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:31:48.806343Z",
     "iopub.status.busy": "2025-07-03T18:31:48.805971Z",
     "iopub.status.idle": "2025-07-03T18:32:03.640054Z",
     "shell.execute_reply": "2025-07-03T18:32:03.639163Z"
    },
    "papermill": {
     "duration": 14.872063,
     "end_time": "2025-07-03T18:32:03.641588",
     "exception": false,
     "start_time": "2025-07-03T18:31:48.769525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 14.83\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Who was the first American president?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** The first American president was **George Washington**. He served from 1789 to 1797.\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Who was the first American president?\"\n",
    "response, total_time = query_model_v3(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c224239",
   "metadata": {
    "papermill": {
     "duration": 0.034878,
     "end_time": "2025-07-03T18:32:03.711760",
     "exception": false,
     "start_time": "2025-07-03T18:32:03.676882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Let's ask some pop culture question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b738ed1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:32:03.783881Z",
     "iopub.status.busy": "2025-07-03T18:32:03.783559Z",
     "iopub.status.idle": "2025-07-03T18:32:21.640255Z",
     "shell.execute_reply": "2025-07-03T18:32:21.639376Z"
    },
    "papermill": {
     "duration": 17.894565,
     "end_time": "2025-07-03T18:32:21.641727",
     "exception": false,
     "start_time": "2025-07-03T18:32:03.747162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 17.85\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** In what novel the number 42 is important?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** The number 42 is famously important in Douglas Adams's science fiction comedy series **The Hitchhiker's Guide to the Galaxy**. \n",
       "\n",
       "In"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"In what novel the number 42 is important?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b608f5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:32:21.713923Z",
     "iopub.status.busy": "2025-07-03T18:32:21.713342Z",
     "iopub.status.idle": "2025-07-03T18:32:31.612999Z",
     "shell.execute_reply": "2025-07-03T18:32:31.612129Z"
    },
    "papermill": {
     "duration": 9.937499,
     "end_time": "2025-07-03T18:32:31.614454",
     "exception": false,
     "start_time": "2025-07-03T18:32:21.676955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 9.89\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Name the famous boyfriend of Yoko Ono.\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** The famous boyfriend of Yoko Ono is **John Lennon**.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Name the famous boyfriend of Yoko Ono.\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1af2e522",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:32:31.686807Z",
     "iopub.status.busy": "2025-07-03T18:32:31.686481Z",
     "iopub.status.idle": "2025-07-03T18:32:49.515463Z",
     "shell.execute_reply": "2025-07-03T18:32:49.514564Z"
    },
    "papermill": {
     "duration": 17.866798,
     "end_time": "2025-07-03T18:32:49.516910",
     "exception": false,
     "start_time": "2025-07-03T18:32:31.650112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 17.82\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Who was nicknamed 'The King' in music?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** There are several musicians who have been nicknamed \"The King\" in music, but the most famous and widely recognized is **Elvis Presley**. \n",
       "\n",
       "However, another"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Who was nicknamed 'The King' in music?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2b0d475",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:32:49.589992Z",
     "iopub.status.busy": "2025-07-03T18:32:49.589689Z",
     "iopub.status.idle": "2025-07-03T18:33:07.575338Z",
     "shell.execute_reply": "2025-07-03T18:33:07.574446Z"
    },
    "papermill": {
     "duration": 18.023945,
     "end_time": "2025-07-03T18:33:07.576849",
     "exception": false,
     "start_time": "2025-07-03T18:32:49.552904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 17.98\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What actor played Sheldon in TBBT?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** The actor who played Sheldon Cooper in The Big Bang Theory is **Jim Parsons**. \n",
       "\n",
       "He won a Primetime Emmy Award for Outstanding Lead Actor in a Comedy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What actor played Sheldon in TBBT?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b41994b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:33:07.649614Z",
     "iopub.status.busy": "2025-07-03T18:33:07.649244Z",
     "iopub.status.idle": "2025-07-03T18:33:17.953736Z",
     "shell.execute_reply": "2025-07-03T18:33:17.952764Z"
    },
    "papermill": {
     "duration": 10.342725,
     "end_time": "2025-07-03T18:33:17.955386",
     "exception": false,
     "start_time": "2025-07-03T18:33:07.612661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 10.3\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What is the maiden name of Princess of Wales?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** The Princess of Wales's maiden name is **Phillips**. \n",
       "\n",
       "She was"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What is the maiden name of Princess of Wales?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=16)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6928f00",
   "metadata": {
    "papermill": {
     "duration": 0.036657,
     "end_time": "2025-07-03T18:33:18.028886",
     "exception": false,
     "start_time": "2025-07-03T18:33:17.992229",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Math questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75d057dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:33:18.103452Z",
     "iopub.status.busy": "2025-07-03T18:33:18.103087Z",
     "iopub.status.idle": "2025-07-03T18:33:25.940929Z",
     "shell.execute_reply": "2025-07-03T18:33:25.940104Z"
    },
    "papermill": {
     "duration": 7.87626,
     "end_time": "2025-07-03T18:33:25.942481",
     "exception": false,
     "start_time": "2025-07-03T18:33:18.066221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 7.83\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** 34 + 21\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 34 + 21 = 55\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"34 + 21\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=16)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f241b54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:33:26.017106Z",
     "iopub.status.busy": "2025-07-03T18:33:26.016725Z",
     "iopub.status.idle": "2025-07-03T18:33:36.048014Z",
     "shell.execute_reply": "2025-07-03T18:33:36.046942Z"
    },
    "papermill": {
     "duration": 10.071142,
     "end_time": "2025-07-03T18:33:36.049849",
     "exception": false,
     "start_time": "2025-07-03T18:33:25.978707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 10.02\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** 49 x 27\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 49 x 27 = 1323\n",
       "\n",
       "Here's"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"49 x 27\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=16)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9de7921e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:33:36.125405Z",
     "iopub.status.busy": "2025-07-03T18:33:36.125039Z",
     "iopub.status.idle": "2025-07-03T18:34:00.189349Z",
     "shell.execute_reply": "2025-07-03T18:34:00.188251Z"
    },
    "papermill": {
     "duration": 24.103225,
     "end_time": "2025-07-03T18:34:00.190908",
     "exception": false,
     "start_time": "2025-07-03T18:33:36.087683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 24.06\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Brian and Sarah are brothers. Brian is 5yo, Sarah is 6 years older. How old is Sarah?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Sarah is 6 years old.  The problem states Sarah is 6 years older than Brian, who is 5.  So Sarah is 5 + 6 = 11 years old"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Brian and Sarah are brothers. Brian is 5yo, Sarah is 6 years older. How old is Sarah?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=40)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "120c468a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:34:00.267018Z",
     "iopub.status.busy": "2025-07-03T18:34:00.266721Z",
     "iopub.status.idle": "2025-07-03T18:34:11.790974Z",
     "shell.execute_reply": "2025-07-03T18:34:11.790113Z"
    },
    "papermill": {
     "duration": 11.564783,
     "end_time": "2025-07-03T18:34:11.792576",
     "exception": false,
     "start_time": "2025-07-03T18:34:00.227793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 11.52\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** x + 2 y = 5; y - x = 1. What are x and y? Just return x and y.\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** x = 1\n",
       "y = 2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"x + 2 y = 5; y - x = 1. What are x and y? Just return x and y.\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=64)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3eedeb88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:34:11.867963Z",
     "iopub.status.busy": "2025-07-03T18:34:11.867158Z",
     "iopub.status.idle": "2025-07-03T18:34:25.675735Z",
     "shell.execute_reply": "2025-07-03T18:34:25.674804Z"
    },
    "papermill": {
     "duration": 13.848176,
     "end_time": "2025-07-03T18:34:25.677257",
     "exception": false,
     "start_time": "2025-07-03T18:34:11.829081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 13.8\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What is the total area of a sphere or radius 3? Just return the result.\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 113.09733552923255\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What is the total area of a sphere or radius 3? Just return the result.\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=64)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbb45b26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:34:25.751233Z",
     "iopub.status.busy": "2025-07-03T18:34:25.750883Z",
     "iopub.status.idle": "2025-07-03T18:35:57.246458Z",
     "shell.execute_reply": "2025-07-03T18:35:57.245492Z"
    },
    "papermill": {
     "duration": 91.571421,
     "end_time": "2025-07-03T18:35:57.285137",
     "exception": false,
     "start_time": "2025-07-03T18:34:25.713716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 91.49\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** A rectangle with diagonal 4 is circumscribed by a circle. What is the circle's area?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Let the rectangle have length $l$ and width $w$. The diagonal of the rectangle is given by the Pythagorean theorem:\n",
       "$$l^2 + w^2 = 4^2 = 16$$\n",
       "Since the rectangle is circumscribed by a circle, the diameter of the circle is equal to the length of the diagonal of the rectangle. Thus, the diameter of the circle is 4, and the radius is $r = \\frac{4}{2} = 2$.\n",
       "The area of the circle is given by the formula $A = \\pi r^2$. Substituting $r=2$, we get\n",
       "$$A = \\pi (2^2) = 4\\pi$$\n",
       "\n",
       "Thus, the area of the circle is $4\\pi$.\n",
       "\n",
       "Final Answer: The final answer is $\\boxed{4\\pi}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"A rectangle with diagonal 4 is circumscribed by a circle. What is the circle's area?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=256)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc252327",
   "metadata": {
    "papermill": {
     "duration": 0.03631,
     "end_time": "2025-07-03T18:35:57.358632",
     "exception": false,
     "start_time": "2025-07-03T18:35:57.322322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Multiple languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73dc8a91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:35:57.433741Z",
     "iopub.status.busy": "2025-07-03T18:35:57.433407Z",
     "iopub.status.idle": "2025-07-03T18:37:03.238536Z",
     "shell.execute_reply": "2025-07-03T18:37:03.237604Z"
    },
    "papermill": {
     "duration": 65.880165,
     "end_time": "2025-07-03T18:37:03.275559",
     "exception": false,
     "start_time": "2025-07-03T18:35:57.395394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 65.8\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Cine este Mircea Cartarescu?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Mircea Cartarescu este unul dintre cei mai importanți și influenți scriitori români contemporani. Este recunoscut pentru stilul său inovator, poetic și suprarealist, precum și pentru explorarea temelor complexe ale existenței umane, ale memoriei, ale timpului și ale spațiului.\n",
       "\n",
       "Iată câteva aspecte cheie despre Mircea Cartarescu:\n",
       "\n",
       "* **Cariera literară:** Cartarescu a publicat o serie de romane, nuvele, poezii și eseuri, fiind autorul unor opere importante precum:\n",
       "    * **"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Romanian\n",
    "prompt = \"Cine este Mircea Cartarescu?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=128)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9642cdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:37:03.350141Z",
     "iopub.status.busy": "2025-07-03T18:37:03.349842Z",
     "iopub.status.idle": "2025-07-03T18:38:09.010957Z",
     "shell.execute_reply": "2025-07-03T18:38:09.009860Z"
    },
    "papermill": {
     "duration": 65.737874,
     "end_time": "2025-07-03T18:38:09.050436",
     "exception": false,
     "start_time": "2025-07-03T18:37:03.312562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 65.65\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Kush ishte Ismail Kadare?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Ismail Kadare (1938-2022) ishte **një shkruar, poet, dramaturg, dhe diplomatik shqiptar, i njohur ndryshe si një nga më të rëndësishmit e sotme të letërsisë shqiptare dhe një nga autorët më të vlerësuar në botën e letërsisë shqiptare**.\n",
       "\n",
       "Këtu janë disa pika kryesore për të kuptuar kush ishte Ismail Kadare:\n",
       "\n",
       "* **Kariera Letrare:** Ai është i nj"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Albanian\n",
    "prompt = \"Kush ishte Ismail Kadare?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=128)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d88ff9e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:38:09.125680Z",
     "iopub.status.busy": "2025-07-03T18:38:09.125358Z",
     "iopub.status.idle": "2025-07-03T18:39:15.540643Z",
     "shell.execute_reply": "2025-07-03T18:39:15.539733Z"
    },
    "papermill": {
     "duration": 66.491769,
     "end_time": "2025-07-03T18:39:15.578982",
     "exception": false,
     "start_time": "2025-07-03T18:38:09.087213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 66.41\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** 夏目漱石とは誰ですか?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 夏目漱石（なつめ そうせき、1867年7月19日 - 1916年4月29日）は、日本の近代文学を代表する作家です。\n",
       "\n",
       "**主な特徴と業績:**\n",
       "\n",
       "*   **近代日本文学の確立:** 明治時代を代表する作家として、日本の近代文学の基礎を築き、その後の文学に大きな影響を与えました。\n",
       "*   **多様な作品:** 小説、論文、評論など、幅広いジャンルで活躍しました。\n",
       "*   **代表作:**\n",
       "    *   **吾輩"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Japanese\n",
    "prompt = \"夏目漱石とは誰ですか?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=128)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2105bbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:39:15.657354Z",
     "iopub.status.busy": "2025-07-03T18:39:15.656974Z",
     "iopub.status.idle": "2025-07-03T18:40:21.656136Z",
     "shell.execute_reply": "2025-07-03T18:40:21.655196Z"
    },
    "papermill": {
     "duration": 66.0773,
     "end_time": "2025-07-03T18:40:21.694729",
     "exception": false,
     "start_time": "2025-07-03T18:39:15.617429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 65.99\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** 马拉多纳是谁?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 迭戈·马拉多纳 (Diego Armando Maradona，1960年10月30日－) 是阿根廷足球历史上最伟大的球员之一，被广泛认为是足球史上最伟大的球员之一。\n",
       "\n",
       "以下是关于马拉多纳的一些关键信息：\n",
       "\n",
       "*   **职业生涯：** 他在阿根廷俱乐部博卡青年（Boca Juniors）开始职业生涯，并在1982年转会到意大利俱乐部拿玻利（Napoli）。在那里，他带领拿玻利赢得了意大利足球甲级联赛冠军，并以其非凡的个人能力和创造"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Chinese\n",
    "prompt = \"马拉多纳是谁?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=128)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b37753f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T18:40:21.770451Z",
     "iopub.status.busy": "2025-07-03T18:40:21.770097Z",
     "iopub.status.idle": "2025-07-03T18:41:29.868286Z",
     "shell.execute_reply": "2025-07-03T18:41:29.867223Z"
    },
    "papermill": {
     "duration": 68.175732,
     "end_time": "2025-07-03T18:41:29.907540",
     "exception": false,
     "start_time": "2025-07-03T18:40:21.731808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 68.09\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Qui était Marguerite Yourcenar?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Marguerite Yourcenar (1900-1984) était une **écrivaine française majeure, reconnue pour ses romans historiques et ses essais philosophiques**. Elle est considérée comme l'une des figures les plus importantes de la littérature française du XXe siècle.\n",
       "\n",
       "Voici quelques points clés pour mieux la comprendre :\n",
       "\n",
       "* **Genre :** Elle est surtout connue pour ses romans historiques, particulièrement ceux qui se déroulent à l'époque antique.  Son œuvre la plus célèbre est sans aucun doute *Les Mémoires de Marc Aurèle*, un roman historique incroyablement riche en philosophie stoïci"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#French\n",
    "prompt = \"Qui était Marguerite Yourcenar?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=128)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505e5289",
   "metadata": {
    "papermill": {
     "duration": 0.036919,
     "end_time": "2025-07-03T18:41:29.981501",
     "exception": false,
     "start_time": "2025-07-03T18:41:29.944582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "\n",
    "Preliminary conclusion after testing the model with:\n",
    "* History questions  \n",
    "* Pop culture  \n",
    "* Math (arithmetics, algebra, geometry)\n",
    "* Multiple languages.\n",
    "  \n",
    "is that the model is performing reasonably well with easy and medium-level questions.\n",
    "\n",
    "**Good points**:\n",
    "- When prompted to answer to the point, the model tend to behave well.\n",
    "- Math seems to be accurate.\n",
    "- Language capability is extensive.\n",
    "\n",
    "**Areas to improve**:\n",
    "- Modify the output to stop at the end of a phrase.\n",
    "- Continue to test the model with multi-modal input."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12693789,
     "sourceId": 105267,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 317146,
     "modelInstanceId": 365533,
     "sourceId": 450403,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 943.589498,
   "end_time": "2025-07-03T18:41:33.687281",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-03T18:25:50.097783",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0dca806c1d7844778dcf108aab730456": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_34d7f56374cb468d9f7fcafd0e253685",
        "IPY_MODEL_8295a96aac1d46f2983084b59f6931a1",
        "IPY_MODEL_a5489b47e3704b1781fb6dcf8e08d370"
       ],
       "layout": "IPY_MODEL_a27859c47bc64292a865d940d8ccba07",
       "tabbable": null,
       "tooltip": null
      }
     },
     "34d7f56374cb468d9f7fcafd0e253685": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_eed6578361044a2f87340c35d832fd87",
       "placeholder": "​",
       "style": "IPY_MODEL_7d8a8acf739143b2b45b72944194074a",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "491b64aa7d7d4ea29f2f8a8c8577effd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5653f9f62bef4539bb941d2595b77364": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5dd6475e3eaa4eef8bde584922220c46": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7d8a8acf739143b2b45b72944194074a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8295a96aac1d46f2983084b59f6931a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_491b64aa7d7d4ea29f2f8a8c8577effd",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5653f9f62bef4539bb941d2595b77364",
       "tabbable": null,
       "tooltip": null,
       "value": 3.0
      }
     },
     "a27859c47bc64292a865d940d8ccba07": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a5489b47e3704b1781fb6dcf8e08d370": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5dd6475e3eaa4eef8bde584922220c46",
       "placeholder": "​",
       "style": "IPY_MODEL_a72e20848f9744c7b6487d2ae91f7f39",
       "tabbable": null,
       "tooltip": null,
       "value": " 3/3 [00:04&lt;00:00,  1.30s/it]"
      }
     },
     "a72e20848f9744c7b6487d2ae91f7f39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "eed6578361044a2f87340c35d832fd87": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
