{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab17466",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.006517,
     "end_time": "2025-07-03T14:24:45.409459",
     "exception": false,
     "start_time": "2025-07-03T14:24:45.402942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook presents a functional prototype built with Gemma 3n, Google’s latest on-device, multimodal AI model. The goal: use compact, private, offline-ready AI to solve a real-world problem. Full demo, code, and technical details follow.\n",
    "\n",
    "## What are the key features of Gemma 3n?\n",
    "\n",
    "The key features of this new model from Google are:\n",
    "\n",
    "1. On-Device Performance\n",
    "Optimized for mobile and edge devices, Gemma 3n delivers real-time AI with minimal memory usage. The 5B and 8B models run like 2B and 4B models, thanks to innovations like Per-Layer Embeddings (PLE).\n",
    "\n",
    "2. Mix’n’Match Model Scaling\n",
    "A single model can act as multiple: the 4B version includes a 2B submodel, enabling dynamic tradeoffs between performance and efficiency. Developers can also create custom-sized submodels tailored to specific tasks.\n",
    "\n",
    "3. Privacy-First and Offline-Ready\n",
    "Gemma 3n runs entirely on-device, ensuring user data never leaves the device. This makes it ideal for privacy-sensitive applications and for use in low- or no-connectivity environments.\n",
    "\n",
    "4. Multimodal Understanding\n",
    "Supports text, image, audio, and enhanced video input, enabling powerful applications like voice interfaces, transcription, translation, visual recognition, and more—all locally.\n",
    "\n",
    "5. Multilingual Proficiency\n",
    "Strong performance across major global languages including Japanese, German, Korean, Spanish, and French, expanding access and inclusivity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3609aed",
   "metadata": {
    "papermill": {
     "duration": 0.004721,
     "end_time": "2025-07-03T14:24:45.419364",
     "exception": false,
     "start_time": "2025-07-03T14:24:45.414643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare the model\n",
    "\n",
    "## Install prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8371f682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:24:45.430876Z",
     "iopub.status.busy": "2025-07-03T14:24:45.430591Z",
     "iopub.status.idle": "2025-07-03T14:26:44.785302Z",
     "shell.execute_reply": "2025-07-03T14:26:44.784219Z"
    },
    "papermill": {
     "duration": 119.363201,
     "end_time": "2025-07-03T14:26:44.787869",
     "exception": false,
     "start_time": "2025-07-03T14:24:45.424668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\r\n",
      "Collecting timm\r\n",
      "  Downloading timm-1.0.16-py3-none-any.whl.metadata (57 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\r\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.31.1)\r\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (25.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.2)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->timm)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->timm)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->timm)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->timm)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->timm)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->timm)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->timm)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.1.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->timm) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->timm) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->timm) (2024.2.0)\r\n",
      "Downloading timm-1.0.16-py3-none-any.whl (2.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\r\n",
      "  Attempting uninstall: timm\r\n",
      "    Found existing installation: timm 1.0.15\r\n",
      "    Uninstalling timm-1.0.15:\r\n",
      "      Successfully uninstalled timm-1.0.15\r\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 timm-1.0.16\r\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\r\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\r\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.31.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\r\n",
      "Collecting git+https://github.com/huggingface/transformers.git\r\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-5vwdz8fb\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-5vwdz8fb\r\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit e15b06d8dc6fa132550311d63c9758b580f39bcc\r\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (3.18.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (0.31.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (0.21.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (0.5.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (4.67.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.54.0.dev0) (2025.3.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.54.0.dev0) (4.13.2)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.54.0.dev0) (1.1.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.54.0.dev0) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.54.0.dev0) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.54.0.dev0) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.54.0.dev0) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.54.0.dev0) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.54.0.dev0) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.54.0.dev0) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.54.0.dev0) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.54.0.dev0) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.54.0.dev0) (2025.4.26)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.54.0.dev0) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.54.0.dev0) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.54.0.dev0) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.54.0.dev0) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.54.0.dev0) (2024.2.0)\r\n",
      "Building wheels for collected packages: transformers\r\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for transformers: filename=transformers-4.54.0.dev0-py3-none-any.whl size=11777247 sha256=65983742591fbfc98babb21b1d03272b508fefc323047abec8acf163b587f101\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2hv4zvhr/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\r\n",
      "Successfully built transformers\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.51.3\r\n",
      "    Uninstalling transformers-4.51.3:\r\n",
      "      Successfully uninstalled transformers-4.51.3\r\n",
      "Successfully installed transformers-4.54.0.dev0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install timm --upgrade\n",
    "!pip install accelerate\n",
    "!pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f141cc67",
   "metadata": {
    "papermill": {
     "duration": 0.02767,
     "end_time": "2025-07-03T14:26:44.843654",
     "exception": false,
     "start_time": "2025-07-03T14:26:44.815984",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f8acc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:26:44.968017Z",
     "iopub.status.busy": "2025-07-03T14:26:44.967685Z",
     "iopub.status.idle": "2025-07-03T14:27:16.954548Z",
     "shell.execute_reply": "2025-07-03T14:27:16.953639Z"
    },
    "papermill": {
     "duration": 32.084304,
     "end_time": "2025-07-03T14:27:16.955898",
     "exception": false,
     "start_time": "2025-07-03T14:26:44.871594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 14:27:01.609157: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751552821.866423      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751552821.940765      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import kagglehub\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce90851",
   "metadata": {
    "papermill": {
     "duration": 0.027207,
     "end_time": "2025-07-03T14:27:17.011192",
     "exception": false,
     "start_time": "2025-07-03T14:27:16.983985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e44636",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:27:17.068063Z",
     "iopub.status.busy": "2025-07-03T14:27:17.067407Z",
     "iopub.status.idle": "2025-07-03T14:27:32.130155Z",
     "shell.execute_reply": "2025-07-03T14:27:32.129407Z"
    },
    "papermill": {
     "duration": 15.092901,
     "end_time": "2025-07-03T14:27:32.131690",
     "exception": false,
     "start_time": "2025-07-03T14:27:17.038789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a47d00ddb74f938de87b56747349da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GEMMA_PATH = kagglehub.model_download(\"google/gemma-3n/transformers/gemma-3n-e2b-it\")\n",
    "processor = AutoProcessor.from_pretrained(GEMMA_PATH)\n",
    "model = AutoModelForImageTextToText.from_pretrained(GEMMA_PATH, torch_dtype=\"auto\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff324d64",
   "metadata": {
    "papermill": {
     "duration": 0.027688,
     "end_time": "2025-07-03T14:27:32.187373",
     "exception": false,
     "start_time": "2025-07-03T14:27:32.159685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test the model with a simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2891aa4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:27:32.245659Z",
     "iopub.status.busy": "2025-07-03T14:27:32.245365Z",
     "iopub.status.idle": "2025-07-03T14:28:10.092130Z",
     "shell.execute_reply": "2025-07-03T14:28:10.091125Z"
    },
    "papermill": {
     "duration": 37.905371,
     "end_time": "2025-07-03T14:28:10.121758",
     "exception": false,
     "start_time": "2025-07-03T14:27:32.216387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the France capital?\n",
      "\n",
      "Paris.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"What is the France capital?\"\"\"\n",
    "input_ids = processor(text=prompt, \n",
    "                      return_tensors=\"pt\").to(model.device, \n",
    "                                              dtype=model.dtype)\n",
    "\n",
    "outputs = model.generate(**input_ids, \n",
    "                         max_new_tokens=32, \n",
    "                         disable_compile=True)\n",
    "text = processor.batch_decode(\n",
    "    outputs,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630c437a",
   "metadata": {
    "papermill": {
     "duration": 0.027943,
     "end_time": "2025-07-03T14:28:10.178369",
     "exception": false,
     "start_time": "2025-07-03T14:28:10.150426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's wrap this inside a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ea9e09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:28:10.236172Z",
     "iopub.status.busy": "2025-07-03T14:28:10.235863Z",
     "iopub.status.idle": "2025-07-03T14:28:10.241466Z",
     "shell.execute_reply": "2025-07-03T14:28:10.240688Z"
    },
    "papermill": {
     "duration": 0.03539,
     "end_time": "2025-07-03T14:28:10.242664",
     "exception": false,
     "start_time": "2025-07-03T14:28:10.207274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_model(prompt, max_new_tokens=32):\n",
    "    start_time = time()\n",
    "    input_ids = processor(text=prompt, \n",
    "                          return_tensors=\"pt\").to(model.device, \n",
    "                                                  dtype=model.dtype)\n",
    "    \n",
    "    outputs = model.generate(**input_ids, \n",
    "                             max_new_tokens=max_new_tokens, \n",
    "                             disable_compile=True)\n",
    "    text = processor.batch_decode(\n",
    "        outputs,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    total_time = round(time() - start_time, 2)\n",
    "    response = text[0].split(prompt)[-1]\n",
    "    return response, total_time\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c2ddf33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:28:10.300994Z",
     "iopub.status.busy": "2025-07-03T14:28:10.300680Z",
     "iopub.status.idle": "2025-07-03T14:28:13.326227Z",
     "shell.execute_reply": "2025-07-03T14:28:13.325051Z"
    },
    "papermill": {
     "duration": 3.05606,
     "end_time": "2025-07-03T14:28:13.327643",
     "exception": false,
     "start_time": "2025-07-03T14:28:10.271583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 3.02\n",
      "Question: Quelle est la capitale de la France?\n",
      "Response: \n",
      "\n",
      "Paris.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Quelle est la capitale de la France?\"\n",
    "response, total_time = query_model(prompt, max_new_tokens=16)\n",
    "print(f\"Execution time: {total_time}\")\n",
    "print(f\"Question: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25265f7a",
   "metadata": {
    "papermill": {
     "duration": 0.028434,
     "end_time": "2025-07-03T14:28:13.384882",
     "exception": false,
     "start_time": "2025-07-03T14:28:13.356448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test the model with history questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21f23784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:28:13.443347Z",
     "iopub.status.busy": "2025-07-03T14:28:13.443044Z",
     "iopub.status.idle": "2025-07-03T14:28:19.245444Z",
     "shell.execute_reply": "2025-07-03T14:28:19.244795Z"
    },
    "papermill": {
     "duration": 5.833171,
     "end_time": "2025-07-03T14:28:19.246732",
     "exception": false,
     "start_time": "2025-07-03T14:28:13.413561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 5.8\n",
      "Question: When started WW2?\n",
      "Response: \n",
      "WW2 began in 1939.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"When started WW2?\"\n",
    "response, total_time = query_model(prompt, max_new_tokens=32)\n",
    "print(f\"Execution time: {total_time}\")\n",
    "print(f\"Question: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483a2b3b",
   "metadata": {
    "papermill": {
     "duration": 0.026497,
     "end_time": "2025-07-03T14:28:19.305434",
     "exception": false,
     "start_time": "2025-07-03T14:28:19.278937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It doesn't look too right, I would like to keep it as short as possible. Let's refine a bit the function, we will add a system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fef21f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:28:19.360497Z",
     "iopub.status.busy": "2025-07-03T14:28:19.360237Z",
     "iopub.status.idle": "2025-07-03T14:28:19.365112Z",
     "shell.execute_reply": "2025-07-03T14:28:19.364500Z"
    },
    "papermill": {
     "duration": 0.033607,
     "end_time": "2025-07-03T14:28:19.366151",
     "exception": false,
     "start_time": "2025-07-03T14:28:19.332544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_model_v2(prompt, max_new_tokens=32):\n",
    "    start_time = time()\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "            You are a smart AI expert in aswering questions.\n",
    "            Just answer to the point, do not elaborate.\n",
    "            For example, if you are asked to provide a year, a name, a location,\n",
    "            return just the information, without any other words.\n",
    "            \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": system_prompt}\n",
    "            ],\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device, dtype=model.dtype)\n",
    "\n",
    "    # retrieve input length\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    \n",
    "    outputs = model.generate(**inputs, \n",
    "                             max_new_tokens=max_new_tokens, \n",
    "                             disable_compile=True)\n",
    "    text = processor.batch_decode(\n",
    "        # use input length to filter only the response from the output\n",
    "        outputs[:, input_len:],\n",
    "        # skip special tokens\n",
    "        skip_special_tokens=True,\n",
    "        # cleanup tokenization spaces\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    total_time = round(time() - start_time, 2)\n",
    "    response = text[0]\n",
    "    return response, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0fe67ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:28:19.436866Z",
     "iopub.status.busy": "2025-07-03T14:28:19.436495Z",
     "iopub.status.idle": "2025-07-03T14:28:26.678847Z",
     "shell.execute_reply": "2025-07-03T14:28:26.677634Z"
    },
    "papermill": {
     "duration": 7.283664,
     "end_time": "2025-07-03T14:28:26.680197",
     "exception": false,
     "start_time": "2025-07-03T14:28:19.396533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 7.24\n",
      "Question: What year started WW2?\n",
      "Response: World War II started in **1939**. \n"
     ]
    }
   ],
   "source": [
    "prompt = \"What year started WW2?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=12)\n",
    "print(f\"Execution time: {total_time}\")\n",
    "print(f\"Question: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b107f251",
   "metadata": {
    "papermill": {
     "duration": 0.028192,
     "end_time": "2025-07-03T14:28:26.737178",
     "exception": false,
     "start_time": "2025-07-03T14:28:26.708986",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Colorize the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ea27695",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:28:26.796651Z",
     "iopub.status.busy": "2025-07-03T14:28:26.796349Z",
     "iopub.status.idle": "2025-07-03T14:28:26.801128Z",
     "shell.execute_reply": "2025-07-03T14:28:26.800418Z"
    },
    "papermill": {
     "duration": 0.036332,
     "end_time": "2025-07-03T14:28:26.802355",
     "exception": false,
     "start_time": "2025-07-03T14:28:26.766023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Reasoning\", \"Question\", \"Response\", \"Explanation\", \"Execution time\"], [\"blue\", \"red\", \"green\", \"darkblue\",  \"magenta\"]):\n",
    "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24ea8786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:28:26.862565Z",
     "iopub.status.busy": "2025-07-03T14:28:26.862303Z",
     "iopub.status.idle": "2025-07-03T14:28:38.481871Z",
     "shell.execute_reply": "2025-07-03T14:28:38.481281Z"
    },
    "papermill": {
     "duration": 11.650608,
     "end_time": "2025-07-03T14:28:38.482824",
     "exception": false,
     "start_time": "2025-07-03T14:28:26.832216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 11.62\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Between what years was Obama president?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Barack Obama was president of the United States from **2009 to 2017**.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Between what years was Obama president?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd6e9891",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:28:38.540277Z",
     "iopub.status.busy": "2025-07-03T14:28:38.539945Z",
     "iopub.status.idle": "2025-07-03T14:28:53.830993Z",
     "shell.execute_reply": "2025-07-03T14:28:53.830331Z"
    },
    "papermill": {
     "duration": 15.321305,
     "end_time": "2025-07-03T14:28:53.832102",
     "exception": false,
     "start_time": "2025-07-03T14:28:38.510797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 15.29\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Between what years was the 30 years war?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** The Thirty Years' War was fought between **1618 and 1648**. \n",
       "\n",
       "While it's often considered to have started in"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Between what years was the 30 years war?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f112b60b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:28:53.892161Z",
     "iopub.status.busy": "2025-07-03T14:28:53.891863Z",
     "iopub.status.idle": "2025-07-03T14:29:04.790983Z",
     "shell.execute_reply": "2025-07-03T14:29:04.790151Z"
    },
    "papermill": {
     "duration": 10.930333,
     "end_time": "2025-07-03T14:29:04.792572",
     "exception": false,
     "start_time": "2025-07-03T14:28:53.862239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 10.89\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Between what years was the WW1?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** World War I took place between **1914 and 1918**. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Between what years was the WW1?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36811d47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:29:04.853131Z",
     "iopub.status.busy": "2025-07-03T14:29:04.852815Z",
     "iopub.status.idle": "2025-07-03T14:29:13.805903Z",
     "shell.execute_reply": "2025-07-03T14:29:13.805066Z"
    },
    "papermill": {
     "duration": 8.985244,
     "end_time": "2025-07-03T14:29:13.807578",
     "exception": false,
     "start_time": "2025-07-03T14:29:04.822334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 8.95\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What year was the Lepanto battle?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** The Battle of Lepanto took place in **1571**.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What year was the Lepanto battle?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0addf1f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:29:13.866787Z",
     "iopub.status.busy": "2025-07-03T14:29:13.866508Z",
     "iopub.status.idle": "2025-07-03T14:29:42.025204Z",
     "shell.execute_reply": "2025-07-03T14:29:42.024637Z"
    },
    "papermill": {
     "duration": 28.189798,
     "end_time": "2025-07-03T14:29:42.026226",
     "exception": false,
     "start_time": "2025-07-03T14:29:13.836428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 28.15\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What happened in 1868 in Japan?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 1868 was a pivotal year in Japanese history, marking the end of the Edo period and the beginning of the Meiji Restoration. Here's a breakdown of the key events:\n",
       "\n",
       "*   **The Boshin War Begins:** This was a civil war between the Tokugawa shogunate (the ruling military government"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What happened in 1868 in Japan?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=64)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72561d",
   "metadata": {
    "papermill": {
     "duration": 0.028016,
     "end_time": "2025-07-03T14:29:42.084210",
     "exception": false,
     "start_time": "2025-07-03T14:29:42.056194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's modify the query function to stop the generation after a maximum character number was reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee0b9a2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:29:42.144763Z",
     "iopub.status.busy": "2025-07-03T14:29:42.144444Z",
     "iopub.status.idle": "2025-07-03T14:29:42.152452Z",
     "shell.execute_reply": "2025-07-03T14:29:42.151409Z"
    },
    "papermill": {
     "duration": 0.041283,
     "end_time": "2025-07-03T14:29:42.154211",
     "exception": false,
     "start_time": "2025-07-03T14:29:42.112928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class MaxCharLengthCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, max_chars, input_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_chars = max_chars\n",
    "        self.input_len = input_len\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Decode only the generated part\n",
    "        gen_tokens = input_ids[:, self.input_len:]\n",
    "        text = self.tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
    "        return len(text) >= self.max_chars\n",
    "\n",
    "def query_model_v3(prompt, max_chars=128, max_new_tokens=64):\n",
    "    start_time = time()\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "            You are a smart AI expert in aswering questions.\n",
    "            Just answer to the point, do not elaborate.\n",
    "            For example, if you are asked to provide a year, a name, a location,\n",
    "            return just the information, without any other words.\n",
    "            \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": system_prompt}\n",
    "            ],\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device, dtype=model.dtype)\n",
    "\n",
    "    # retrieve input length\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    \n",
    "    stopping_criteria = StoppingCriteriaList([\n",
    "        MaxCharLengthCriteria(processor, max_chars=max_chars, input_len=input_len)\n",
    "    ])\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        disable_compile=True\n",
    "    )\n",
    "\n",
    "    text = processor.batch_decode(\n",
    "        # use input length to filter only the response from the output\n",
    "        outputs[:, input_len:],\n",
    "        # skip special tokens\n",
    "        skip_special_tokens=True,\n",
    "        # cleanup tokenization spaces\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    total_time = round(time() - start_time, 2)\n",
    "    response = text[0]\n",
    "    return response, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f57510ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:29:42.215624Z",
     "iopub.status.busy": "2025-07-03T14:29:42.215350Z",
     "iopub.status.idle": "2025-07-03T14:29:56.671141Z",
     "shell.execute_reply": "2025-07-03T14:29:56.670497Z"
    },
    "papermill": {
     "duration": 14.487723,
     "end_time": "2025-07-03T14:29:56.672402",
     "exception": false,
     "start_time": "2025-07-03T14:29:42.184679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 14.45\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What happened in 1868 in Japan?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 1868 was a pivotal year in Japanese history, marking the end of the Tokugawa Shogunate and the beginning of the Meiji Restoration"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What happened in 1868 in Japan?\"\n",
    "response, total_time = query_model_v3(prompt, max_chars=128, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "055cd0fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:29:56.732373Z",
     "iopub.status.busy": "2025-07-03T14:29:56.732047Z",
     "iopub.status.idle": "2025-07-03T14:30:09.582824Z",
     "shell.execute_reply": "2025-07-03T14:30:09.582024Z"
    },
    "papermill": {
     "duration": 12.882196,
     "end_time": "2025-07-03T14:30:09.584065",
     "exception": false,
     "start_time": "2025-07-03T14:29:56.701869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 12.85\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Who was the first American president?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** The first American president was **George Washington**. He served from 1789 to 1797.\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Who was the first American president?\"\n",
    "response, total_time = query_model_v3(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065566bd",
   "metadata": {
    "papermill": {
     "duration": 0.028301,
     "end_time": "2025-07-03T14:30:09.641333",
     "exception": false,
     "start_time": "2025-07-03T14:30:09.613032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Let's ask some pop culture question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7efb7fb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:30:09.702331Z",
     "iopub.status.busy": "2025-07-03T14:30:09.702020Z",
     "iopub.status.idle": "2025-07-03T14:30:25.953620Z",
     "shell.execute_reply": "2025-07-03T14:30:25.952882Z"
    },
    "papermill": {
     "duration": 16.284609,
     "end_time": "2025-07-03T14:30:25.955317",
     "exception": false,
     "start_time": "2025-07-03T14:30:09.670708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 16.25\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** In what novel the number 42 is important?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** The number 42 is famously important in **\"The Hitchhiker's Guide to the Galaxy\" by Douglas Adams.**\n",
       "\n",
       "In the story, a"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"In what novel the number 42 is important?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "981c20fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:30:26.015940Z",
     "iopub.status.busy": "2025-07-03T14:30:26.015646Z",
     "iopub.status.idle": "2025-07-03T14:30:41.812409Z",
     "shell.execute_reply": "2025-07-03T14:30:41.811410Z"
    },
    "papermill": {
     "duration": 15.828738,
     "end_time": "2025-07-03T14:30:41.813862",
     "exception": false,
     "start_time": "2025-07-03T14:30:25.985124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 15.79\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Name the famous boyfriend of Yoko Ono.\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** The famous boyfriend of Yoko Ono was **John Lennon**. \n",
       "\n",
       "They were married from 1969 to 1970.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Name the famous boyfriend of Yoko Ono.\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e797f37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:30:41.873771Z",
     "iopub.status.busy": "2025-07-03T14:30:41.873498Z",
     "iopub.status.idle": "2025-07-03T14:30:57.447380Z",
     "shell.execute_reply": "2025-07-03T14:30:57.446659Z"
    },
    "papermill": {
     "duration": 15.605069,
     "end_time": "2025-07-03T14:30:57.448410",
     "exception": false,
     "start_time": "2025-07-03T14:30:41.843341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 15.57\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Who was nicknamed 'The King' in music?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** There are several musicians who have been nicknamed \"The King\" in music, but the most famous and widely recognized is **Elvis Presley**. \n",
       "\n",
       "However, here"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Who was nicknamed 'The King' in music?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a96ac103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:30:57.506050Z",
     "iopub.status.busy": "2025-07-03T14:30:57.505741Z",
     "iopub.status.idle": "2025-07-03T14:31:09.269473Z",
     "shell.execute_reply": "2025-07-03T14:31:09.267829Z"
    },
    "papermill": {
     "duration": 11.797842,
     "end_time": "2025-07-03T14:31:09.274224",
     "exception": false,
     "start_time": "2025-07-03T14:30:57.476382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 11.76\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What actor played Sheldon in TBBT?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Jim Parsons played Sheldon Cooper in The Big Bang Theory (TBBT). \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What actor played Sheldon in TBBT?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40388808",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:31:09.348813Z",
     "iopub.status.busy": "2025-07-03T14:31:09.348314Z",
     "iopub.status.idle": "2025-07-03T14:31:19.893198Z",
     "shell.execute_reply": "2025-07-03T14:31:19.892506Z"
    },
    "papermill": {
     "duration": 10.57985,
     "end_time": "2025-07-03T14:31:19.895729",
     "exception": false,
     "start_time": "2025-07-03T14:31:09.315879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 10.54\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What acctress from `The Friends` married Brad Pitt?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** This is a trick question! **No actress from \"The Friends\" married Brad"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What acctress from `The Friends` married Brad Pitt?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=16)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0768311",
   "metadata": {
    "papermill": {
     "duration": 0.029854,
     "end_time": "2025-07-03T14:31:19.956657",
     "exception": false,
     "start_time": "2025-07-03T14:31:19.926803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Math questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85f0e513",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:31:20.022464Z",
     "iopub.status.busy": "2025-07-03T14:31:20.022181Z",
     "iopub.status.idle": "2025-07-03T14:31:27.539615Z",
     "shell.execute_reply": "2025-07-03T14:31:27.538914Z"
    },
    "papermill": {
     "duration": 7.552494,
     "end_time": "2025-07-03T14:31:27.541285",
     "exception": false,
     "start_time": "2025-07-03T14:31:19.988791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 7.51\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** 34 + 21\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 34 + 21 = 55\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"34 + 21\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=16)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e051ca4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:31:27.603295Z",
     "iopub.status.busy": "2025-07-03T14:31:27.602937Z",
     "iopub.status.idle": "2025-07-03T14:31:37.198195Z",
     "shell.execute_reply": "2025-07-03T14:31:37.197237Z"
    },
    "papermill": {
     "duration": 9.627532,
     "end_time": "2025-07-03T14:31:37.199339",
     "exception": false,
     "start_time": "2025-07-03T14:31:27.571807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 9.59\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** 49 x 27\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 49 x 27 = 1323\n",
       "\n",
       "Here's"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"49 x 27\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=16)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe70fb85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:31:37.263086Z",
     "iopub.status.busy": "2025-07-03T14:31:37.262742Z",
     "iopub.status.idle": "2025-07-03T14:31:56.429810Z",
     "shell.execute_reply": "2025-07-03T14:31:56.428613Z"
    },
    "papermill": {
     "duration": 19.201043,
     "end_time": "2025-07-03T14:31:56.431141",
     "exception": false,
     "start_time": "2025-07-03T14:31:37.230098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 19.16\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Brian and Sarah are brothers. Brian is 5yo, Sarah is 6 years older. How old is Sarah?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Sarah is 6 years old.\n",
       "\n",
       "Since Sarah is 6 years older than Brian, and Brian is 5, Sarah is 5 + 6 ="
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Brian and Sarah are brothers. Brian is 5yo, Sarah is 6 years older. How old is Sarah?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7a5131d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:31:56.496510Z",
     "iopub.status.busy": "2025-07-03T14:31:56.496148Z",
     "iopub.status.idle": "2025-07-03T14:32:07.585021Z",
     "shell.execute_reply": "2025-07-03T14:32:07.583867Z"
    },
    "papermill": {
     "duration": 11.124066,
     "end_time": "2025-07-03T14:32:07.586750",
     "exception": false,
     "start_time": "2025-07-03T14:31:56.462684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 11.08\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** x + 2 y = 5; y - x = 1. What are x and y? Just return x and y.\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** x = 1\n",
       "y = 2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"x + 2 y = 5; y - x = 1. What are x and y? Just return x and y.\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=64)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a6df3b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:32:07.651616Z",
     "iopub.status.busy": "2025-07-03T14:32:07.651321Z",
     "iopub.status.idle": "2025-07-03T14:32:20.557021Z",
     "shell.execute_reply": "2025-07-03T14:32:20.556426Z"
    },
    "papermill": {
     "duration": 12.939453,
     "end_time": "2025-07-03T14:32:20.558110",
     "exception": false,
     "start_time": "2025-07-03T14:32:07.618657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 12.9\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What is the total area of a sphere or radius 3? Just return the result.\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 113.09733552923255\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What is the total area of a sphere or radius 3? Just return the result.\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=64)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2e2ba1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:32:20.618230Z",
     "iopub.status.busy": "2025-07-03T14:32:20.617979Z",
     "iopub.status.idle": "2025-07-03T14:33:25.875390Z",
     "shell.execute_reply": "2025-07-03T14:33:25.874691Z"
    },
    "papermill": {
     "duration": 65.317753,
     "end_time": "2025-07-03T14:33:25.905096",
     "exception": false,
     "start_time": "2025-07-03T14:32:20.587343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 65.25\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** A rectangle with diagonal 4 is circumscribed by a circle. What is the circle's area?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Let the rectangle have length $l$ and width $w$.\n",
       "Since the rectangle is circumscribed by a circle, the diagonal of the rectangle is equal to the diameter of the circle.\n",
       "We are given that the diagonal of the rectangle is 4, so the diameter of the circle is 4.\n",
       "Therefore, the radius of the circle is $r = \\frac{4}{2} = 2$.\n",
       "The area of the circle is given by the formula $A = \\pi r^2$.\n",
       "Substituting $r=2$, we have $A = \\pi (2^2) = 4\\pi$.\n",
       "\n",
       "Final Answer: The final answer is $\\boxed{4\\pi}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"A rectangle with diagonal 4 is circumscribed by a circle. What is the circle's area?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=200)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2b9b5",
   "metadata": {
    "papermill": {
     "duration": 0.029092,
     "end_time": "2025-07-03T14:33:25.963499",
     "exception": false,
     "start_time": "2025-07-03T14:33:25.934407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Multiple languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba49414a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:33:26.023431Z",
     "iopub.status.busy": "2025-07-03T14:33:26.023162Z",
     "iopub.status.idle": "2025-07-03T14:34:21.494939Z",
     "shell.execute_reply": "2025-07-03T14:34:21.494016Z"
    },
    "papermill": {
     "duration": 55.531995,
     "end_time": "2025-07-03T14:34:21.524922",
     "exception": false,
     "start_time": "2025-07-03T14:33:25.992927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 55.47\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Cine este Mircea Cartarescu?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Mircea Cartarescu este unul dintre cei mai importanți și controversați scriitori români contemporani. Este un autor de ficțiune complexă, care explorează teme precum timpul, memoria, spațiul, identitatea și condiția umană într-o manieră poetică și neconvențională. \n",
       "\n",
       "Iată câteva aspecte cheie despre Mircea Cartarescu:\n",
       "\n",
       "**Cariera și Stil:**\n",
       "\n",
       "*   **Scriitor:** Este cunoscut pentru romanele sale ambițioase, stilul său unic și utilizarea metaforelor complexe.\n",
       "*   **Poet"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Romanian\n",
    "prompt = \"Cine este Mircea Cartarescu?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=128)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adef9942",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:34:21.584077Z",
     "iopub.status.busy": "2025-07-03T14:34:21.583805Z",
     "iopub.status.idle": "2025-07-03T14:35:16.931222Z",
     "shell.execute_reply": "2025-07-03T14:35:16.930221Z"
    },
    "papermill": {
     "duration": 55.405719,
     "end_time": "2025-07-03T14:35:16.959801",
     "exception": false,
     "start_time": "2025-07-03T14:34:21.554082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 55.34\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Kush ishte Ismail Kadare?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Ismail Kadare ishte një shkrija shqiptar i njohur ndërmjetësor i literatura. Ai vildi për ndërrimet politike në Shqipëri, si në mënyrë direkte edhe me alegori dhe simbolizëm.\n",
       "\n",
       "Këtu janë disa pika të rëndësishme për të shkruar për Ismail Kadare:\n",
       "\n",
       "*   **Përancë:** Ai është një nga autorët më të vlerësuar shqiptarë në botë, i njohur me stilin e tij të unikut"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Albanian\n",
    "prompt = \"Kush ishte Ismail Kadare?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=128)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfdb7f61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:35:17.021005Z",
     "iopub.status.busy": "2025-07-03T14:35:17.020695Z",
     "iopub.status.idle": "2025-07-03T14:36:12.371061Z",
     "shell.execute_reply": "2025-07-03T14:36:12.370171Z"
    },
    "papermill": {
     "duration": 55.410855,
     "end_time": "2025-07-03T14:36:12.400621",
     "exception": false,
     "start_time": "2025-07-03T14:35:16.989766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 55.35\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** 夏目漱石とは誰ですか?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 夏目漱石（なつめ そうせき、1867年7月9日 – 1916年4月29日）は、日本の近代文学を代表する作家です。明治時代から昭和時代初期にかけて活躍し、小説、評論、随筆など幅広い分野で才能を発揮しました。\n",
       "\n",
       "**彼の代表的な特徴や業績は以下の通りです。**\n",
       "\n",
       "*   **近代日本文学の確立:** 西洋文学の影響を受けながら、日本の社会や国民性、人間の心理を深く掘り下げた作品を数多く残し、近代日本文学の確立"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Japanese\n",
    "prompt = \"夏目漱石とは誰ですか?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=128)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52d6e3cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:36:12.459473Z",
     "iopub.status.busy": "2025-07-03T14:36:12.459166Z",
     "iopub.status.idle": "2025-07-03T14:37:06.955092Z",
     "shell.execute_reply": "2025-07-03T14:37:06.954045Z"
    },
    "papermill": {
     "duration": 54.556398,
     "end_time": "2025-07-03T14:37:06.985827",
     "exception": false,
     "start_time": "2025-07-03T14:36:12.429429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 54.49\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** 马拉多纳是谁?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 迭戈·马拉多纳（Diego Armando Maradona，1960年10月30日 – 2020年11月25日）是一位阿根廷足球运动员，被广泛认为是足球史上最伟大的球员之一。\n",
       "\n",
       "以下是关于马拉多纳的一些关键信息：\n",
       "\n",
       "* **职业生涯：** 他在阿根廷的俱乐部博卡青年开始职业生涯，后来在欧洲的俱乐部踢球，包括意大利的纳多尔和拿破仑。他以其出色的技术、盘带、组织能力和进球能力而闻名。\n",
       "* **"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Chinese\n",
    "prompt = \"马拉多纳是谁?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=128)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c3a1f3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T14:37:07.048772Z",
     "iopub.status.busy": "2025-07-03T14:37:07.048458Z",
     "iopub.status.idle": "2025-07-03T14:38:02.177650Z",
     "shell.execute_reply": "2025-07-03T14:38:02.176982Z"
    },
    "papermill": {
     "duration": 55.193366,
     "end_time": "2025-07-03T14:38:02.209545",
     "exception": false,
     "start_time": "2025-07-03T14:37:07.016179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 55.12\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Qui était Marguerite Yourcenar?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Marguerite Yourcenar (1900-1983) était une écrivaine française majeure, reconnue pour ses romans historiques exceptionnels et son style d'écriture raffiné et intellectuel. Elle est souvent considérée comme l'une des plus grandes écrivaines françaises du XXe siècle. Voici un résumé de sa vie et de son œuvre :\n",
       "\n",
       "**Sa vie:**\n",
       "\n",
       "* **Origines et éducation:** Née à Nancy en 1900, elle bénéficia d'une éducation brillante et étudiée.  Elle était passionnée par l'histoire et les langues anciennes, notamment le grec"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#French\n",
    "prompt = \"Qui était Marguerite Yourcenar?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=128)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a1c9ae",
   "metadata": {
    "papermill": {
     "duration": 0.028769,
     "end_time": "2025-07-03T14:38:02.269166",
     "exception": false,
     "start_time": "2025-07-03T14:38:02.240397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "\n",
    "Preliminary conclusion after testing the model with:\n",
    "* History questions  \n",
    "* Pop culture  \n",
    "* Math (arithmetics, algebra, geometry)\n",
    "* Multiple languages.\n",
    "  \n",
    "is that the model is performing reasonably well with easy and medium-level questions.\n",
    "\n",
    "**Good points**:\n",
    "- When prompted to answer to the point, the model tend to behave well.\n",
    "- Math seems to be accurate.\n",
    "- Language capability is extensive.\n",
    "\n",
    "**Areas to improve**:\n",
    "- Modify the output to stop at the end of a phrase."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12693789,
     "sourceId": 105267,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 317146,
     "modelInstanceId": 365533,
     "sourceId": 450403,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 804.171317,
   "end_time": "2025-07-03T14:38:05.123173",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-03T14:24:40.951856",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "08a47d00ddb74f938de87b56747349da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0fd033b1d486426c8fdf2ae4ce564975",
        "IPY_MODEL_3347c49409804e99aef03d33a305a1b7",
        "IPY_MODEL_a0980e38b2874275aa2b9c0616862f6d"
       ],
       "layout": "IPY_MODEL_5f1fcb08345645f99f1f7484f4d94c1b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "0fd033b1d486426c8fdf2ae4ce564975": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5b3ae2a13fc4469ba77a40a90a996d2d",
       "placeholder": "​",
       "style": "IPY_MODEL_6d3067b3ffe745a5afc7c012cdd98b29",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "18b006a169624087896c3e4981469596": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3347c49409804e99aef03d33a305a1b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_18b006a169624087896c3e4981469596",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_560c45e40d99484589e39680e04b6ddd",
       "tabbable": null,
       "tooltip": null,
       "value": 3.0
      }
     },
     "560c45e40d99484589e39680e04b6ddd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5b3ae2a13fc4469ba77a40a90a996d2d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5f1fcb08345645f99f1f7484f4d94c1b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6d3067b3ffe745a5afc7c012cdd98b29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "980f934dfe8f49b78aadd646303ac762": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a0980e38b2874275aa2b9c0616862f6d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ce99bd84ea2e4c649caa4ca89c62de3d",
       "placeholder": "​",
       "style": "IPY_MODEL_980f934dfe8f49b78aadd646303ac762",
       "tabbable": null,
       "tooltip": null,
       "value": " 3/3 [00:05&lt;00:00,  1.85s/it]"
      }
     },
     "ce99bd84ea2e4c649caa4ca89c62de3d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
