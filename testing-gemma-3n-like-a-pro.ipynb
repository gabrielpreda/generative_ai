{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "272551d4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.00869,
     "end_time": "2025-07-03T15:12:24.662888",
     "exception": false,
     "start_time": "2025-07-03T15:12:24.654198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook presents a functional prototype built with Gemma 3n, Google’s latest on-device, multimodal AI model. The goal: use compact, private, offline-ready AI to solve a real-world problem. Full demo, code, and technical details follow.\n",
    "\n",
    "## What are the key features of Gemma 3n?\n",
    "\n",
    "The key features of this new model from Google are:\n",
    "\n",
    "1. On-Device Performance\n",
    "Optimized for mobile and edge devices, Gemma 3n delivers real-time AI with minimal memory usage. The 5B and 8B models run like 2B and 4B models, thanks to innovations like Per-Layer Embeddings (PLE).\n",
    "\n",
    "2. Mix’n’Match Model Scaling\n",
    "A single model can act as multiple: the 4B version includes a 2B submodel, enabling dynamic tradeoffs between performance and efficiency. Developers can also create custom-sized submodels tailored to specific tasks.\n",
    "\n",
    "3. Privacy-First and Offline-Ready\n",
    "Gemma 3n runs entirely on-device, ensuring user data never leaves the device. This makes it ideal for privacy-sensitive applications and for use in low- or no-connectivity environments.\n",
    "\n",
    "4. Multimodal Understanding\n",
    "Supports text, image, audio, and enhanced video input, enabling powerful applications like voice interfaces, transcription, translation, visual recognition, and more—all locally.\n",
    "\n",
    "5. Multilingual Proficiency\n",
    "Strong performance across major global languages including Japanese, German, Korean, Spanish, and French, expanding access and inclusivity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac47842",
   "metadata": {
    "papermill": {
     "duration": 0.007113,
     "end_time": "2025-07-03T15:12:24.677675",
     "exception": false,
     "start_time": "2025-07-03T15:12:24.670562",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare the model\n",
    "\n",
    "## Install prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc0614d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:12:24.695092Z",
     "iopub.status.busy": "2025-07-03T15:12:24.694738Z",
     "iopub.status.idle": "2025-07-03T15:14:35.654131Z",
     "shell.execute_reply": "2025-07-03T15:14:35.652841Z"
    },
    "papermill": {
     "duration": 130.970287,
     "end_time": "2025-07-03T15:14:35.656155",
     "exception": false,
     "start_time": "2025-07-03T15:12:24.685868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\r\n",
      "Collecting timm\r\n",
      "  Downloading timm-1.0.16-py3-none-any.whl.metadata (57 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\r\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.31.1)\r\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (25.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.2)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->timm)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->timm)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->timm)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->timm)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->timm)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->timm)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->timm)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.1.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->timm) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->timm) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->timm) (2024.2.0)\r\n",
      "Downloading timm-1.0.16-py3-none-any.whl (2.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\r\n",
      "  Attempting uninstall: timm\r\n",
      "    Found existing installation: timm 1.0.15\r\n",
      "    Uninstalling timm-1.0.15:\r\n",
      "      Successfully uninstalled timm-1.0.15\r\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 timm-1.0.16\r\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\r\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\r\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.31.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\r\n",
      "Collecting git+https://github.com/huggingface/transformers.git\r\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-vda15ptu\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-vda15ptu\r\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit e15b06d8dc6fa132550311d63c9758b580f39bcc\r\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (3.18.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (0.31.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (0.21.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (0.5.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.0.dev0) (4.67.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.54.0.dev0) (2025.3.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.54.0.dev0) (4.13.2)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.54.0.dev0) (1.1.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.54.0.dev0) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.54.0.dev0) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.54.0.dev0) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.54.0.dev0) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.54.0.dev0) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.54.0.dev0) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.54.0.dev0) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.54.0.dev0) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.54.0.dev0) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.54.0.dev0) (2025.4.26)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.54.0.dev0) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.54.0.dev0) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.54.0.dev0) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.54.0.dev0) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.54.0.dev0) (2024.2.0)\r\n",
      "Building wheels for collected packages: transformers\r\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for transformers: filename=transformers-4.54.0.dev0-py3-none-any.whl size=11777247 sha256=3f221b389be6020e6831798d0d3697404bb889b87ca6bd894631ebde9d7f0d37\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-gpm9lncj/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\r\n",
      "Successfully built transformers\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.51.3\r\n",
      "    Uninstalling transformers-4.51.3:\r\n",
      "      Successfully uninstalled transformers-4.51.3\r\n",
      "Successfully installed transformers-4.54.0.dev0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install timm --upgrade\n",
    "!pip install accelerate\n",
    "!pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b935b202",
   "metadata": {
    "papermill": {
     "duration": 0.03483,
     "end_time": "2025-07-03T15:14:35.727040",
     "exception": false,
     "start_time": "2025-07-03T15:14:35.692210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fee55d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:14:35.798850Z",
     "iopub.status.busy": "2025-07-03T15:14:35.798464Z",
     "iopub.status.idle": "2025-07-03T15:15:12.570572Z",
     "shell.execute_reply": "2025-07-03T15:15:12.569495Z"
    },
    "papermill": {
     "duration": 36.810153,
     "end_time": "2025-07-03T15:15:12.572508",
     "exception": false,
     "start_time": "2025-07-03T15:14:35.762355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 15:14:54.523979: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751555694.769544      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751555694.840120      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import kagglehub\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f165d",
   "metadata": {
    "papermill": {
     "duration": 0.108696,
     "end_time": "2025-07-03T15:15:12.717365",
     "exception": false,
     "start_time": "2025-07-03T15:15:12.608669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b8f0ee6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:15:12.797950Z",
     "iopub.status.busy": "2025-07-03T15:15:12.797221Z",
     "iopub.status.idle": "2025-07-03T15:15:30.247554Z",
     "shell.execute_reply": "2025-07-03T15:15:30.245998Z"
    },
    "papermill": {
     "duration": 17.494228,
     "end_time": "2025-07-03T15:15:30.251086",
     "exception": false,
     "start_time": "2025-07-03T15:15:12.756858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190ec11ab06e4801ab111348080fb9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GEMMA_PATH = kagglehub.model_download(\"google/gemma-3n/transformers/gemma-3n-e2b-it\")\n",
    "processor = AutoProcessor.from_pretrained(GEMMA_PATH)\n",
    "model = AutoModelForImageTextToText.from_pretrained(GEMMA_PATH, torch_dtype=\"auto\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8dba8b",
   "metadata": {
    "papermill": {
     "duration": 0.048828,
     "end_time": "2025-07-03T15:15:30.357080",
     "exception": false,
     "start_time": "2025-07-03T15:15:30.308252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test the model with a simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ae6d1fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:15:30.449388Z",
     "iopub.status.busy": "2025-07-03T15:15:30.448744Z",
     "iopub.status.idle": "2025-07-03T15:16:06.129794Z",
     "shell.execute_reply": "2025-07-03T15:16:06.128770Z"
    },
    "papermill": {
     "duration": 35.768444,
     "end_time": "2025-07-03T15:16:06.169839",
     "exception": false,
     "start_time": "2025-07-03T15:15:30.401395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the France capital?\n",
      "\n",
      "Paris is the capital of France.\n",
      "\n",
      "Final Answer: Paris\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"What is the France capital?\"\"\"\n",
    "input_ids = processor(text=prompt, \n",
    "                      return_tensors=\"pt\").to(model.device, \n",
    "                                              dtype=model.dtype)\n",
    "\n",
    "outputs = model.generate(**input_ids, \n",
    "                         max_new_tokens=32, \n",
    "                         disable_compile=True)\n",
    "text = processor.batch_decode(\n",
    "    outputs,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb51732",
   "metadata": {
    "papermill": {
     "duration": 0.035304,
     "end_time": "2025-07-03T15:16:06.241198",
     "exception": false,
     "start_time": "2025-07-03T15:16:06.205894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's wrap this inside a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9905366a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:16:06.313407Z",
     "iopub.status.busy": "2025-07-03T15:16:06.313069Z",
     "iopub.status.idle": "2025-07-03T15:16:06.319755Z",
     "shell.execute_reply": "2025-07-03T15:16:06.318985Z"
    },
    "papermill": {
     "duration": 0.044733,
     "end_time": "2025-07-03T15:16:06.321143",
     "exception": false,
     "start_time": "2025-07-03T15:16:06.276410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_model(prompt, max_new_tokens=32):\n",
    "    start_time = time()\n",
    "    input_ids = processor(text=prompt, \n",
    "                          return_tensors=\"pt\").to(model.device, \n",
    "                                                  dtype=model.dtype)\n",
    "    \n",
    "    outputs = model.generate(**input_ids, \n",
    "                             max_new_tokens=max_new_tokens, \n",
    "                             disable_compile=True)\n",
    "    text = processor.batch_decode(\n",
    "        outputs,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    total_time = round(time() - start_time, 2)\n",
    "    response = text[0].split(prompt)[-1]\n",
    "    return response, total_time\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92fa4545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:16:06.394912Z",
     "iopub.status.busy": "2025-07-03T15:16:06.394582Z",
     "iopub.status.idle": "2025-07-03T15:16:09.839002Z",
     "shell.execute_reply": "2025-07-03T15:16:09.837821Z"
    },
    "papermill": {
     "duration": 3.482876,
     "end_time": "2025-07-03T15:16:09.840667",
     "exception": false,
     "start_time": "2025-07-03T15:16:06.357791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 3.44\n",
      "Question: Quelle est la capitale de la France?\n",
      "Response: \n",
      "\n",
      "Paris.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Quelle est la capitale de la France?\"\n",
    "response, total_time = query_model(prompt, max_new_tokens=16)\n",
    "print(f\"Execution time: {total_time}\")\n",
    "print(f\"Question: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9b4f5",
   "metadata": {
    "papermill": {
     "duration": 0.035094,
     "end_time": "2025-07-03T15:16:09.913840",
     "exception": false,
     "start_time": "2025-07-03T15:16:09.878746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test the model \n",
    "\n",
    "\n",
    "## Let's start with some history questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4b16b23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:16:09.988013Z",
     "iopub.status.busy": "2025-07-03T15:16:09.987626Z",
     "iopub.status.idle": "2025-07-03T15:16:27.393131Z",
     "shell.execute_reply": "2025-07-03T15:16:27.391803Z"
    },
    "papermill": {
     "duration": 17.445138,
     "end_time": "2025-07-03T15:16:27.394757",
     "exception": false,
     "start_time": "2025-07-03T15:16:09.949619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 17.4\n",
      "Question: When started WW2?\n",
      "Response: \n",
      "\n",
      "WW2 started on September 1, 1939, with Germany's invasion of Poland.\n",
      "\n",
      "Final Answer: The final answer is $\\\n"
     ]
    }
   ],
   "source": [
    "prompt = \"When started WW2?\"\n",
    "response, total_time = query_model(prompt, max_new_tokens=32)\n",
    "print(f\"Execution time: {total_time}\")\n",
    "print(f\"Question: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed58e21",
   "metadata": {
    "papermill": {
     "duration": 0.035138,
     "end_time": "2025-07-03T15:16:27.465673",
     "exception": false,
     "start_time": "2025-07-03T15:16:27.430535",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It doesn't look too right, I would like to keep it as short as possible. Let's refine a bit the function, we will add a system prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded00c55",
   "metadata": {
    "papermill": {
     "duration": 0.035285,
     "end_time": "2025-07-03T15:16:27.536508",
     "exception": false,
     "start_time": "2025-07-03T15:16:27.501223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Improve the query function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b212672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:16:27.610419Z",
     "iopub.status.busy": "2025-07-03T15:16:27.609630Z",
     "iopub.status.idle": "2025-07-03T15:16:27.617650Z",
     "shell.execute_reply": "2025-07-03T15:16:27.616702Z"
    },
    "papermill": {
     "duration": 0.047316,
     "end_time": "2025-07-03T15:16:27.619292",
     "exception": false,
     "start_time": "2025-07-03T15:16:27.571976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_model_v2(prompt, max_new_tokens=32):\n",
    "    start_time = time()\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "            You are a smart AI expert in aswering questions.\n",
    "            Just answer to the point, do not elaborate.\n",
    "            For example, if you are asked to provide a year, a name, a location,\n",
    "            return just the information, without any other words.\n",
    "            \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": system_prompt}\n",
    "            ],\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device, dtype=model.dtype)\n",
    "\n",
    "    # retrieve input length\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    \n",
    "    outputs = model.generate(**inputs, \n",
    "                             max_new_tokens=max_new_tokens, \n",
    "                             disable_compile=True)\n",
    "    text = processor.batch_decode(\n",
    "        # use input length to filter only the response from the output\n",
    "        outputs[:, input_len:],\n",
    "        # skip special tokens\n",
    "        skip_special_tokens=True,\n",
    "        # cleanup tokenization spaces\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    total_time = round(time() - start_time, 2)\n",
    "    response = text[0]\n",
    "    return response, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bd3f6c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:16:27.692867Z",
     "iopub.status.busy": "2025-07-03T15:16:27.691818Z",
     "iopub.status.idle": "2025-07-03T15:16:36.183086Z",
     "shell.execute_reply": "2025-07-03T15:16:36.182024Z"
    },
    "papermill": {
     "duration": 8.529102,
     "end_time": "2025-07-03T15:16:36.184606",
     "exception": false,
     "start_time": "2025-07-03T15:16:27.655504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 8.49\n",
      "Question: What year started WW2?\n",
      "Response: World War II started in **1939**. \n"
     ]
    }
   ],
   "source": [
    "prompt = \"What year started WW2?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=12)\n",
    "print(f\"Execution time: {total_time}\")\n",
    "print(f\"Question: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ca3aab",
   "metadata": {
    "papermill": {
     "duration": 0.035169,
     "end_time": "2025-07-03T15:16:36.255497",
     "exception": false,
     "start_time": "2025-07-03T15:16:36.220328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Colorize the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "113629c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:16:36.327602Z",
     "iopub.status.busy": "2025-07-03T15:16:36.326793Z",
     "iopub.status.idle": "2025-07-03T15:16:36.332621Z",
     "shell.execute_reply": "2025-07-03T15:16:36.331711Z"
    },
    "papermill": {
     "duration": 0.04345,
     "end_time": "2025-07-03T15:16:36.334125",
     "exception": false,
     "start_time": "2025-07-03T15:16:36.290675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Reasoning\", \"Question\", \"Response\", \"Explanation\", \"Execution time\"], [\"blue\", \"red\", \"green\", \"darkblue\",  \"magenta\"]):\n",
    "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdbcfef9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:16:36.409425Z",
     "iopub.status.busy": "2025-07-03T15:16:36.408878Z",
     "iopub.status.idle": "2025-07-03T15:16:51.092834Z",
     "shell.execute_reply": "2025-07-03T15:16:51.091898Z"
    },
    "papermill": {
     "duration": 14.72389,
     "end_time": "2025-07-03T15:16:51.094435",
     "exception": false,
     "start_time": "2025-07-03T15:16:36.370545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 14.68\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Between what years was Obama president?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Barack Obama was president of the United States from **2009 to 2017**.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Between what years was Obama president?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14117d07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:16:51.167028Z",
     "iopub.status.busy": "2025-07-03T15:16:51.166655Z",
     "iopub.status.idle": "2025-07-03T15:17:11.139288Z",
     "shell.execute_reply": "2025-07-03T15:17:11.138341Z"
    },
    "papermill": {
     "duration": 20.010457,
     "end_time": "2025-07-03T15:17:11.140735",
     "exception": false,
     "start_time": "2025-07-03T15:16:51.130278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 19.97\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Between what years was the 30 years war?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** The Thirty Years' War was fought between **1618 and 1648**. \n",
       "\n",
       "While it began in 1618 with"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Between what years was the 30 years war?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6413cb3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:17:11.214036Z",
     "iopub.status.busy": "2025-07-03T15:17:11.213682Z",
     "iopub.status.idle": "2025-07-03T15:17:24.019953Z",
     "shell.execute_reply": "2025-07-03T15:17:24.019076Z"
    },
    "papermill": {
     "duration": 12.844213,
     "end_time": "2025-07-03T15:17:24.021519",
     "exception": false,
     "start_time": "2025-07-03T15:17:11.177306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 12.8\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Between what years was the WW1?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** World War I lasted from **1914 to 1918**. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Between what years was the WW1?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8f993e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:17:24.094967Z",
     "iopub.status.busy": "2025-07-03T15:17:24.094629Z",
     "iopub.status.idle": "2025-07-03T15:17:35.956800Z",
     "shell.execute_reply": "2025-07-03T15:17:35.955966Z"
    },
    "papermill": {
     "duration": 11.901203,
     "end_time": "2025-07-03T15:17:35.958611",
     "exception": false,
     "start_time": "2025-07-03T15:17:24.057408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 11.86\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What year was the Lepanto battle?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** The Battle of Lepanto took place in **1571**. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What year was the Lepanto battle?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ae69289",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:17:36.035476Z",
     "iopub.status.busy": "2025-07-03T15:17:36.035110Z",
     "iopub.status.idle": "2025-07-03T15:18:12.658099Z",
     "shell.execute_reply": "2025-07-03T15:18:12.657145Z"
    },
    "papermill": {
     "duration": 36.701483,
     "end_time": "2025-07-03T15:18:12.697758",
     "exception": false,
     "start_time": "2025-07-03T15:17:35.996275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 36.62\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What happened in 1868 in Japan?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 1868 was a pivotal year in Japanese history, marking the end of the Edo period and the beginning of the Meiji Restoration. Here's a breakdown of the key events:\n",
       "\n",
       "*   **The Boshin War (1868-1869):** This was a civil war between"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What happened in 1868 in Japan?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=64)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8707948",
   "metadata": {
    "papermill": {
     "duration": 0.035677,
     "end_time": "2025-07-03T15:18:12.770630",
     "exception": false,
     "start_time": "2025-07-03T15:18:12.734953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's modify the query function to stop the generation after a maximum character number was reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86698f76",
   "metadata": {
    "papermill": {
     "duration": 0.035218,
     "end_time": "2025-07-03T15:18:12.841581",
     "exception": false,
     "start_time": "2025-07-03T15:18:12.806363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Add a custom stopping criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a415fc61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:18:12.915213Z",
     "iopub.status.busy": "2025-07-03T15:18:12.914853Z",
     "iopub.status.idle": "2025-07-03T15:18:12.924744Z",
     "shell.execute_reply": "2025-07-03T15:18:12.923990Z"
    },
    "papermill": {
     "duration": 0.04918,
     "end_time": "2025-07-03T15:18:12.926216",
     "exception": false,
     "start_time": "2025-07-03T15:18:12.877036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class MaxCharLengthCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, max_chars, input_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_chars = max_chars\n",
    "        self.input_len = input_len\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Decode only the generated part\n",
    "        gen_tokens = input_ids[:, self.input_len:]\n",
    "        text = self.tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
    "        return len(text) >= self.max_chars\n",
    "\n",
    "def query_model_v3(prompt, max_chars=128, max_new_tokens=64):\n",
    "    start_time = time()\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "            You are a smart AI expert in aswering questions.\n",
    "            Just answer to the point, do not elaborate.\n",
    "            For example, if you are asked to provide a year, a name, a location,\n",
    "            return just the information, without any other words.\n",
    "            \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": system_prompt}\n",
    "            ],\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device, dtype=model.dtype)\n",
    "\n",
    "    # retrieve input length\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    \n",
    "    stopping_criteria = StoppingCriteriaList([\n",
    "        MaxCharLengthCriteria(processor, max_chars=max_chars, input_len=input_len)\n",
    "    ])\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        disable_compile=True\n",
    "    )\n",
    "\n",
    "    text = processor.batch_decode(\n",
    "        # use input length to filter only the response from the output\n",
    "        outputs[:, input_len:],\n",
    "        # skip special tokens\n",
    "        skip_special_tokens=True,\n",
    "        # cleanup tokenization spaces\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    total_time = round(time() - start_time, 2)\n",
    "    response = text[0]\n",
    "    return response, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2e69135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:18:13.000026Z",
     "iopub.status.busy": "2025-07-03T15:18:12.999068Z",
     "iopub.status.idle": "2025-07-03T15:18:31.611993Z",
     "shell.execute_reply": "2025-07-03T15:18:31.611009Z"
    },
    "papermill": {
     "duration": 18.651361,
     "end_time": "2025-07-03T15:18:31.613834",
     "exception": false,
     "start_time": "2025-07-03T15:18:12.962473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 18.6\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What happened in 1868 in Japan?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 1868 was a pivotal year in Japanese history, marking the end of the Edo period and the beginning of the Meiji Restoration. Here'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What happened in 1868 in Japan?\"\n",
    "response, total_time = query_model_v3(prompt, max_chars=128, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bbf1e6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:18:31.691125Z",
     "iopub.status.busy": "2025-07-03T15:18:31.690350Z",
     "iopub.status.idle": "2025-07-03T15:18:49.196623Z",
     "shell.execute_reply": "2025-07-03T15:18:49.195636Z"
    },
    "papermill": {
     "duration": 17.545521,
     "end_time": "2025-07-03T15:18:49.198269",
     "exception": false,
     "start_time": "2025-07-03T15:18:31.652748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 17.5\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Who was the first American president?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** The first American president was **George Washington**. \n",
       "\n",
       "He served from 1789 to 1797.\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Who was the first American president?\"\n",
    "response, total_time = query_model_v3(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48920237",
   "metadata": {
    "papermill": {
     "duration": 0.036662,
     "end_time": "2025-07-03T15:18:49.274487",
     "exception": false,
     "start_time": "2025-07-03T15:18:49.237825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Let's ask some pop culture question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b310ee11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:18:49.349111Z",
     "iopub.status.busy": "2025-07-03T15:18:49.348743Z",
     "iopub.status.idle": "2025-07-03T15:19:09.244386Z",
     "shell.execute_reply": "2025-07-03T15:19:09.243292Z"
    },
    "papermill": {
     "duration": 19.934722,
     "end_time": "2025-07-03T15:19:09.246092",
     "exception": false,
     "start_time": "2025-07-03T15:18:49.311370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 19.89\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** In what novel the number 42 is important?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** The number 42 is famously important in Douglas Adams's science fiction comedy series, **The Hitchhiker's Guide to the Galaxy**. \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"In what novel the number 42 is important?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a67bf01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:19:09.319881Z",
     "iopub.status.busy": "2025-07-03T15:19:09.319562Z",
     "iopub.status.idle": "2025-07-03T15:19:28.455765Z",
     "shell.execute_reply": "2025-07-03T15:19:28.454903Z"
    },
    "papermill": {
     "duration": 19.174482,
     "end_time": "2025-07-03T15:19:28.457355",
     "exception": false,
     "start_time": "2025-07-03T15:19:09.282873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 19.13\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Name the famous boyfriend of Yoko Ono.\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** The famous boyfriend of Yoko Ono is **John Lennon**. \n",
       "\n",
       "They were married from 1969 to 1970 and a hugely"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Name the famous boyfriend of Yoko Ono.\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0ccfd34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:19:28.531042Z",
     "iopub.status.busy": "2025-07-03T15:19:28.530700Z",
     "iopub.status.idle": "2025-07-03T15:19:48.516223Z",
     "shell.execute_reply": "2025-07-03T15:19:48.515315Z"
    },
    "papermill": {
     "duration": 20.024034,
     "end_time": "2025-07-03T15:19:48.517796",
     "exception": false,
     "start_time": "2025-07-03T15:19:28.493762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 19.98\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Who was nicknamed 'The King' in music?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** There are many musicians who have been nicknamed \"The King\" in music, but the most famous and widely recognized is **Elvis Presley**. \n",
       "\n",
       "He earned the"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Who was nicknamed 'The King' in music?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21c613e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:19:48.593928Z",
     "iopub.status.busy": "2025-07-03T15:19:48.593621Z",
     "iopub.status.idle": "2025-07-03T15:20:00.754019Z",
     "shell.execute_reply": "2025-07-03T15:20:00.753020Z"
    },
    "papermill": {
     "duration": 12.200177,
     "end_time": "2025-07-03T15:20:00.755652",
     "exception": false,
     "start_time": "2025-07-03T15:19:48.555475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 12.15\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What actor played Sheldon in TBBT?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Jim Parsons played Sheldon Cooper in The Big Bang Theory (TBBT). \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What actor played Sheldon in TBBT?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3f93ed2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:20:00.830174Z",
     "iopub.status.busy": "2025-07-03T15:20:00.829808Z",
     "iopub.status.idle": "2025-07-03T15:20:13.313290Z",
     "shell.execute_reply": "2025-07-03T15:20:13.312024Z"
    },
    "papermill": {
     "duration": 12.52241,
     "end_time": "2025-07-03T15:20:13.314874",
     "exception": false,
     "start_time": "2025-07-03T15:20:00.792464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 12.48\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What acctress from `The Friends` married Brad Pitt?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** This is a trick question! There is no actress from \"The Friends\" who"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What acctress from `The Friends` married Brad Pitt?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=16)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1a65de",
   "metadata": {
    "papermill": {
     "duration": 0.036356,
     "end_time": "2025-07-03T15:20:13.390477",
     "exception": false,
     "start_time": "2025-07-03T15:20:13.354121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Math questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a381123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:20:13.465392Z",
     "iopub.status.busy": "2025-07-03T15:20:13.465073Z",
     "iopub.status.idle": "2025-07-03T15:20:21.999209Z",
     "shell.execute_reply": "2025-07-03T15:20:21.998262Z"
    },
    "papermill": {
     "duration": 8.573671,
     "end_time": "2025-07-03T15:20:22.000843",
     "exception": false,
     "start_time": "2025-07-03T15:20:13.427172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 8.53\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** 34 + 21\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 34 + 21 = 55\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"34 + 21\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=16)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4b650de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:20:22.079114Z",
     "iopub.status.busy": "2025-07-03T15:20:22.078746Z",
     "iopub.status.idle": "2025-07-03T15:20:32.641977Z",
     "shell.execute_reply": "2025-07-03T15:20:32.641000Z"
    },
    "papermill": {
     "duration": 10.602724,
     "end_time": "2025-07-03T15:20:32.643609",
     "exception": false,
     "start_time": "2025-07-03T15:20:22.040885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 10.56\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** 49 x 27\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 49 x 27 = 1323\n",
       "\n",
       "Here's"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"49 x 27\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=16)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f779696c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:20:32.720135Z",
     "iopub.status.busy": "2025-07-03T15:20:32.719779Z",
     "iopub.status.idle": "2025-07-03T15:20:55.058329Z",
     "shell.execute_reply": "2025-07-03T15:20:55.057376Z"
    },
    "papermill": {
     "duration": 22.378775,
     "end_time": "2025-07-03T15:20:55.059908",
     "exception": false,
     "start_time": "2025-07-03T15:20:32.681133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 22.33\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Brian and Sarah are brothers. Brian is 5yo, Sarah is 6 years older. How old is Sarah?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Sarah is 6 years old. \n",
       "\n",
       "Since Brian is 5 and Sarah is 6 years older, Sarah is 5 + 6 = 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Brian and Sarah are brothers. Brian is 5yo, Sarah is 6 years older. How old is Sarah?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=32)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f23aa043",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:20:55.139665Z",
     "iopub.status.busy": "2025-07-03T15:20:55.139333Z",
     "iopub.status.idle": "2025-07-03T15:21:06.513091Z",
     "shell.execute_reply": "2025-07-03T15:21:06.512012Z"
    },
    "papermill": {
     "duration": 11.415065,
     "end_time": "2025-07-03T15:21:06.515222",
     "exception": false,
     "start_time": "2025-07-03T15:20:55.100157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 11.37\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** x + 2 y = 5; y - x = 1. What are x and y? Just return x and y.\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** x = 1\n",
       "y = 2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"x + 2 y = 5; y - x = 1. What are x and y? Just return x and y.\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=64)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "059cd30e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:21:06.597662Z",
     "iopub.status.busy": "2025-07-03T15:21:06.597231Z",
     "iopub.status.idle": "2025-07-03T15:21:22.125593Z",
     "shell.execute_reply": "2025-07-03T15:21:22.124489Z"
    },
    "papermill": {
     "duration": 15.572737,
     "end_time": "2025-07-03T15:21:22.127850",
     "exception": false,
     "start_time": "2025-07-03T15:21:06.555113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 15.52\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What is the total area of a sphere or radius 3? Just return the result.\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 113.09733552923255\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"What is the total area of a sphere or radius 3? Just return the result.\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=64)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2e788fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:21:22.209624Z",
     "iopub.status.busy": "2025-07-03T15:21:22.209339Z",
     "iopub.status.idle": "2025-07-03T15:23:14.871484Z",
     "shell.execute_reply": "2025-07-03T15:23:14.870465Z"
    },
    "papermill": {
     "duration": 112.744387,
     "end_time": "2025-07-03T15:23:14.914314",
     "exception": false,
     "start_time": "2025-07-03T15:21:22.169927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 112.66\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** A rectangle with diagonal 4 is circumscribed by a circle. What is the circle's area?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Let the rectangle be $ABCD$, with sides $AB = x$ and $BC = y$. Since the rectangle is circumscribed by a circle, the diameter of the circle is equal to the length of the diagonal of the rectangle.\n",
       "We are given that the diagonal of the rectangle is 4, so the diameter of the circle is 4. Thus, the radius of the circle is $r = \\frac{4}{2} = 2$.\n",
       "The area of the circle is given by the formula $A = \\pi r^2$.\n",
       "Since $r=2$, the area of the circle is $A = \\pi (2^2) = 4\\pi$.\n",
       "\n",
       "Now, we write out the final answer.\n",
       "The diagonal of the rectangle is 4, so the diameter of the circumscribed circle is 4.\n",
       "Therefore, the radius of the circle is $r = \\frac{4}{2} = 2$.\n",
       "The area"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"A rectangle with diagonal 4 is circumscribed by a circle. What is the circle's area?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=200)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88362c2",
   "metadata": {
    "papermill": {
     "duration": 0.037401,
     "end_time": "2025-07-03T15:23:14.989670",
     "exception": false,
     "start_time": "2025-07-03T15:23:14.952269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Multiple languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8bc08082",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:23:15.067859Z",
     "iopub.status.busy": "2025-07-03T15:23:15.067503Z",
     "iopub.status.idle": "2025-07-03T15:24:27.142714Z",
     "shell.execute_reply": "2025-07-03T15:24:27.141766Z"
    },
    "papermill": {
     "duration": 72.154793,
     "end_time": "2025-07-03T15:24:27.182392",
     "exception": false,
     "start_time": "2025-07-03T15:23:15.027599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 72.07\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Cine este Mircea Cartarescu?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Mircea Cartarescu este unul dintre cei mai importanți și influenți scriitori români contemporani. Este recunoscut pentru stilul său unic, neconvențional, complex și poetic, care amestecă elemente de fantastic, suprarealism, postmodernism și postmodernism. \n",
       "\n",
       "Iată câteva aspecte cheie despre Mircea Cartarescu:\n",
       "\n",
       "* **Stil distinctiv:** Caracterizat de folosirea unui limbaj bogat, evocativ și adesea neașteptat, cu metafore și simboluri complexe. Stilul lui Cartarescu este greu de"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Romanian\n",
    "prompt = \"Cine este Mircea Cartarescu?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=128)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c82070c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:24:27.258593Z",
     "iopub.status.busy": "2025-07-03T15:24:27.258293Z",
     "iopub.status.idle": "2025-07-03T15:25:40.059915Z",
     "shell.execute_reply": "2025-07-03T15:25:40.058761Z"
    },
    "papermill": {
     "duration": 72.895733,
     "end_time": "2025-07-03T15:25:40.115183",
     "exception": false,
     "start_time": "2025-07-03T15:24:27.219450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 72.8\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Kush ishte Ismail Kadare?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Ismail Kadare ishte një shkruar shqiptar, i njohur me thellësinë e tij filozofike, shkrimin e tij të shkëlqyer dhe përdorimin e elementeve të historisë, mitologjisë dhe politikanisë shqiptare. Ai konsiderohet një nga më të rëndësishmit dhe më të vlerësuar shkruarës në shqip të shekullit të 20 dhe të 21.\n",
       "\n",
       "**Këtu janë disa pika të rëndësish"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Albanian\n",
    "prompt = \"Kush ishte Ismail Kadare?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=128)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa433596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:25:40.218376Z",
     "iopub.status.busy": "2025-07-03T15:25:40.218011Z",
     "iopub.status.idle": "2025-07-03T15:26:55.370891Z",
     "shell.execute_reply": "2025-07-03T15:26:55.369783Z"
    },
    "papermill": {
     "duration": 75.24533,
     "end_time": "2025-07-03T15:26:55.411320",
     "exception": false,
     "start_time": "2025-07-03T15:25:40.165990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 75.15\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** 夏目漱石とは誰ですか?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 夏目漱石（なつめ そうせき、1867年9月19日 – 1916年4月29日）は、日本の近代文学を代表する作家です。明治時代から大正時代にかけて活躍し、日本の文学史に多大な影響を与えました。\n",
       "\n",
       "**主な特徴と業績:**\n",
       "\n",
       "*   **近代日本の文学の確立:** 西洋文学の影響を受けつつ、日本独自の文化や精神性を反映した作品を数多く残しました。\n",
       "*   **多様な作品:** 小説、評論、随筆など、幅広いジャンルの作品を手"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Japanese\n",
    "prompt = \"夏目漱石とは誰ですか?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=128)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a1dfb42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:26:55.490669Z",
     "iopub.status.busy": "2025-07-03T15:26:55.490312Z",
     "iopub.status.idle": "2025-07-03T15:28:06.475539Z",
     "shell.execute_reply": "2025-07-03T15:28:06.474525Z"
    },
    "papermill": {
     "duration": 71.064055,
     "end_time": "2025-07-03T15:28:06.514444",
     "exception": false,
     "start_time": "2025-07-03T15:26:55.450389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 70.98\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** 马拉多纳是谁?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** 迭戈·马拉多纳（Diego Armando Maradona，1960年10月30日－），是一位阿根廷足球运动员，被广泛认为是足球史上最伟大的球员之一。\n",
       "\n",
       "以下是关于他的一些关键信息：\n",
       "\n",
       "* **职业生涯：** 马拉多纳在职业生涯中效力于多个俱乐部，包括：\n",
       "    * **博卡 Juniors (Boca Juniors):** 从年轻时就加入，成为俱乐部历史上最伟大的球员之一。\n",
       "    * **拿破仑 (Napoli):** 效力于意大利球队，并带领他们赢得了意大利杯"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Chinese\n",
    "prompt = \"马拉多纳是谁?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=128)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6b8fe84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:28:06.595130Z",
     "iopub.status.busy": "2025-07-03T15:28:06.594732Z",
     "iopub.status.idle": "2025-07-03T15:29:18.524755Z",
     "shell.execute_reply": "2025-07-03T15:29:18.523683Z"
    },
    "papermill": {
     "duration": 72.014267,
     "end_time": "2025-07-03T15:29:18.566376",
     "exception": false,
     "start_time": "2025-07-03T15:28:06.552109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='magenta'>Execution time:</font>** 71.92\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Qui était Marguerite Yourcenar?\n",
       "\n",
       "\n",
       "\n",
       "**<font color='green'>Response:</font>** Marguerite Yourcenar (1900-1984) était une écrivaine française majeure, reconnue mondialement pour son style raffiné, son intelligence et son exploration profonde de l'histoire et de la condition humaine. Elle est considérée comme l'une des plus grandes écrivaines du XXe siècle. Voici un résumé de sa vie et de son œuvre :\n",
       "\n",
       "**Vie et parcours:**\n",
       "\n",
       "* **Origines et éducation:** Née à Nancy, en Lorraine, en 1900, elle a reçu une éducation privilégiée et a été influencée par les idées des écrivains et"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#French\n",
    "prompt = \"Qui était Marguerite Yourcenar?\"\n",
    "response, total_time = query_model_v2(prompt, max_new_tokens=128)\n",
    "display(Markdown(colorize_text(f\"Execution time: {total_time}\\n\\nQuestion: {prompt}\\n\\nResponse: {response}\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0770c156",
   "metadata": {
    "papermill": {
     "duration": 0.037931,
     "end_time": "2025-07-03T15:29:18.642703",
     "exception": false,
     "start_time": "2025-07-03T15:29:18.604772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "\n",
    "Preliminary conclusion after testing the model with:\n",
    "* History questions  \n",
    "* Pop culture  \n",
    "* Math (arithmetics, algebra, geometry)\n",
    "* Multiple languages.\n",
    "  \n",
    "is that the model is performing reasonably well with easy and medium-level questions.\n",
    "\n",
    "**Good points**:\n",
    "- When prompted to answer to the point, the model tend to behave well.\n",
    "- Math seems to be accurate.\n",
    "- Language capability is extensive.\n",
    "\n",
    "**Areas to improve**:\n",
    "- Modify the output to stop at the end of a phrase."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12693789,
     "sourceId": 105267,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 317146,
     "modelInstanceId": 365533,
     "sourceId": 450403,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1021.760632,
   "end_time": "2025-07-03T15:29:21.407688",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-03T15:12:19.647056",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "190ec11ab06e4801ab111348080fb9c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_32ffc0d3e85b4dc7a40b31ebdabaec96",
        "IPY_MODEL_ee2a04037ab946cd9907010fe627bbe9",
        "IPY_MODEL_771830e8c6ba4973a29ea28e0df09bde"
       ],
       "layout": "IPY_MODEL_a9aef31f269d48f5b5fdac1ae635dda8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "32ffc0d3e85b4dc7a40b31ebdabaec96": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_77dab1b509194247b531ebfdc176b99d",
       "placeholder": "​",
       "style": "IPY_MODEL_3aab5638caf447f0bc18e081524b1cdc",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "3aab5638caf447f0bc18e081524b1cdc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "771830e8c6ba4973a29ea28e0df09bde": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b8289130e8f04cf8ba7223dad1bee89d",
       "placeholder": "​",
       "style": "IPY_MODEL_dbdfd38e15a940c59ca3a557d1e1a90c",
       "tabbable": null,
       "tooltip": null,
       "value": " 3/3 [00:05&lt;00:00,  1.59s/it]"
      }
     },
     "77dab1b509194247b531ebfdc176b99d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a9aef31f269d48f5b5fdac1ae635dda8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8289130e8f04cf8ba7223dad1bee89d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d2515c6ed2ab4f269eca86cffd156d03": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dbdfd38e15a940c59ca3a557d1e1a90c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ed4a95e77d4c4a7d8fbdeac6b94e8db7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ee2a04037ab946cd9907010fe627bbe9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d2515c6ed2ab4f269eca86cffd156d03",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ed4a95e77d4c4a7d8fbdeac6b94e8db7",
       "tabbable": null,
       "tooltip": null,
       "value": 3.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
