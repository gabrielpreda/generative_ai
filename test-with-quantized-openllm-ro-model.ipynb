{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "480c7f33",
   "metadata": {
    "papermill": {
     "duration": 0.005098,
     "end_time": "2024-05-18T17:45:59.477200",
     "exception": false,
     "start_time": "2024-05-18T17:45:59.472102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "OpenLLM-Ro is a new initiative to develop a Romanian LLM, based on existing large language models. The first version is based on Llama2, fine-tuned with a corpus of Romanian language sources data.\n",
    "\n",
    "For more details about the project, check this link: https://ilds.ro/llm-for-romanian/\n",
    "        \n",
    "## The current model\n",
    "\n",
    "The model we are using here was obtained by quantization of the [OpenLLM-Ro/RoLlama2-7b-Instruct](https://huggingface.co/OpenLLM-Ro/RoLlama2-7b-Instruct) model using the notebook [Quantized Romanian LLM (OpenLLM-Ro) Tests](https://www.kaggle.com/code/gpreda/quantized-romanian-llm-openllm-ro-tests).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3dfb8b",
   "metadata": {
    "papermill": {
     "duration": 0.003823,
     "end_time": "2024-05-18T17:45:59.485621",
     "exception": false,
     "start_time": "2024-05-18T17:45:59.481798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356c60fa",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-05-18T17:45:59.495556Z",
     "iopub.status.busy": "2024-05-18T17:45:59.495145Z",
     "iopub.status.idle": "2024-05-18T17:46:33.487685Z",
     "shell.execute_reply": "2024-05-18T17:46:33.486236Z"
    },
    "papermill": {
     "duration": 34.001084,
     "end_time": "2024-05-18T17:46:33.490844",
     "exception": false,
     "start_time": "2024-05-18T17:45:59.489760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\r\n",
      "Collecting transformers\r\n",
      "  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\r\n",
      "Collecting accelerate\r\n",
      "  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\r\n",
      "Collecting bitsandbytes\r\n",
      "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\r\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\r\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\r\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\r\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.2.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n",
      "Downloading transformers-4.41.0-py3-none-any.whl (9.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading accelerate-0.30.1-py3-none-any.whl (302 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, bitsandbytes, accelerate, transformers\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface-hub 0.22.2\r\n",
      "    Uninstalling huggingface-hub-0.22.2:\r\n",
      "      Successfully uninstalled huggingface-hub-0.22.2\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.15.2\r\n",
      "    Uninstalling tokenizers-0.15.2:\r\n",
      "      Successfully uninstalled tokenizers-0.15.2\r\n",
      "  Attempting uninstall: accelerate\r\n",
      "    Found existing installation: accelerate 0.29.3\r\n",
      "    Uninstalling accelerate-0.29.3:\r\n",
      "      Successfully uninstalled accelerate-0.29.3\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.39.3\r\n",
      "    Uninstalling transformers-4.39.3:\r\n",
      "      Successfully uninstalled transformers-4.39.3\r\n",
      "Successfully installed accelerate-0.30.1 bitsandbytes-0.43.1 huggingface-hub-0.23.0 tokenizers-0.19.1 transformers-4.41.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "095007a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:46:33.524973Z",
     "iopub.status.busy": "2024-05-18T17:46:33.524405Z",
     "iopub.status.idle": "2024-05-18T17:46:53.422410Z",
     "shell.execute_reply": "2024-05-18T17:46:53.420732Z"
    },
    "papermill": {
     "duration": 19.923039,
     "end_time": "2024-05-18T17:46:53.426247",
     "exception": false,
     "start_time": "2024-05-18T17:46:33.503208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-18 17:46:40.235207: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-18 17:46:40.235351: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-18 17:46:40.354225: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from IPython.display import display, Markdown\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c3b0d32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:46:53.458587Z",
     "iopub.status.busy": "2024-05-18T17:46:53.456866Z",
     "iopub.status.idle": "2024-05-18T17:47:35.561226Z",
     "shell.execute_reply": "2024-05-18T17:47:35.560339Z"
    },
    "papermill": {
     "duration": 42.122326,
     "end_time": "2024-05-18T17:47:35.563664",
     "exception": false,
     "start_time": "2024-05-18T17:46:53.441338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", \n",
    "                model=\"/kaggle/input/openllm-ro-q4b/transformers/7b-instruct-q4b/1/RoLlama2-7b-Instruct-v1-q4b\",\n",
    "               kwargs=['_load_in_4bit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a554a105",
   "metadata": {
    "papermill": {
     "duration": 0.010306,
     "end_time": "2024-05-18T17:47:35.584919",
     "exception": false,
     "start_time": "2024-05-18T17:47:35.574613",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare the query function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5818b84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:47:35.608376Z",
     "iopub.status.busy": "2024-05-18T17:47:35.608000Z",
     "iopub.status.idle": "2024-05-18T17:47:35.617455Z",
     "shell.execute_reply": "2024-05-18T17:47:35.616480Z"
    },
    "papermill": {
     "duration": 0.023899,
     "end_time": "2024-05-18T17:47:35.619571",
     "exception": false,
     "start_time": "2024-05-18T17:47:35.595672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_model(\n",
    "        system_message,\n",
    "        user_message,\n",
    "        temperature=0.7,\n",
    "        max_length=1024\n",
    "        ):\n",
    "    start_time = time()\n",
    "    user_message = \"Question: \" + user_message + \" Answer:\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "        ]\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "        )\n",
    "    terminators = [\n",
    "        pipe.tokenizer.eos_token_id,\n",
    "        pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    sequences = pipe(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=temperature,\n",
    "        #num_return_sequences=1,\n",
    "        eos_token_id=terminators,\n",
    "        max_new_tokens=max_length,\n",
    "        return_full_text=False,\n",
    "        pad_token_id=pipe.model.config.eos_token_id\n",
    "    )\n",
    "    #answer = f\"{sequences[0]['generated_text'][len(prompt):]}\\n\"\n",
    "    answer = sequences[0]['generated_text']\n",
    "    end_time = time()\n",
    "    ttime = f\"Total time: {round(end_time-start_time, 2)} sec.\"\n",
    "\n",
    "    return user_message + \" \" + answer  + \" \" +  ttime\n",
    "\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are an AI assistant designed to answer simple questions.\n",
    "Please restrict your answer to the exact question asked.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd521d6c",
   "metadata": {
    "papermill": {
     "duration": 0.01065,
     "end_time": "2024-05-18T17:47:35.640893",
     "exception": false,
     "start_time": "2024-05-18T17:47:35.630243",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test with few questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28d69122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:47:35.663706Z",
     "iopub.status.busy": "2024-05-18T17:47:35.663319Z",
     "iopub.status.idle": "2024-05-18T17:47:35.669106Z",
     "shell.execute_reply": "2024-05-18T17:47:35.668096Z"
    },
    "papermill": {
     "duration": 0.0195,
     "end_time": "2024-05-18T17:47:35.671182",
     "exception": false,
     "start_time": "2024-05-18T17:47:35.651682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Total time\"], [\"blue\", \"red\", \"green\", \"magenta\"]):\n",
    "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a4b267d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:47:35.694974Z",
     "iopub.status.busy": "2024-05-18T17:47:35.694646Z",
     "iopub.status.idle": "2024-05-18T17:47:40.385628Z",
     "shell.execute_reply": "2024-05-18T17:47:40.384485Z"
    },
    "papermill": {
     "duration": 4.705114,
     "end_time": "2024-05-18T17:47:40.388102",
     "exception": false,
     "start_time": "2024-05-18T17:47:35.682988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Care este suprafata Romaniei? \n",
       "\n",
       "**<font color='green'>Answer:</font>**  Suprafața României este de aproximativ 238.391 de kilometri pătrați (92.04 \n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 4.68 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_model(\n",
    "    system_message,\n",
    "    user_message=\"Care este suprafata Romaniei?\",\n",
    "    temperature=0.1,\n",
    "    max_length=32)\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76bdf2ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:47:40.412275Z",
     "iopub.status.busy": "2024-05-18T17:47:40.411815Z",
     "iopub.status.idle": "2024-05-18T17:47:46.783249Z",
     "shell.execute_reply": "2024-05-18T17:47:46.782343Z"
    },
    "papermill": {
     "duration": 6.386034,
     "end_time": "2024-05-18T17:47:46.785631",
     "exception": false,
     "start_time": "2024-05-18T17:47:40.399597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** In ce an a castigat faima mondiala Nadia Comaneci? \n",
       "\n",
       "**<font color='green'>Answer:</font>**  Nadia Comăneci a câștigat faima mondială în 1976, când a devenit prima gimnastă care a obținut nota perfectă de zece în timpul Jocurilor Olimpice de la Montreal.  \n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 6.36 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_model(\n",
    "    system_message,\n",
    "    user_message=\"In ce an a castigat faima mondiala Nadia Comaneci?\",\n",
    "    temperature=0.1,\n",
    "    max_length=64)\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54c35880",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:47:46.808777Z",
     "iopub.status.busy": "2024-05-18T17:47:46.808048Z",
     "iopub.status.idle": "2024-05-18T17:47:53.246156Z",
     "shell.execute_reply": "2024-05-18T17:47:53.245189Z"
    },
    "papermill": {
     "duration": 6.452125,
     "end_time": "2024-05-18T17:47:53.248599",
     "exception": false,
     "start_time": "2024-05-18T17:47:46.796474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Numeste cateva parcuri din Bucuresti. \n",
       "\n",
       "**<font color='green'>Answer:</font>**  Parcurile din București sunt:\n",
       "1. Herăstrău\n",
       "2. Cișmigiu\n",
       "3. Carol\n",
       "4. Kiseleff\n",
       "5. Tineretului\n",
       "6. Floreasca\n",
       "7. Cișmigiu\n",
       "8. Alexandru Io \n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 6.43 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_model(\n",
    "    system_message,\n",
    "    user_message=\"Numeste cateva parcuri din Bucuresti.\",\n",
    "    temperature=0.1,\n",
    "    max_length=64)\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31883352",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:47:53.272307Z",
     "iopub.status.busy": "2024-05-18T17:47:53.271947Z",
     "iopub.status.idle": "2024-05-18T17:47:55.578091Z",
     "shell.execute_reply": "2024-05-18T17:47:55.576955Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 2.320318,
     "end_time": "2024-05-18T17:47:55.580408",
     "exception": false,
     "start_time": "2024-05-18T17:47:53.260090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Cine este actualul presedinte al Romaniei? \n",
       "\n",
       "**<font color='green'>Answer:</font>**  Actualul președinte al României este Klaus Iohannis.  \n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 2.3 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_model(\n",
    "    system_message,\n",
    "    user_message=\"Cine este actualul presedinte al Romaniei?\",\n",
    "    temperature=0.1,\n",
    "    max_length=64)\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96adf968",
   "metadata": {
    "papermill": {
     "duration": 0.01174,
     "end_time": "2024-05-18T17:47:55.604524",
     "exception": false,
     "start_time": "2024-05-18T17:47:55.592784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "Using the quantized model derived from OpenLLM-Ro 7B Instruct (v1) model published by **ILDS**, we can run fast queries in Romanian language."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 43578,
     "sourceId": 51817,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 122.398774,
   "end_time": "2024-05-18T17:47:58.780940",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-18T17:45:56.382166",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
