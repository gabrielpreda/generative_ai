{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ef7aad",
   "metadata": {
    "papermill": {
     "duration": 0.006843,
     "end_time": "2024-04-15T20:43:36.326238",
     "exception": false,
     "start_time": "2024-04-15T20:43:36.319395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction   \n",
    "\n",
    "\n",
    "We will experiment now with the Mistral model.\n",
    "\n",
    "\n",
    "## Model specification  \n",
    "\n",
    "The model details are:\n",
    "\n",
    "* **Model**: Mistral\n",
    "* **Variation**: 7b-v0.1-hf (7b: 7B dimm. hf: HuggingFace build)\n",
    "* **Version**: V1\n",
    "* **Framework**: PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91f019d",
   "metadata": {
    "papermill": {
     "duration": 0.005978,
     "end_time": "2024-04-15T20:43:36.338296",
     "exception": false,
     "start_time": "2024-04-15T20:43:36.332318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Install and import packages  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6fcfb60",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-04-15T20:43:36.352168Z",
     "iopub.status.busy": "2024-04-15T20:43:36.351801Z",
     "iopub.status.idle": "2024-04-15T20:44:35.909714Z",
     "shell.execute_reply": "2024-04-15T20:44:35.908327Z"
    },
    "papermill": {
     "duration": 59.567872,
     "end_time": "2024-04-15T20:44:35.912372",
     "exception": false,
     "start_time": "2024-04-15T20:43:36.344500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U transformers\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae4ffd5",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-04-15T20:44:35.928234Z",
     "iopub.status.busy": "2024-04-15T20:44:35.927385Z",
     "iopub.status.idle": "2024-04-15T20:45:26.500314Z",
     "shell.execute_reply": "2024-04-15T20:45:26.498997Z"
    },
    "papermill": {
     "duration": 50.583553,
     "end_time": "2024-04-15T20:45:26.502972",
     "exception": false,
     "start_time": "2024-04-15T20:44:35.919419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\r\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\r\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\r\n",
      "distributed 2023.7.1 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\r\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\r\n",
      "jupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "momepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\r\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.2 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\r\n",
      "ydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06507f14",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-15T20:45:26.517451Z",
     "iopub.status.busy": "2024-04-15T20:45:26.517129Z",
     "iopub.status.idle": "2024-04-15T20:45:48.541717Z",
     "shell.execute_reply": "2024-04-15T20:45:48.540780Z"
    },
    "papermill": {
     "duration": 22.034752,
     "end_time": "2024-04-15T20:45:48.544176",
     "exception": false,
     "start_time": "2024-04-15T20:45:26.509424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from time import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383f583e",
   "metadata": {
    "papermill": {
     "duration": 0.00628,
     "end_time": "2024-04-15T20:45:48.557119",
     "exception": false,
     "start_time": "2024-04-15T20:45:48.550839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce6e1a1",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-15T20:45:48.571082Z",
     "iopub.status.busy": "2024-04-15T20:45:48.570616Z",
     "iopub.status.idle": "2024-04-15T20:48:40.727392Z",
     "shell.execute_reply": "2024-04-15T20:48:40.726311Z"
    },
    "papermill": {
     "duration": 172.166054,
     "end_time": "2024-04-15T20:48:40.729462",
     "exception": false,
     "start_time": "2024-04-15T20:45:48.563408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626c9e93cef549bd84aedab690c276f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer & pipeline: 172 sec.\n"
     ]
    }
   ],
   "source": [
    "model_id = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'\n",
    "time_1 = time()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model_name = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "print(f\"Tokenizer & pipeline: {round(time() - time_1)} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942293fb",
   "metadata": {
    "papermill": {
     "duration": 0.006468,
     "end_time": "2024-04-15T20:48:40.742703",
     "exception": false,
     "start_time": "2024-04-15T20:48:40.736235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test model  \n",
    "\n",
    "Let's test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30508607",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-15T20:48:40.757495Z",
     "iopub.status.busy": "2024-04-15T20:48:40.756623Z",
     "iopub.status.idle": "2024-04-15T20:48:40.763370Z",
     "shell.execute_reply": "2024-04-15T20:48:40.762423Z"
    },
    "papermill": {
     "duration": 0.015987,
     "end_time": "2024-04-15T20:48:40.765190",
     "exception": false,
     "start_time": "2024-04-15T20:48:40.749203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare pipeline: 0.0 sec.\n"
     ]
    }
   ],
   "source": [
    "time_1 = time()\n",
    "query_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_name,\n",
    "        tokenizer=tokenizer,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.1,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        max_length=256,)\n",
    "time_2 = time()\n",
    "print(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46faf7c3",
   "metadata": {
    "papermill": {
     "duration": 0.006373,
     "end_time": "2024-04-15T20:48:40.778124",
     "exception": false,
     "start_time": "2024-04-15T20:48:40.771751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Let's define a function to test the query pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ce92252",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-15T20:48:40.792864Z",
     "iopub.status.busy": "2024-04-15T20:48:40.792065Z",
     "iopub.status.idle": "2024-04-15T20:48:40.798387Z",
     "shell.execute_reply": "2024-04-15T20:48:40.797534Z"
    },
    "papermill": {
     "duration": 0.01582,
     "end_time": "2024-04-15T20:48:40.800513",
     "exception": false,
     "start_time": "2024-04-15T20:48:40.784693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(tokenizer, pipeline, prompt_to_test):\n",
    "    \"\"\"\n",
    "    Perform a query\n",
    "    print the result\n",
    "    Args:\n",
    "        tokenizer: the tokenizer\n",
    "        pipeline: the pipeline\n",
    "        prompt_to_test: the prompt\n",
    "    Returns\n",
    "        None\n",
    "    \"\"\"\n",
    "    # adapted from https://huggingface.co/blog/llama2#using-transformers\n",
    "    time_1 = time()\n",
    "    sequences = pipeline(\n",
    "        prompt_to_test,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.1,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,)\n",
    "    time_2 = time()\n",
    "    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1435c60f",
   "metadata": {
    "papermill": {
     "duration": 0.006387,
     "end_time": "2024-04-15T20:48:40.813984",
     "exception": false,
     "start_time": "2024-04-15T20:48:40.807597",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's try a question about touristic attractions in France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61e8181f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T20:48:40.828806Z",
     "iopub.status.busy": "2024-04-15T20:48:40.828507Z",
     "iopub.status.idle": "2024-04-15T20:48:56.367233Z",
     "shell.execute_reply": "2024-04-15T20:48:56.366028Z"
    },
    "papermill": {
     "duration": 15.548355,
     "end_time": "2024-04-15T20:48:56.369363",
     "exception": false,
     "start_time": "2024-04-15T20:48:40.821008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test inference: 15.534 sec.\n",
      "Result: Please let me know which cities are mostly visited in France.\n",
      "\n",
      "Comment: @JulienBernard I'm not sure if you are asking about the number of visitors or the number of tourist attractions.\n",
      "\n",
      "Comment: @JulienBernard I'm asking about the number of visitors.\n",
      "\n",
      "Comment: @JulienBernard I'm asking about the number of visitors.\n",
      "\n",
      "Comment: @JulienBernard I'm asking about the number of visitors.\n",
      "\n",
      "Comment: @JulienBernard I'm asking about the number of visitors.\n",
      "\n",
      "Comment: @JulienBernard I'm asking about the number of visitors.\n",
      "\n",
      "Comment: @JulienBernard I'm asking about the number of visitors.\n",
      "\n",
      "Comment: @JulienBernard I'm asking about the number of visitors.\n",
      "\n",
      "Comment: @Julien\n"
     ]
    }
   ],
   "source": [
    "test_model(tokenizer,\n",
    "           query_pipeline,\n",
    "           \"Please let me know which cities are mostly visited in France.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbddb29f",
   "metadata": {
    "papermill": {
     "duration": 0.006871,
     "end_time": "2024-04-15T20:48:56.383693",
     "exception": false,
     "start_time": "2024-04-15T20:48:56.376822",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's adjust the prompt, since we ar not really happy with this answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb90efa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T20:48:56.399112Z",
     "iopub.status.busy": "2024-04-15T20:48:56.398750Z",
     "iopub.status.idle": "2024-04-15T20:48:57.997493Z",
     "shell.execute_reply": "2024-04-15T20:48:57.996257Z"
    },
    "papermill": {
     "duration": 1.609257,
     "end_time": "2024-04-15T20:48:57.999915",
     "exception": false,
     "start_time": "2024-04-15T20:48:56.390658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test inference: 1.594 sec.\n",
      "Result: What are the three most visited cities in France?\n",
      "\n",
      "The three most visited cities in France are Paris, Marseille, and Lyon.\n"
     ]
    }
   ],
   "source": [
    "test_model(tokenizer,\n",
    "           query_pipeline,\n",
    "           \"What are the three most visited cities in France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0835dc5",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-15T20:48:58.017573Z",
     "iopub.status.busy": "2024-04-15T20:48:58.016946Z",
     "iopub.status.idle": "2024-04-15T20:49:00.368287Z",
     "shell.execute_reply": "2024-04-15T20:49:00.367211Z"
    },
    "papermill": {
     "duration": 2.362762,
     "end_time": "2024-04-15T20:49:00.370454",
     "exception": false,
     "start_time": "2024-04-15T20:48:58.007692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test inference: 2.346 sec.\n",
      "Result: What are the three most visited tourist attractions in Paris?\n",
      "\n",
      "The three most visited tourist attractions in Paris are the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\n"
     ]
    }
   ],
   "source": [
    "test_model(tokenizer,\n",
    "           query_pipeline,\n",
    "           \"What are the three most visited tourist attractions in Paris?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68007724",
   "metadata": {
    "papermill": {
     "duration": 0.007082,
     "end_time": "2024-04-15T20:49:00.384942",
     "exception": false,
     "start_time": "2024-04-15T20:49:00.377860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It looks like how the prompt is created is really important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad7b594",
   "metadata": {
    "papermill": {
     "duration": 0.007006,
     "end_time": "2024-04-15T20:49:00.399076",
     "exception": false,
     "start_time": "2024-04-15T20:49:00.392070",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define and execute the sequential chain  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cf5e535",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-15T20:49:00.414569Z",
     "iopub.status.busy": "2024-04-15T20:49:00.414293Z",
     "iopub.status.idle": "2024-04-15T20:49:00.419447Z",
     "shell.execute_reply": "2024-04-15T20:49:00.418745Z"
    },
    "papermill": {
     "duration": 0.01509,
     "end_time": "2024-04-15T20:49:00.421289",
     "exception": false,
     "start_time": "2024-04-15T20:49:00.406199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=query_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43ca6099",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T20:49:00.436561Z",
     "iopub.status.busy": "2024-04-15T20:49:00.436291Z",
     "iopub.status.idle": "2024-04-15T20:49:02.701573Z",
     "shell.execute_reply": "2024-04-15T20:49:02.700643Z"
    },
    "papermill": {
     "duration": 2.275191,
     "end_time": "2024-04-15T20:49:02.703632",
     "exception": false,
     "start_time": "2024-04-15T20:49:00.428441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What are the three most visited cities in France?\\n\\nThe three most visited cities in France are Paris, Marseille, and Lyon.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What are the three most visited cities in France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6284c5ee",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-15T20:49:02.721201Z",
     "iopub.status.busy": "2024-04-15T20:49:02.720537Z",
     "iopub.status.idle": "2024-04-15T20:49:02.727368Z",
     "shell.execute_reply": "2024-04-15T20:49:02.726539Z"
    },
    "papermill": {
     "duration": 0.016951,
     "end_time": "2024-04-15T20:49:02.729130",
     "exception": false,
     "start_time": "2024-04-15T20:49:02.712179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sequential_chain(country, llm):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        country: country selected\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    time_1 = time()\n",
    "    template = \"What is the most popular food in {country} for tourists?\"\n",
    "\n",
    "    #  first task in chain\n",
    "    first_prompt = PromptTemplate(\n",
    "        input_variables=[\"country\"],\n",
    "        template=template)\n",
    "\n",
    "    chain_one = LLMChain(llm = llm, prompt = first_prompt)\n",
    "\n",
    "    # second step in chain\n",
    "    second_prompt = PromptTemplate(\n",
    "        input_variables=[\"food\"],\n",
    "        template=\"What are the top three ingredients in {food}. Just return the answer as three bullet points.\",)\n",
    "\n",
    "    chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "    # combine the two steps and run the chain sequence\n",
    "    overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)\n",
    "    overall_chain.run(country)\n",
    "    time_2 = time()\n",
    "    print(f\"Run sequential chain: {round(time_2-time_1, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "117aa00c",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-15T20:49:02.745450Z",
     "iopub.status.busy": "2024-04-15T20:49:02.745162Z",
     "iopub.status.idle": "2024-04-15T20:49:18.264301Z",
     "shell.execute_reply": "2024-04-15T20:49:18.262975Z"
    },
    "papermill": {
     "duration": 15.53042,
     "end_time": "2024-04-15T20:49:18.267031",
     "exception": false,
     "start_time": "2024-04-15T20:49:02.736611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mWhat is the most popular food in France for tourists?\n",
      "\n",
      "The most popular food for tourists in France is undoubtedly escargot, a dish made from snails cooked in garlic butter. Other popular dishes include croissants, macarons, cheese, and wine.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mWhat are the top three ingredients in What is the most popular food in France for tourists?\n",
      "\n",
      "The most popular food for tourists in France is undoubtedly escargot, a dish made from snails cooked in garlic butter. Other popular dishes include croissants, macarons, cheese, and wine.. Just return the answer as three bullet points.\n",
      "\n",
      "* Escargot\n",
      "* Croissants\n",
      "* Macarons\n",
      "\n",
      "## Answer (1)\n",
      "\n",
      "The most popular food in France for tourists is undoubtedly escargot, a dish made from snails cooked in garlic butter. Other popular dishes include croissants, macarons, cheese, and wine.\n",
      "\n",
      "Comment: I'm not sure if this is the most popular food for tourists, but it's certainly one of the most famous.\n",
      "\n",
      "Comment: @JonathanReez: I'm not sure either, but I've seen it mentioned more often than any other dish.\n",
      "\n",
      "Comment: @JonathanReez: I've seen it mentioned more often than any other dish.\n",
      "\n",
      "Comment: @JonathanReez: I've seen it mentioned more often than any other dish.\n",
      "\n",
      "Comment: @J\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Run sequential chain: 15.514 sec.\n"
     ]
    }
   ],
   "source": [
    "final_answer = sequential_chain(\"France\", llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ec4821a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T20:49:18.284812Z",
     "iopub.status.busy": "2024-04-15T20:49:18.284505Z",
     "iopub.status.idle": "2024-04-15T20:49:26.917532Z",
     "shell.execute_reply": "2024-04-15T20:49:26.916377Z"
    },
    "papermill": {
     "duration": 8.643924,
     "end_time": "2024-04-15T20:49:26.919620",
     "exception": false,
     "start_time": "2024-04-15T20:49:18.275696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mWhat is the most popular food in Italy for tourists?\n",
      "\n",
      "Pizza and pasta are two of the most popular foods in Italy for tourists. Pizza is a versatile dish that can be found all over Italy, with different regional variations. It's a great option for a quick and easy meal. Pasta, on the other hand, is a staple of Italian cuisine and can be found in many different forms, from spaghetti to lasagna to ravioli. Both pizza and pasta are often served with a variety of sauces, cheeses, and toppings.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mWhat are the top three ingredients in What is the most popular food in Italy for tourists?\n",
      "\n",
      "Pizza and pasta are two of the most popular foods in Italy for tourists. Pizza is a versatile dish that can be found all over Italy, with different regional variations. It's a great option for a quick and easy meal. Pasta, on the other hand, is a staple of Italian cuisine and can be found in many different forms, from spaghetti to lasagna to ravioli. Both pizza and pasta are often served with a variety of sauces, cheeses, and toppings.. Just return the answer as three bullet points.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Run sequential chain: 8.629 sec.\n"
     ]
    }
   ],
   "source": [
    "final_answer = sequential_chain(\"Italy\", llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53de36d",
   "metadata": {
    "papermill": {
     "duration": 0.00796,
     "end_time": "2024-04-15T20:49:26.936051",
     "exception": false,
     "start_time": "2024-04-15T20:49:26.928091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusions   \n",
    "\n",
    "\n",
    "Mistral `7b-v0.1-hf`, with careful adjusted queries, seems to work just fine.  \n",
    "We had to adjust carefully the prompts to get a correct answer."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 3899,
     "sourceId": 5111,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 358.545619,
   "end_time": "2024-04-15T20:49:30.140192",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-15T20:43:31.594573",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "17db5755d1254a35ba9898cbc6b310a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2dcb855fc9ac48619863b546f0f9be71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3957941eae764b46b287fb1155c9facb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3fc3ad15cd9b48fa9f855cd3e2ac67e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4b23cdedb5154e048a534c590fbfe8e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "626c9e93cef549bd84aedab690c276f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6c78beaa3e644e149db46fe4ac2ea12f",
        "IPY_MODEL_e39e51467e7b43ec89cc1cd21bcfac9a",
        "IPY_MODEL_fbe7fba088434636833ab0b134973ef8"
       ],
       "layout": "IPY_MODEL_17db5755d1254a35ba9898cbc6b310a4"
      }
     },
     "6c78beaa3e644e149db46fe4ac2ea12f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e04134f8b0b04906b4e1a5637dfdce0f",
       "placeholder": "​",
       "style": "IPY_MODEL_4b23cdedb5154e048a534c590fbfe8e9",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "e04134f8b0b04906b4e1a5637dfdce0f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e39e51467e7b43ec89cc1cd21bcfac9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3957941eae764b46b287fb1155c9facb",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3fc3ad15cd9b48fa9f855cd3e2ac67e7",
       "value": 2.0
      }
     },
     "f1abf2d00aef48cc80f84c7a00ce73dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fbe7fba088434636833ab0b134973ef8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2dcb855fc9ac48619863b546f0f9be71",
       "placeholder": "​",
       "style": "IPY_MODEL_f1abf2d00aef48cc80f84c7a00ce73dc",
       "value": " 2/2 [02:49&lt;00:00, 79.95s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
