{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gpreda/simple-sequential-chain-with-llama-2-and-langchain?scriptVersionId=144684662\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Introduction\n\n\n## Objective  \n\nUse Llama 2 and Langchain to create a multi-step task chain. \n\n## Model details  \n\n* **Model**: Llama 2  \n* **Variation**: 7b-chat-hf    \n* **Version**: V1  \n* **Framework**: PyTorch  \n\nLlaMA 2 model is pretrained and fine-tuned with 2 Trillion tokens and 7 to 70 Billion parameters which makes it one of the powerful open source models. It is a highly improvement over LlaMA 1 model.","metadata":{}},{"cell_type":"markdown","source":"# InstalIing, imports, utils","metadata":{}},{"cell_type":"markdown","source":"Install packages.","metadata":{}},{"cell_type":"code","source":"!pip install transformers accelerate einops langchain xformers bitsandbytes chromadb sentence_transformers","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-26T18:24:58.446762Z","iopub.execute_input":"2023-09-26T18:24:58.44745Z","iopub.status.idle":"2023-09-26T18:28:37.509154Z","shell.execute_reply.started":"2023-09-26T18:24:58.447412Z","shell.execute_reply":"2023-09-26T18:28:37.507889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import packages.","metadata":{}},{"cell_type":"code","source":"from torch import cuda, bfloat16\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer\nfrom time import time\n\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.chains import LLMChain, SimpleSequentialChain\nfrom langchain import PromptTemplate\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-26T18:28:37.514432Z","iopub.execute_input":"2023-09-26T18:28:37.515309Z","iopub.status.idle":"2023-09-26T18:28:46.609988Z","shell.execute_reply.started":"2023-09-26T18:28:37.515262Z","shell.execute_reply":"2023-09-26T18:28:46.607955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initialize model, tokenizer, query pipeline  \n\nDefine the model, the device, and the bitsandbytes configuration.","metadata":{}},{"cell_type":"code","source":"model_id = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n# set quantization configuration to load large model with less GPU memory\n# this requires the `bitsandbytes` library\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-26T18:28:46.611418Z","iopub.execute_input":"2023-09-26T18:28:46.611966Z","iopub.status.idle":"2023-09-26T18:28:46.734825Z","shell.execute_reply.started":"2023-09-26T18:28:46.611929Z","shell.execute_reply":"2023-09-26T18:28:46.733897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prepare the model and the tokenizer.","metadata":{}},{"cell_type":"code","source":"time_1 = time()\nmodel_config = transformers.AutoConfig.from_pretrained(\n    model_id,\n)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n    device_map='auto',\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntime_2 = time()\nprint(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-26T18:28:46.73687Z","iopub.execute_input":"2023-09-26T18:28:46.737237Z","iopub.status.idle":"2023-09-26T18:32:01.815179Z","shell.execute_reply.started":"2023-09-26T18:28:46.737202Z","shell.execute_reply":"2023-09-26T18:32:01.8141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define a pipeline. ","metadata":{}},{"cell_type":"code","source":"time_1 = time()\nquery_pipeline = transformers.pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",)\ntime_2 = time()\nprint(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")\n\nllm = HuggingFacePipeline(pipeline=query_pipeline)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-26T18:34:35.008875Z","iopub.execute_input":"2023-09-26T18:34:35.009477Z","iopub.status.idle":"2023-09-26T18:34:37.284463Z","shell.execute_reply.started":"2023-09-26T18:34:35.009444Z","shell.execute_reply":"2023-09-26T18:34:37.283432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nWe test it by running a simple query.","metadata":{}},{"cell_type":"code","source":"# checking again that everything is working fine\nllm(prompt=\"What is the most popular city in France for tourists? Just return the name of the city.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-26T18:34:44.423957Z","iopub.execute_input":"2023-09-26T18:34:44.425014Z","iopub.status.idle":"2023-09-26T18:34:54.999606Z","shell.execute_reply.started":"2023-09-26T18:34:44.424955Z","shell.execute_reply":"2023-09-26T18:34:54.998512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define and execute the sequential chain\n\n\nWe define a chain with two tasks in sequence.  \nThe input for the second step is the output of the first step.  ","metadata":{}},{"cell_type":"code","source":"def sequential_chain(country):\n    \"\"\"\n    Args:\n        country: country selected\n    Returns:\n        None\n    \"\"\"\n    time_1 = time()\n    template = \"What is the most popular city in {country} for tourists? Just return the name of the city.\"\n\n    #  first task in chain\n    first_prompt = PromptTemplate(\n\n    input_variables=[\"country\"],\n\n    template=template)\n\n    chain_one = LLMChain(llm = llm, prompt = first_prompt)\n\n    # second step in chain\n    second_prompt = PromptTemplate(\n\n    input_variables=[\"city\"],\n\n    template=\"What are the top three things to do in this: {city} for tourists. Just return the answer as three bullet points.\",)\n\n    chain_two = LLMChain(llm=llm, prompt=second_prompt)\n\n    # combine the two steps and run the chain sequence\n    overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)\n    overall_chain.run(country)\n    time_2 = time()\n    print(f\"Run sequential chain: {round(time_2-time_1, 3)} sec.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-26T18:42:59.650376Z","iopub.execute_input":"2023-09-26T18:42:59.650766Z","iopub.status.idle":"2023-09-26T18:42:59.658196Z","shell.execute_reply.started":"2023-09-26T18:42:59.650733Z","shell.execute_reply":"2023-09-26T18:42:59.657127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_answer = sequential_chain(\"France\")","metadata":{"execution":{"iopub.status.busy":"2023-09-26T18:43:07.975213Z","iopub.execute_input":"2023-09-26T18:43:07.975587Z","iopub.status.idle":"2023-09-26T18:43:27.914537Z","shell.execute_reply.started":"2023-09-26T18:43:07.975555Z","shell.execute_reply":"2023-09-26T18:43:27.913413Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_answer = sequential_chain(\"Germany\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-26T19:24:32.117139Z","iopub.execute_input":"2023-09-26T19:24:32.117545Z","iopub.status.idle":"2023-09-26T19:25:14.72015Z","shell.execute_reply.started":"2023-09-26T19:24:32.117508Z","shell.execute_reply":"2023-09-26T19:25:14.719061Z"},"trusted":true},"execution_count":null,"outputs":[]}]}