{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e04544a5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.008937,
     "end_time": "2024-03-31T21:41:54.871038",
     "exception": false,
     "start_time": "2024-03-31T21:41:54.862101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center><h1>Google AI Assistant for Data Science teaching</h1></center>\n",
    "\n",
    "<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This Notebook will build an AI Assistant for Data Science teaching.\n",
    "\n",
    "## The method\n",
    "\n",
    "We will fine-tune a LLM with Data Science questions and answers. Then we will create a custom class that will query this model.\n",
    "\n",
    "## The model\n",
    "\n",
    "As model, we will use Gemma model, fine-tuned with our Data Science Q&A data, using LoRA.\n",
    "\n",
    "## The data\n",
    "\n",
    "As data for fine-tuning Gemma, we will use [Data Science Q&A Treasury](https://www.kaggle.com/datasets/memocan/data-science-interview-q-and-a-treasury) data.\n",
    "\n",
    "## Previous work\n",
    "\n",
    "This work is largely based on previous work. Here I list the sources:\n",
    "\n",
    "1. Gemma Model Card, Kaggle Models, https://www.kaggle.com/models/google/gemma\n",
    "2. Kaggle QA with Gemma - KerasNLP Starter, Kaggle Code, https://www.kaggle.com/code/awsaf49/kaggle-qa-with-gemma-kerasnlp-starter (Version 11)\n",
    "3. Fine-tune Gemma models in Keras using LoRA, Kaggle Code, https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora (Version 1)\n",
    "4. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, LoRA: Low-Rank Adaptation of Large Language Models, ArXiv, https://arxiv.org/pdf/2106.09685.pdf\n",
    "5. Abheesht Sharma, Matthew Watson, Parameter-efficient fine-tuning of GPT-2 with LoRA, https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/\n",
    "6. Keras 3 API documentation / KerasNLP / Models / Gemma, https://keras.io/api/keras_nlp/models/gemma/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9caf13b",
   "metadata": {
    "papermill": {
     "duration": 0.008154,
     "end_time": "2024-03-31T21:41:54.887616",
     "exception": false,
     "start_time": "2024-03-31T21:41:54.879462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction about Gemma\n",
    "\n",
    "\n",
    "Gemma is a collection of lightweight source generative AI models designed to be used mostly by developers and researchers. Created by Google DeepMind research lab that also developed Gemini, Gemma is available in several versions, with 2B and 7B parameters, as following:\n",
    "\n",
    "| Model                  | Parameters      | Tuned versions    | Description                                    | Recomemnded target platforms       |\n",
    "|------------------------|-----------------|-------------------|------------------------------------------------|------------------------------------|\n",
    "| `gemma_2b_en`          | 2.51B           | Pretrained        | 18-layer Gemma model (Gemma with 2B parameters)|Mobile devices and laptops          |\n",
    "| `gemma_instruct_2b_en` | 2.51B           | Instruction tuned | 18-layer Gemma model (Gemma with 2B parameters)| Mobile devices and laptops         | \n",
    "| `gemma_7b_en`          | 8.54B           | Pretrained        | 28-layer Gemma model (Gemma with 7B parameters)| Desktop computers and small servers|\n",
    "| `gemma_instruct_7b_en` | 8.54B           | Instruction tuned | 28-layer Gemma model (Gemma with 7B parameters)| Desktop computers and small servers|\n",
    "\n",
    "\n",
    "For this notebook, we will fine-tune `gemma_2b_en` model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb8612",
   "metadata": {
    "papermill": {
     "duration": 0.008134,
     "end_time": "2024-03-31T21:41:54.904078",
     "exception": false,
     "start_time": "2024-03-31T21:41:54.895944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LoRA introduction\n",
    "\n",
    "LoRA stands for Low-Rank Adaptation. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to LoRA paper, this number decreases 10,000 times, and the computational resources size decreases 3 times. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a30b11",
   "metadata": {
    "papermill": {
     "duration": 0.008081,
     "end_time": "2024-03-31T21:41:54.920533",
     "exception": false,
     "start_time": "2024-03-31T21:41:54.912452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Installations and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5527a21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T21:41:54.938549Z",
     "iopub.status.busy": "2024-03-31T21:41:54.938246Z",
     "iopub.status.idle": "2024-03-31T21:42:25.238636Z",
     "shell.execute_reply": "2024-03-31T21:42:25.237667Z"
    },
    "papermill": {
     "duration": 30.312401,
     "end_time": "2024-03-31T21:42:25.241215",
     "exception": false,
     "start_time": "2024-03-31T21:41:54.928814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras>=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db69a60d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T21:42:25.261439Z",
     "iopub.status.busy": "2024-03-31T21:42:25.261109Z",
     "iopub.status.idle": "2024-03-31T21:42:39.594654Z",
     "shell.execute_reply": "2024-03-31T21:42:39.593669Z"
    },
    "papermill": {
     "duration": 14.3456,
     "end_time": "2024-03-31T21:42:39.596969",
     "exception": false,
     "start_time": "2024-03-31T21:42:25.251369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 21:42:29.540505: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-31 21:42:29.540623: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-31 21:42:29.667879: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas() # progress bar for pandas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b96a83c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T21:42:39.622085Z",
     "iopub.status.busy": "2024-03-31T21:42:39.621239Z",
     "iopub.status.idle": "2024-03-31T21:42:39.627260Z",
     "shell.execute_reply": "2024-03-31T21:42:39.626353Z"
    },
    "papermill": {
     "duration": 0.022828,
     "end_time": "2024-03-31T21:42:39.631050",
     "exception": false,
     "start_time": "2024-03-31T21:42:39.608222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    seed = 42\n",
    "    dataset_path = \"/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv\"\n",
    "    preset = \"gemma_2b_en\" # name of pretrained Gemma\n",
    "    sequence_length = 512 # max size of input sequence for training\n",
    "    batch_size = 1 # size of the input batch in training, x 2 as two GPUs\n",
    "    epochs = 15 # number of epochs to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "965e443c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T21:42:39.656570Z",
     "iopub.status.busy": "2024-03-31T21:42:39.655776Z",
     "iopub.status.idle": "2024-03-31T21:42:39.662711Z",
     "shell.execute_reply": "2024-03-31T21:42:39.661752Z"
    },
    "papermill": {
     "duration": 0.021429,
     "end_time": "2024-03-31T21:42:39.665388",
     "exception": false,
     "start_time": "2024-03-31T21:42:39.643959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(Config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62ad3c2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T21:42:39.685753Z",
     "iopub.status.busy": "2024-03-31T21:42:39.684817Z",
     "iopub.status.idle": "2024-03-31T21:42:39.690289Z",
     "shell.execute_reply": "2024-03-31T21:42:39.689457Z"
    },
    "papermill": {
     "duration": 0.017163,
     "end_time": "2024-03-31T21:42:39.692209",
     "exception": false,
     "start_time": "2024-03-31T21:42:39.675046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Question\", \"Answer\"], [\"blue\", \"red\"]):\n",
    "        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba10aa",
   "metadata": {
    "papermill": {
     "duration": 0.008508,
     "end_time": "2024-03-31T21:42:39.709408",
     "exception": false,
     "start_time": "2024-03-31T21:42:39.700900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edd8c7d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T21:42:39.728002Z",
     "iopub.status.busy": "2024-03-31T21:42:39.727737Z",
     "iopub.status.idle": "2024-03-31T21:42:39.755522Z",
     "shell.execute_reply": "2024-03-31T21:42:39.754847Z"
    },
    "papermill": {
     "duration": 0.039204,
     "end_time": "2024-03-31T21:42:39.757419",
     "exception": false,
     "start_time": "2024-03-31T21:42:39.718215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is supervised machine learning? 👶</td>\n",
       "      <td>Supervised learning is a type of machine learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is regression? Which models can you use t...</td>\n",
       "      <td>Regression is a part of supervised ML. Regress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is linear regression? When do we use it? 👶</td>\n",
       "      <td>Linear regression is a model that assumes a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the main assumptions of linear regres...</td>\n",
       "      <td>There are several assumptions of linear regres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What’s the normal distribution? Why do we care...</td>\n",
       "      <td>The normal distribution is a continuous probab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0             What is supervised machine learning? 👶   \n",
       "1  What is regression? Which models can you use t...   \n",
       "2    What is linear regression? When do we use it? 👶   \n",
       "3  What are the main assumptions of linear regres...   \n",
       "4  What’s the normal distribution? Why do we care...   \n",
       "\n",
       "                                              answer  \n",
       "0  Supervised learning is a type of machine learn...  \n",
       "1  Regression is a part of supervised ML. Regress...  \n",
       "2  Linear regression is a model that assumes a li...  \n",
       "3  There are several assumptions of linear regres...  \n",
       "4  The normal distribution is a continuous probab...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{Config.dataset_path}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278681f3",
   "metadata": {
    "papermill": {
     "duration": 0.008698,
     "end_time": "2024-03-31T21:42:39.775185",
     "exception": false,
     "start_time": "2024-03-31T21:42:39.766487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fine-tune Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23ba17d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T21:42:39.793946Z",
     "iopub.status.busy": "2024-03-31T21:42:39.793694Z",
     "iopub.status.idle": "2024-03-31T21:42:39.822462Z",
     "shell.execute_reply": "2024-03-31T21:42:39.821636Z"
    },
    "papermill": {
     "duration": 0.042724,
     "end_time": "2024-03-31T21:42:39.826753",
     "exception": false,
     "start_time": "2024-03-31T21:42:39.784029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3044d32b5242aaa872d01eee3cdf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "template = \"\\n\\nQuestion:\\n{question}\\n\\nAnswer:\\n{answer}\"\n",
    "df[\"prompt\"] = df.progress_apply(lambda row: template.format(question=row.question,\n",
    "                                                             answer=row.answer), axis=1)\n",
    "data = df.prompt.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd55513",
   "metadata": {
    "papermill": {
     "duration": 0.009227,
     "end_time": "2024-03-31T21:42:39.845424",
     "exception": false,
     "start_time": "2024-03-31T21:42:39.836197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initialize the code for Gemma Causal LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ce8b787",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T21:42:39.864415Z",
     "iopub.status.busy": "2024-03-31T21:42:39.864148Z",
     "iopub.status.idle": "2024-03-31T21:43:36.192090Z",
     "shell.execute_reply": "2024-03-31T21:43:36.191210Z"
    },
    "papermill": {
     "duration": 56.339604,
     "end_time": "2024-03-31T21:43:36.193991",
     "exception": false,
     "start_time": "2024-03-31T21:42:39.854387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b607e01",
   "metadata": {
    "papermill": {
     "duration": 0.010703,
     "end_time": "2024-03-31T21:43:36.215485",
     "exception": false,
     "start_time": "2024-03-31T21:43:36.204782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Gemma preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1e647ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T21:43:36.238284Z",
     "iopub.status.busy": "2024-03-31T21:43:36.237518Z",
     "iopub.status.idle": "2024-03-31T21:43:36.559669Z",
     "shell.execute_reply": "2024-03-31T21:43:36.558879Z"
    },
    "papermill": {
     "duration": 0.336044,
     "end_time": "2024-03-31T21:43:36.562090",
     "exception": false,
     "start_time": "2024-03-31T21:43:36.226046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y, sample_weight = gemma_lm.preprocessor(data[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ba8088",
   "metadata": {
    "papermill": {
     "duration": 0.010526,
     "end_time": "2024-03-31T21:43:36.583576",
     "exception": false,
     "start_time": "2024-03-31T21:43:36.573050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Enable LoRA for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b82e2410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T21:43:36.606503Z",
     "iopub.status.busy": "2024-03-31T21:43:36.606196Z",
     "iopub.status.idle": "2024-03-31T21:43:37.056762Z",
     "shell.execute_reply": "2024-03-31T21:43:37.055805Z"
    },
    "papermill": {
     "duration": 0.464564,
     "end_time": "2024-03-31T21:43:37.058784",
     "exception": false,
     "start_time": "2024-03-31T21:43:36.594220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to 4.\n",
    "gemma_lm.backbone.enable_lora(rank=4)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3181b5",
   "metadata": {
    "papermill": {
     "duration": 0.011956,
     "end_time": "2024-03-31T21:43:37.082949",
     "exception": false,
     "start_time": "2024-03-31T21:43:37.070993",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run the fine-tuning sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc1e98cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T21:43:37.107701Z",
     "iopub.status.busy": "2024-03-31T21:43:37.107416Z",
     "iopub.status.idle": "2024-03-31T22:15:03.129975Z",
     "shell.execute_reply": "2024-03-31T22:15:03.129068Z"
    },
    "papermill": {
     "duration": 1886.036943,
     "end_time": "2024-03-31T22:15:03.131840",
     "exception": false,
     "start_time": "2024-03-31T21:43:37.094897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 736ms/step - loss: 0.5156 - sparse_categorical_accuracy: 0.5393\n",
      "Epoch 2/15\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 730ms/step - loss: 0.4363 - sparse_categorical_accuracy: 0.5805\n",
      "Epoch 3/15\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 730ms/step - loss: 0.4144 - sparse_categorical_accuracy: 0.5900\n",
      "Epoch 4/15\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 730ms/step - loss: 0.4016 - sparse_categorical_accuracy: 0.6002\n",
      "Epoch 5/15\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 730ms/step - loss: 0.3925 - sparse_categorical_accuracy: 0.6070\n",
      "Epoch 6/15\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 860ms/step - loss: 0.3820 - sparse_categorical_accuracy: 0.6161\n",
      "Epoch 7/15\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 730ms/step - loss: 0.3689 - sparse_categorical_accuracy: 0.6251\n",
      "Epoch 8/15\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 730ms/step - loss: 0.3534 - sparse_categorical_accuracy: 0.6368\n",
      "Epoch 9/15\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 860ms/step - loss: 0.3356 - sparse_categorical_accuracy: 0.6475\n",
      "Epoch 10/15\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 730ms/step - loss: 0.3142 - sparse_categorical_accuracy: 0.6674\n",
      "Epoch 11/15\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 730ms/step - loss: 0.2912 - sparse_categorical_accuracy: 0.6878\n",
      "Epoch 12/15\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 730ms/step - loss: 0.2652 - sparse_categorical_accuracy: 0.7153\n",
      "Epoch 13/15\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 730ms/step - loss: 0.2377 - sparse_categorical_accuracy: 0.7455\n",
      "Epoch 14/15\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 730ms/step - loss: 0.2149 - sparse_categorical_accuracy: 0.7683\n",
      "Epoch 15/15\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 730ms/step - loss: 0.1991 - sparse_categorical_accuracy: 0.7841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7da4f06ebdc0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit the input sequence length to 512 (to control memory usage).\n",
    "gemma_lm.preprocessor.sequence_length = Config.sequence_length \n",
    "\n",
    "# Compile the model with loss, optimizer, and metric\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=8e-5),\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "gemma_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a30f427",
   "metadata": {
    "papermill": {
     "duration": 0.265032,
     "end_time": "2024-03-31T22:15:03.614918",
     "exception": false,
     "start_time": "2024-03-31T22:15:03.349886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c6275",
   "metadata": {
    "papermill": {
     "duration": 0.215123,
     "end_time": "2024-03-31T22:15:04.054601",
     "exception": false,
     "start_time": "2024-03-31T22:15:03.839478",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define the specialized class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72f2c3f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T22:15:04.486379Z",
     "iopub.status.busy": "2024-03-31T22:15:04.485518Z",
     "iopub.status.idle": "2024-03-31T22:15:04.491827Z",
     "shell.execute_reply": "2024-03-31T22:15:04.490927Z"
    },
    "papermill": {
     "duration": 0.224956,
     "end_time": "2024-03-31T22:15:04.493728",
     "exception": false,
     "start_time": "2024-03-31T22:15:04.268772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GemmaQA:\n",
    "    def __init__(self, max_length=512):\n",
    "        self.max_length = max_length\n",
    "        self.prompt = template\n",
    "        self.gemma_lm = gemma_lm\n",
    "        \n",
    "    def query(self, question):\n",
    "        response = self.gemma_lm.generate(\n",
    "            self.prompt.format(\n",
    "                question=question,\n",
    "                answer=\"\"), \n",
    "            max_length=self.max_length)\n",
    "        display(Markdown(colorize_text(response)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f1a2b7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T22:15:04.933745Z",
     "iopub.status.busy": "2024-03-31T22:15:04.933359Z",
     "iopub.status.idle": "2024-03-31T22:15:04.937593Z",
     "shell.execute_reply": "2024-03-31T22:15:04.936761Z"
    },
    "papermill": {
     "duration": 0.225794,
     "end_time": "2024-03-31T22:15:04.939525",
     "exception": false,
     "start_time": "2024-03-31T22:15:04.713731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_qa = GemmaQA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3578ad40",
   "metadata": {
    "papermill": {
     "duration": 0.217444,
     "end_time": "2024-03-31T22:15:05.373651",
     "exception": false,
     "start_time": "2024-03-31T22:15:05.156207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f74a7a80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T22:15:05.809190Z",
     "iopub.status.busy": "2024-03-31T22:15:05.808818Z",
     "iopub.status.idle": "2024-03-31T22:15:22.774838Z",
     "shell.execute_reply": "2024-03-31T22:15:22.773751Z"
    },
    "papermill": {
     "duration": 17.1872,
     "end_time": "2024-03-31T22:15:22.777457",
     "exception": false,
     "start_time": "2024-03-31T22:15:05.590257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Question:</font>**\n",
       "How do we check if a variable follows the normal distribution? ‍⭐️\n",
       "\n",
       "**<font color='red'>Answer:</font>**\n",
       "* Skewness is a measure of the distribution’s skew. Skewed distributions have a skewness of 1 or more. If the skewness is negative, the curve is to the left of the mean and the distribution is said to be “left skewed.” If the skewness is positive, the curve is to the right of the mean and the distribution is said to be “right skewed.”\n",
       "* Kurtosis is a measure of the degree of heaviness (or thinness) of the peak of the distribution. Kurtosis is a measure of how heavy-tailed the distribution is. Distributions with heavy peaks have higher values in kurtosis, and therefore, the peak is said to be “peaked”. Distributions that have a thin peak have lower values in kurtosis, and therefore, the peak is said to be “flat.”"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[5]\n",
    "gemma_qa.query(row.question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634ad5c",
   "metadata": {
    "papermill": {
     "duration": 0.221824,
     "end_time": "2024-03-31T22:15:23.239187",
     "exception": false,
     "start_time": "2024-03-31T22:15:23.017363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d757110",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T22:15:23.728543Z",
     "iopub.status.busy": "2024-03-31T22:15:23.727708Z",
     "iopub.status.idle": "2024-03-31T22:15:24.106858Z",
     "shell.execute_reply": "2024-03-31T22:15:24.105846Z"
    },
    "papermill": {
     "duration": 0.648607,
     "end_time": "2024-03-31T22:15:24.108919",
     "exception": false,
     "start_time": "2024-03-31T22:15:23.460312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Question:</font>**\n",
       "What is SGD  —  stochastic gradient descent? What’s the difference with the usual gradient descent? ‍⭐️\n",
       "\n",
       "**<font color='red'>Answer:</font>**\n",
       "Answer here"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[10]\n",
    "gemma_qa.query(row.question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759bedb3",
   "metadata": {
    "papermill": {
     "duration": 0.218404,
     "end_time": "2024-03-31T22:15:24.546427",
     "exception": false,
     "start_time": "2024-03-31T22:15:24.328023",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8b1aa73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T22:15:24.986594Z",
     "iopub.status.busy": "2024-03-31T22:15:24.985904Z",
     "iopub.status.idle": "2024-03-31T22:15:30.369702Z",
     "shell.execute_reply": "2024-03-31T22:15:30.368738Z"
    },
    "papermill": {
     "duration": 5.605171,
     "end_time": "2024-03-31T22:15:30.371811",
     "exception": false,
     "start_time": "2024-03-31T22:15:24.766640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Question:</font>**\n",
       "How to validate your models? 👶\n",
       "\n",
       "**<font color='red'>Answer:</font>**\n",
       "Validation refers to a process where we test the generalizability of our model. We want to check whether it can make predictions for new, unseen data.\n",
       "For a machine learning model to perform well, we need to make sure that the model doesn't overfit the data.\n",
       "We can use the following evaluation metrics while validating the model:\n",
       "1. Cross-validation: It is used to test the generalizability of the model.\n",
       "2. Test-set validation: It is used when we have a huge amount of data and don't want to test the model's generalizability.\n",
       "3. Grid search: It is used in case of overfitting when you have a large number of hyper-parameters to tune.\n",
       "4. Out-of-sample error : It is used when we have a limited amount of data and don't want to test the model's generalisatio."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[15]\n",
    "gemma_qa.query(row.question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea033409",
   "metadata": {
    "papermill": {
     "duration": 0.220867,
     "end_time": "2024-03-31T22:15:30.816865",
     "exception": false,
     "start_time": "2024-03-31T22:15:30.595998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fresh question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48fdee08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T22:15:31.254913Z",
     "iopub.status.busy": "2024-03-31T22:15:31.254565Z",
     "iopub.status.idle": "2024-03-31T22:15:33.393422Z",
     "shell.execute_reply": "2024-03-31T22:15:33.392448Z"
    },
    "papermill": {
     "duration": 2.360006,
     "end_time": "2024-03-31T22:15:33.395509",
     "exception": false,
     "start_time": "2024-03-31T22:15:31.035503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Question:</font>**\n",
       "What is regularization?\n",
       "\n",
       "**<font color='red'>Answer:</font>**\n",
       "Regularization is a technique that is used to prevent the overfitting of the model. It is used to prevent the model to learn too much from the training data and make the model general and robust. There are various regularization techniques like Lasso, Ridge, and LightGBM regularization but the most commonly known regularization technique is Shrinkage."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is regularization?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bce7aa",
   "metadata": {
    "papermill": {
     "duration": 0.216704,
     "end_time": "2024-03-31T22:15:33.834073",
     "exception": false,
     "start_time": "2024-03-31T22:15:33.617369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fresh question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5396dc9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T22:15:34.272078Z",
     "iopub.status.busy": "2024-03-31T22:15:34.271655Z",
     "iopub.status.idle": "2024-03-31T22:15:35.749390Z",
     "shell.execute_reply": "2024-03-31T22:15:35.748535Z"
    },
    "papermill": {
     "duration": 1.698405,
     "end_time": "2024-03-31T22:15:35.751437",
     "exception": false,
     "start_time": "2024-03-31T22:15:34.053032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Question:</font>**\n",
       "What is SVM?\n",
       "\n",
       "**<font color='red'>Answer:</font>**\n",
       "SVM stands for support vector machines. It is a classification method that tries to find a separating hyperplane in a high dimensional space to accurately classify the data. It is commonly used in many real-world applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is SVM?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a297880",
   "metadata": {
    "papermill": {
     "duration": 0.218529,
     "end_time": "2024-03-31T22:15:36.188047",
     "exception": false,
     "start_time": "2024-03-31T22:15:35.969518",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fresh question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48553d84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T22:15:36.681004Z",
     "iopub.status.busy": "2024-03-31T22:15:36.680667Z",
     "iopub.status.idle": "2024-03-31T22:15:38.242662Z",
     "shell.execute_reply": "2024-03-31T22:15:38.241584Z"
    },
    "papermill": {
     "duration": 1.836973,
     "end_time": "2024-03-31T22:15:38.244961",
     "exception": false,
     "start_time": "2024-03-31T22:15:36.407988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Question:</font>**\n",
       "What is Dropout?\n",
       "\n",
       "**<font color='red'>Answer:</font>**\n",
       "Dropout is a technique that prevents a neural network from overfitting by temporarily removing some of the neurons during the training process. It helps the network to explore more of the parameter space, thus making it less prone to overfitting."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is Dropout?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa0ef3",
   "metadata": {
    "papermill": {
     "duration": 0.229533,
     "end_time": "2024-03-31T22:15:38.716291",
     "exception": false,
     "start_time": "2024-03-31T22:15:38.486758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "\n",
    "We fine-tuned Gemma with a set of Data Science interview question and answers.   \n",
    "Then we tested the model with questions from the dataset used for fine-tuning.  \n",
    "At the end, we also tested with some questions that were not in the dataset."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7669720,
     "sourceId": 64148,
     "sourceType": "competition"
    },
    {
     "datasetId": 4498747,
     "sourceId": 7705679,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 5171,
     "sourceId": 11371,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2030.29222,
   "end_time": "2024-03-31T22:15:42.321116",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-31T21:41:52.028896",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "066dc9999e1a414fa1601318a124c0f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ce29b65569f643068b16be5c2fb05e3a",
       "placeholder": "​",
       "style": "IPY_MODEL_90fbfcfb6e714643b4569ba7b4d0e33b",
       "value": " 166/166 [00:00&lt;00:00, 8710.04it/s]"
      }
     },
     "2314e98fcdf148a9a17bbd2811cb996a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28e5404f74d34daa929158b62fc5a4f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2314e98fcdf148a9a17bbd2811cb996a",
       "placeholder": "​",
       "style": "IPY_MODEL_2de485b745874d3daa3b9c558564d952",
       "value": "100%"
      }
     },
     "2de485b745874d3daa3b9c558564d952": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6a639d046d5044459ad4e8c05270a0ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7b3044d32b5242aaa872d01eee3cdf03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_28e5404f74d34daa929158b62fc5a4f9",
        "IPY_MODEL_996162c95be342cf923b48566d4883fa",
        "IPY_MODEL_066dc9999e1a414fa1601318a124c0f0"
       ],
       "layout": "IPY_MODEL_c9c6752dbd4248eabd9368c9f7fddeca"
      }
     },
     "90fbfcfb6e714643b4569ba7b4d0e33b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "996162c95be342cf923b48566d4883fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ebd771d6903845a7a29c5e22adc1da09",
       "max": 166.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6a639d046d5044459ad4e8c05270a0ce",
       "value": 166.0
      }
     },
     "c9c6752dbd4248eabd9368c9f7fddeca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ce29b65569f643068b16be5c2fb05e3a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ebd771d6903845a7a29c5e22adc1da09": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
