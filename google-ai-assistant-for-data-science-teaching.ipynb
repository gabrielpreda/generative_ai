{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64bdad72",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.010523,
     "end_time": "2024-04-06T21:05:06.579408",
     "exception": false,
     "start_time": "2024-04-06T21:05:06.568885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center><h1>Google AI Assistant for Data Science teaching</h1></center>\n",
    "\n",
    "<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This Notebook will build an AI Assistant for Data Science teaching.\n",
    "\n",
    "## The method\n",
    "\n",
    "We will fine-tune a LLM with Data Science questions and answers. Then we will create a custom class that will query this model.\n",
    "\n",
    "## The model\n",
    "\n",
    "As model, we will use Gemma model, fine-tuned with our Data Science Q&A data, using LoRA.\n",
    "\n",
    "## The data\n",
    "\n",
    "As data for fine-tuning Gemma, we will use [Data Science Q&A Treasury](https://www.kaggle.com/datasets/memocan/data-science-interview-q-and-a-treasury) dataset. This dataset contains over 150 questions and answering about Data Science.\n",
    "\n",
    "## Previous work\n",
    "\n",
    "This work is largely based on previous work. Here I list the sources:\n",
    "\n",
    "1. Gemma Model Card, Kaggle Models, https://www.kaggle.com/models/google/gemma\n",
    "2. Kaggle QA with Gemma - KerasNLP Starter, Kaggle Code, https://www.kaggle.com/code/awsaf49/kaggle-qa-with-gemma-kerasnlp-starter (Version 11)\n",
    "3. Fine-tune Gemma models in Keras using LoRA, Kaggle Code, https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora (Version 1)\n",
    "4. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, LoRA: Low-Rank Adaptation of Large Language Models, ArXiv, https://arxiv.org/pdf/2106.09685.pdf\n",
    "5. Abheesht Sharma, Matthew Watson, Parameter-efficient fine-tuning of GPT-2 with LoRA, https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/\n",
    "6. Keras 3 API documentation / KerasNLP / Models / Gemma, https://keras.io/api/keras_nlp/models/gemma/   \n",
    "7. Understanding LoRA with a minimal example, https://blogs.rstudio.com/ai/posts/2023-06-22-understanding-lora/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab376ba",
   "metadata": {
    "papermill": {
     "duration": 0.009306,
     "end_time": "2024-04-06T21:05:06.598393",
     "exception": false,
     "start_time": "2024-04-06T21:05:06.589087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction about Gemma\n",
    "\n",
    "\n",
    "Gemma is a collection of lightweight source generative AI models designed to be used mostly by developers and researchers. Created by Google DeepMind research lab that also developed Gemini, Gemma is available in several versions, with 2B and 7B parameters, as following:\n",
    "\n",
    "| Model                  | Parameters      | Tuned versions    | Description                                    | Recomemnded target platforms       |\n",
    "|------------------------|-----------------|-------------------|------------------------------------------------|------------------------------------|\n",
    "| `gemma_2b_en`          | 2.51B           | Pretrained        | 18-layer Gemma model (Gemma with 2B parameters)|Mobile devices and laptops          |\n",
    "| `gemma_instruct_2b_en` | 2.51B           | Instruction tuned | 18-layer Gemma model (Gemma with 2B parameters)| Mobile devices and laptops         | \n",
    "| `gemma_7b_en`          | 8.54B           | Pretrained        | 28-layer Gemma model (Gemma with 7B parameters)| Desktop computers and small servers|\n",
    "| `gemma_instruct_7b_en` | 8.54B           | Instruction tuned | 28-layer Gemma model (Gemma with 7B parameters)| Desktop computers and small servers|\n",
    "\n",
    "\n",
    "For this notebook, we will fine-tune `gemma_2b_en` model, one of the `2B` parameters Gemma models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ba36e",
   "metadata": {
    "papermill": {
     "duration": 0.009274,
     "end_time": "2024-04-06T21:05:06.617141",
     "exception": false,
     "start_time": "2024-04-06T21:05:06.607867",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LoRA introduction\n",
    "\n",
    "LoRA stands for Low-Rank Adaptation. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices.   \n",
    "The number of trainable parameters during fine-tunning will decrease therefore considerably.   \n",
    "According to LoRA paper, this number decreases 10,000 times, and the computational resources size decreases 3 times. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427de557",
   "metadata": {
    "papermill": {
     "duration": 0.009281,
     "end_time": "2024-04-06T21:05:06.636099",
     "exception": false,
     "start_time": "2024-04-06T21:05:06.626818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Installations and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f48263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:05:06.657230Z",
     "iopub.status.busy": "2024-04-06T21:05:06.656046Z",
     "iopub.status.idle": "2024-04-06T21:05:37.697530Z",
     "shell.execute_reply": "2024-04-06T21:05:37.696574Z"
    },
    "papermill": {
     "duration": 31.054289,
     "end_time": "2024-04-06T21:05:37.699886",
     "exception": false,
     "start_time": "2024-04-06T21:05:06.645597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras>=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c18f029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:05:37.721027Z",
     "iopub.status.busy": "2024-04-06T21:05:37.720740Z",
     "iopub.status.idle": "2024-04-06T21:05:51.155865Z",
     "shell.execute_reply": "2024-04-06T21:05:51.155034Z"
    },
    "papermill": {
     "duration": 13.448212,
     "end_time": "2024-04-06T21:05:51.158018",
     "exception": false,
     "start_time": "2024-04-06T21:05:37.709806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 21:05:41.984943: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-06 21:05:41.985075: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-06 21:05:42.111498: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "tqdm.pandas() # progress bar for pandas\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0654017",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:05:51.179359Z",
     "iopub.status.busy": "2024-04-06T21:05:51.178839Z",
     "iopub.status.idle": "2024-04-06T21:05:51.183482Z",
     "shell.execute_reply": "2024-04-06T21:05:51.182654Z"
    },
    "papermill": {
     "duration": 0.01717,
     "end_time": "2024-04-06T21:05:51.185281",
     "exception": false,
     "start_time": "2024-04-06T21:05:51.168111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    seed = 42\n",
    "    dataset_path = \"/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv\"\n",
    "    preset = \"gemma_2b_en\" # name of pretrained Gemma\n",
    "    sequence_length = 512 # max size of input sequence for training\n",
    "    batch_size = 1 # size of the input batch in training\n",
    "    epochs = 10 # number of epochs to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "104d5232",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:05:51.205946Z",
     "iopub.status.busy": "2024-04-06T21:05:51.205429Z",
     "iopub.status.idle": "2024-04-06T21:05:51.209399Z",
     "shell.execute_reply": "2024-04-06T21:05:51.208605Z"
    },
    "papermill": {
     "duration": 0.016281,
     "end_time": "2024-04-06T21:05:51.211206",
     "exception": false,
     "start_time": "2024-04-06T21:05:51.194925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(Config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58611eff",
   "metadata": {
    "papermill": {
     "duration": 0.009285,
     "end_time": "2024-04-06T21:05:51.230119",
     "exception": false,
     "start_time": "2024-04-06T21:05:51.220834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's define as well a small utility function for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f8ac3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:05:51.250315Z",
     "iopub.status.busy": "2024-04-06T21:05:51.249853Z",
     "iopub.status.idle": "2024-04-06T21:05:51.254551Z",
     "shell.execute_reply": "2024-04-06T21:05:51.253732Z"
    },
    "papermill": {
     "duration": 0.016838,
     "end_time": "2024-04-06T21:05:51.256500",
     "exception": false,
     "start_time": "2024-04-06T21:05:51.239662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Question\", \"Answer\"], [\"blue\", \"red\"]):\n",
    "        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cdc887",
   "metadata": {
    "papermill": {
     "duration": 0.009501,
     "end_time": "2024-04-06T21:05:51.275610",
     "exception": false,
     "start_time": "2024-04-06T21:05:51.266109",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the data  \n",
    "\n",
    "We load the data we will use for fine-tunning with LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b407d5ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:05:51.295950Z",
     "iopub.status.busy": "2024-04-06T21:05:51.295440Z",
     "iopub.status.idle": "2024-04-06T21:05:51.326614Z",
     "shell.execute_reply": "2024-04-06T21:05:51.325790Z"
    },
    "papermill": {
     "duration": 0.043263,
     "end_time": "2024-04-06T21:05:51.328440",
     "exception": false,
     "start_time": "2024-04-06T21:05:51.285177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is supervised machine learning? 👶</td>\n",
       "      <td>Supervised learning is a type of machine learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is regression? Which models can you use t...</td>\n",
       "      <td>Regression is a part of supervised ML. Regress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is linear regression? When do we use it? 👶</td>\n",
       "      <td>Linear regression is a model that assumes a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the main assumptions of linear regres...</td>\n",
       "      <td>There are several assumptions of linear regres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What’s the normal distribution? Why do we care...</td>\n",
       "      <td>The normal distribution is a continuous probab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0             What is supervised machine learning? 👶   \n",
       "1  What is regression? Which models can you use t...   \n",
       "2    What is linear regression? When do we use it? 👶   \n",
       "3  What are the main assumptions of linear regres...   \n",
       "4  What’s the normal distribution? Why do we care...   \n",
       "\n",
       "                                              answer  \n",
       "0  Supervised learning is a type of machine learn...  \n",
       "1  Regression is a part of supervised ML. Regress...  \n",
       "2  Linear regression is a model that assumes a li...  \n",
       "3  There are several assumptions of linear regres...  \n",
       "4  The normal distribution is a continuous probab...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{Config.dataset_path}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba1fae",
   "metadata": {
    "papermill": {
     "duration": 0.009726,
     "end_time": "2024-04-06T21:05:51.348420",
     "exception": false,
     "start_time": "2024-04-06T21:05:51.338694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fine-tune Gemma\n",
    "\n",
    "We prepare a template and generate a prompt using this template from each row in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b21d614",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:05:51.369238Z",
     "iopub.status.busy": "2024-04-06T21:05:51.368804Z",
     "iopub.status.idle": "2024-04-06T21:05:51.397020Z",
     "shell.execute_reply": "2024-04-06T21:05:51.396164Z"
    },
    "papermill": {
     "duration": 0.041654,
     "end_time": "2024-04-06T21:05:51.399898",
     "exception": false,
     "start_time": "2024-04-06T21:05:51.358244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6cba64765bc4e66a19a0719b7e37975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "template = \"\\n\\nQuestion:\\n{question}\\n\\nAnswer:\\n{answer}\"\n",
    "df[\"prompt\"] = df.progress_apply(lambda row: template.format(question=row.question,\n",
    "                                                             answer=row.answer), axis=1)\n",
    "data = df.prompt.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade0431",
   "metadata": {
    "papermill": {
     "duration": 0.009865,
     "end_time": "2024-04-06T21:05:51.419797",
     "exception": false,
     "start_time": "2024-04-06T21:05:51.409932",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initialize the code for Gemma Causal LM\n",
    "\n",
    "We initialize `GemmaCausalML` with `gemma_2b_en` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51425ae6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:05:51.441046Z",
     "iopub.status.busy": "2024-04-06T21:05:51.440808Z",
     "iopub.status.idle": "2024-04-06T21:06:48.415096Z",
     "shell.execute_reply": "2024-04-06T21:06:48.414255Z"
    },
    "papermill": {
     "duration": 56.986992,
     "end_time": "2024-04-06T21:06:48.417065",
     "exception": false,
     "start_time": "2024-04-06T21:05:51.430073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7285f67c",
   "metadata": {
    "papermill": {
     "duration": 0.011767,
     "end_time": "2024-04-06T21:06:48.441110",
     "exception": false,
     "start_time": "2024-04-06T21:06:48.429343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Gemma preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "915908cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:06:48.465828Z",
     "iopub.status.busy": "2024-04-06T21:06:48.465543Z",
     "iopub.status.idle": "2024-04-06T21:06:48.776606Z",
     "shell.execute_reply": "2024-04-06T21:06:48.775806Z"
    },
    "papermill": {
     "duration": 0.326059,
     "end_time": "2024-04-06T21:06:48.778842",
     "exception": false,
     "start_time": "2024-04-06T21:06:48.452783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y, sample_weight = gemma_lm.preprocessor(data[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f09284",
   "metadata": {
    "papermill": {
     "duration": 0.011528,
     "end_time": "2024-04-06T21:06:48.802320",
     "exception": false,
     "start_time": "2024-04-06T21:06:48.790792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Enable LoRA for the model\n",
    "\n",
    "We set LoRA rank to 5. The higher the LoRA rank, the higher the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3711231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:06:48.827020Z",
     "iopub.status.busy": "2024-04-06T21:06:48.826715Z",
     "iopub.status.idle": "2024-04-06T21:06:49.277114Z",
     "shell.execute_reply": "2024-04-06T21:06:49.276285Z"
    },
    "papermill": {
     "duration": 0.465032,
     "end_time": "2024-04-06T21:06:49.279116",
     "exception": false,
     "start_time": "2024-04-06T21:06:48.814084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,877,376</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,877,376\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,877,376</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,877,376\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,704,960</span> (6.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,704,960\u001b[0m (6.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to 5.\n",
    "gemma_lm.backbone.enable_lora(rank=5)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9e24b7",
   "metadata": {
    "papermill": {
     "duration": 0.012729,
     "end_time": "2024-04-06T21:06:49.305062",
     "exception": false,
     "start_time": "2024-04-06T21:06:49.292333",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The total trainable parameters is 1.7 M (or 6.5 MB).  \n",
    "This is less than 0.06% of the 2,5G (9.35 GB) total trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f761263",
   "metadata": {
    "papermill": {
     "duration": 0.01262,
     "end_time": "2024-04-06T21:06:49.330580",
     "exception": false,
     "start_time": "2024-04-06T21:06:49.317960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run the fine-tuning sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "273e5074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:06:49.357722Z",
     "iopub.status.busy": "2024-04-06T21:06:49.357451Z",
     "iopub.status.idle": "2024-04-06T21:27:24.023551Z",
     "shell.execute_reply": "2024-04-06T21:27:24.022677Z"
    },
    "papermill": {
     "duration": 1234.681973,
     "end_time": "2024-04-06T21:27:24.025533",
     "exception": false,
     "start_time": "2024-04-06T21:06:49.343560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 731ms/step - loss: 0.5137 - sparse_categorical_accuracy: 0.5402\n",
      "Epoch 2/10\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 729ms/step - loss: 0.4342 - sparse_categorical_accuracy: 0.5783\n",
      "Epoch 3/10\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 729ms/step - loss: 0.4123 - sparse_categorical_accuracy: 0.5927\n",
      "Epoch 4/10\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 729ms/step - loss: 0.3994 - sparse_categorical_accuracy: 0.6016\n",
      "Epoch 5/10\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 729ms/step - loss: 0.3898 - sparse_categorical_accuracy: 0.6081\n",
      "Epoch 6/10\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 729ms/step - loss: 0.3777 - sparse_categorical_accuracy: 0.6162\n",
      "Epoch 7/10\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 729ms/step - loss: 0.3622 - sparse_categorical_accuracy: 0.6280\n",
      "Epoch 8/10\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 729ms/step - loss: 0.3435 - sparse_categorical_accuracy: 0.6435\n",
      "Epoch 9/10\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 729ms/step - loss: 0.3208 - sparse_categorical_accuracy: 0.6625\n",
      "Epoch 10/10\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 729ms/step - loss: 0.2951 - sparse_categorical_accuracy: 0.6858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7de8cc633070>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit the input sequence length to 512 (to control memory usage).\n",
    "gemma_lm.preprocessor.sequence_length = Config.sequence_length \n",
    "\n",
    "# Compile the model with loss, optimizer, and metric\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=8e-5),\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "gemma_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ffea79",
   "metadata": {
    "papermill": {
     "duration": 0.148036,
     "end_time": "2024-04-06T21:27:24.324938",
     "exception": false,
     "start_time": "2024-04-06T21:27:24.176902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f70a7",
   "metadata": {
    "papermill": {
     "duration": 0.149887,
     "end_time": "2024-04-06T21:27:24.667873",
     "exception": false,
     "start_time": "2024-04-06T21:27:24.517986",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define the specialized class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b6d60e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:27:24.965418Z",
     "iopub.status.busy": "2024-04-06T21:27:24.964689Z",
     "iopub.status.idle": "2024-04-06T21:27:24.971331Z",
     "shell.execute_reply": "2024-04-06T21:27:24.970502Z"
    },
    "papermill": {
     "duration": 0.158066,
     "end_time": "2024-04-06T21:27:24.973271",
     "exception": false,
     "start_time": "2024-04-06T21:27:24.815205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GemmaQA:\n",
    "    def __init__(self, max_length=512):\n",
    "        self.max_length = max_length\n",
    "        self.prompt = template\n",
    "        self.gemma_lm = gemma_lm\n",
    "        \n",
    "    def query(self, question):\n",
    "        t_start = time()\n",
    "        response = self.gemma_lm.generate(\n",
    "            self.prompt.format(\n",
    "                question=question,\n",
    "                answer=\"\"), \n",
    "            max_length=self.max_length)\n",
    "        t_end = time()\n",
    "        print(f\"\\nTime: {round(t_end - t_start,2)}\")\n",
    "        display(Markdown(colorize_text(response)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee355742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:27:25.273277Z",
     "iopub.status.busy": "2024-04-06T21:27:25.272922Z",
     "iopub.status.idle": "2024-04-06T21:27:25.277101Z",
     "shell.execute_reply": "2024-04-06T21:27:25.276283Z"
    },
    "papermill": {
     "duration": 0.156678,
     "end_time": "2024-04-06T21:27:25.278920",
     "exception": false,
     "start_time": "2024-04-06T21:27:25.122242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_qa = GemmaQA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebae90d",
   "metadata": {
    "papermill": {
     "duration": 0.148888,
     "end_time": "2024-04-06T21:27:25.574708",
     "exception": false,
     "start_time": "2024-04-06T21:27:25.425820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b5dadbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:27:25.873492Z",
     "iopub.status.busy": "2024-04-06T21:27:25.872690Z",
     "iopub.status.idle": "2024-04-06T21:27:38.141891Z",
     "shell.execute_reply": "2024-04-06T21:27:38.140899Z"
    },
    "papermill": {
     "duration": 12.421361,
     "end_time": "2024-04-06T21:27:38.144013",
     "exception": false,
     "start_time": "2024-04-06T21:27:25.722652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time: 12.26\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Question:</font>**\n",
       "How do we check if a variable follows the normal distribution? ‍⭐️\n",
       "\n",
       "**<font color='red'>Answer:</font>**\n",
       "Answer here"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[5]\n",
    "gemma_qa.query(row.question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33977e23",
   "metadata": {
    "papermill": {
     "duration": 0.15276,
     "end_time": "2024-04-06T21:27:38.453900",
     "exception": false,
     "start_time": "2024-04-06T21:27:38.301140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8e933c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:27:38.756402Z",
     "iopub.status.busy": "2024-04-06T21:27:38.755688Z",
     "iopub.status.idle": "2024-04-06T21:27:41.714476Z",
     "shell.execute_reply": "2024-04-06T21:27:41.713569Z"
    },
    "papermill": {
     "duration": 3.112254,
     "end_time": "2024-04-06T21:27:41.716581",
     "exception": false,
     "start_time": "2024-04-06T21:27:38.604327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time: 2.95\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Question:</font>**\n",
       "What is SGD  —  stochastic gradient descent? What’s the difference with the usual gradient descent? ‍⭐️\n",
       "\n",
       "**<font color='red'>Answer:</font>**\n",
       "Stochastic gradient descent is a variant of gradient descent where the gradient is calculated using only a random sample from the training set. This means that the update step is only based on a random sample from the training set. The usual gradient descent uses the whole training set to calculate the gradient. This means that the update step is based on all the training examples.\n",
       "\n",
       "The difference between SGD and gradient descent is that SGD is faster to compute, since it only requires one calculation of the gradient per iteration."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[10]\n",
    "gemma_qa.query(row.question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091c4b54",
   "metadata": {
    "papermill": {
     "duration": 0.167223,
     "end_time": "2024-04-06T21:27:42.072285",
     "exception": false,
     "start_time": "2024-04-06T21:27:41.905062",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1ae509b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:27:42.385718Z",
     "iopub.status.busy": "2024-04-06T21:27:42.384816Z",
     "iopub.status.idle": "2024-04-06T21:27:42.744385Z",
     "shell.execute_reply": "2024-04-06T21:27:42.743403Z"
    },
    "papermill": {
     "duration": 0.515539,
     "end_time": "2024-04-06T21:27:42.746682",
     "exception": false,
     "start_time": "2024-04-06T21:27:42.231143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time: 0.35\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Question:</font>**\n",
       "How to validate your models? 👶\n",
       "\n",
       "**<font color='red'>Answer:</font>**\n",
       "Answer here"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[15]\n",
    "gemma_qa.query(row.question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244e3bf5",
   "metadata": {
    "papermill": {
     "duration": 0.150556,
     "end_time": "2024-04-06T21:27:43.059405",
     "exception": false,
     "start_time": "2024-04-06T21:27:42.908849",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fresh question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a29ed46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:27:43.359753Z",
     "iopub.status.busy": "2024-04-06T21:27:43.359041Z",
     "iopub.status.idle": "2024-04-06T21:27:44.368297Z",
     "shell.execute_reply": "2024-04-06T21:27:44.367357Z"
    },
    "papermill": {
     "duration": 1.161113,
     "end_time": "2024-04-06T21:27:44.370194",
     "exception": false,
     "start_time": "2024-04-06T21:27:43.209081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time: 1.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Question:</font>**\n",
       "What is regularization?\n",
       "\n",
       "**<font color='red'>Answer:</font>**\n",
       "Regularization is a technique for preventing your neural networks from overfitting. It does this by adding some penalty to your cost function."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is regularization?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda13b6",
   "metadata": {
    "papermill": {
     "duration": 0.150774,
     "end_time": "2024-04-06T21:27:44.670635",
     "exception": false,
     "start_time": "2024-04-06T21:27:44.519861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fresh question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf9de94b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:27:44.971774Z",
     "iopub.status.busy": "2024-04-06T21:27:44.971437Z",
     "iopub.status.idle": "2024-04-06T21:27:46.360140Z",
     "shell.execute_reply": "2024-04-06T21:27:46.359100Z"
    },
    "papermill": {
     "duration": 1.54152,
     "end_time": "2024-04-06T21:27:46.362490",
     "exception": false,
     "start_time": "2024-04-06T21:27:44.820970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time: 1.38\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Question:</font>**\n",
       "What is SVM?\n",
       "\n",
       "**<font color='red'>Answer:</font>**\n",
       "Support Vector Machine (SVM) is a supervised machine learning algorithm that finds a optimal boundary between the data points, called support vectors, such that the data points closest to the boundary are called support vectors."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is SVM?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d84aba7",
   "metadata": {
    "papermill": {
     "duration": 0.149823,
     "end_time": "2024-04-06T21:27:46.680924",
     "exception": false,
     "start_time": "2024-04-06T21:27:46.531101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fresh question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d675d7ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:27:46.984001Z",
     "iopub.status.busy": "2024-04-06T21:27:46.983638Z",
     "iopub.status.idle": "2024-04-06T21:27:47.938343Z",
     "shell.execute_reply": "2024-04-06T21:27:47.937392Z"
    },
    "papermill": {
     "duration": 1.109343,
     "end_time": "2024-04-06T21:27:47.940654",
     "exception": false,
     "start_time": "2024-04-06T21:27:46.831311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time: 0.95\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Question:</font>**\n",
       "What is Dropout?\n",
       "\n",
       "**<font color='red'>Answer:</font>**\n",
       "Dropout is a technique that randomly turns off connections between layers in a neural network, which prevents the network from overfitting."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is Dropout?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a4fd3",
   "metadata": {
    "papermill": {
     "duration": 0.149388,
     "end_time": "2024-04-06T21:27:48.245508",
     "exception": false,
     "start_time": "2024-04-06T21:27:48.096120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fresh question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "972b814f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:27:48.592625Z",
     "iopub.status.busy": "2024-04-06T21:27:48.591920Z",
     "iopub.status.idle": "2024-04-06T21:27:50.196847Z",
     "shell.execute_reply": "2024-04-06T21:27:50.195834Z"
    },
    "papermill": {
     "duration": 1.759794,
     "end_time": "2024-04-06T21:27:50.199103",
     "exception": false,
     "start_time": "2024-04-06T21:27:48.439309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time: 1.6\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Question:</font>**\n",
       "Please describe the principle of decision trees.\n",
       "\n",
       "**<font color='red'>Answer:</font>**\n",
       "A decision tree is a tree-structured prediction model in which the nodes represent the possible inputs (or features), the branches represent the application of a test on the input, and the leaves (or outcomes) represent the outcomes of the prediction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Please describe the principle of decision trees.\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a3be67",
   "metadata": {
    "papermill": {
     "duration": 0.150175,
     "end_time": "2024-04-06T21:27:50.502642",
     "exception": false,
     "start_time": "2024-04-06T21:27:50.352467",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fresh question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "731a184f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:27:50.806658Z",
     "iopub.status.busy": "2024-04-06T21:27:50.806044Z",
     "iopub.status.idle": "2024-04-06T21:27:52.844002Z",
     "shell.execute_reply": "2024-04-06T21:27:52.843054Z"
    },
    "papermill": {
     "duration": 2.193082,
     "end_time": "2024-04-06T21:27:52.846090",
     "exception": false,
     "start_time": "2024-04-06T21:27:50.653008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time: 2.03\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Question:</font>**\n",
       "Please describe logistic regression principle.\n",
       "\n",
       "**<font color='red'>Answer:</font>**\n",
       "Logistic regression is a machine learning algorithm that is used for classification problems. It is also known as the logistic function or sigmoid function. The main idea behind logistic regression is to find a set of weights that minimizes the probability of a prediction being wrong. It is used when we have a binary dependent variable and many independent variables."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Please describe logistic regression principle.\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f5169d",
   "metadata": {
    "papermill": {
     "duration": 0.148409,
     "end_time": "2024-04-06T21:27:53.148920",
     "exception": false,
     "start_time": "2024-04-06T21:27:53.000511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "\n",
    "We fine-tuned Gemma with a set of Data Science interview question and answers.   \n",
    "Then we tested the model with questions from the dataset used for fine-tuning.  \n",
    "At the end, we also tested with some free formulated questions, not from the dataset."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7669720,
     "sourceId": 64148,
     "sourceType": "competition"
    },
    {
     "datasetId": 4498747,
     "sourceId": 7705679,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 5171,
     "sourceId": 11371,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1372.923578,
   "end_time": "2024-04-06T21:27:56.736225",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-06T21:05:03.812647",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0814e37be56a453c9dfbe3de87ac43f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0b4661a382f04271a00229f134b7c1b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5797d7b1979540f7b08223fad9319398": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7c0d9e99a90e4e5ca45335d055a00206": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "930b2ca687144702bb6667a0861712f8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "938e044295dd42a7a13993ba40fc8539": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0814e37be56a453c9dfbe3de87ac43f9",
       "placeholder": "​",
       "style": "IPY_MODEL_0b4661a382f04271a00229f134b7c1b1",
       "value": " 166/166 [00:00&lt;00:00, 9409.23it/s]"
      }
     },
     "95658729efad40b4afc9a403399df306": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a289c984dc564deea46ac36c61621a1e",
       "placeholder": "​",
       "style": "IPY_MODEL_5797d7b1979540f7b08223fad9319398",
       "value": "100%"
      }
     },
     "a289c984dc564deea46ac36c61621a1e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c2864cddc24d46ad94dea733a2df8ac9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_930b2ca687144702bb6667a0861712f8",
       "max": 166.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d4497bf65bba4f629fd2f43ed4840ac4",
       "value": 166.0
      }
     },
     "d4497bf65bba4f629fd2f43ed4840ac4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f6cba64765bc4e66a19a0719b7e37975": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_95658729efad40b4afc9a403399df306",
        "IPY_MODEL_c2864cddc24d46ad94dea733a2df8ac9",
        "IPY_MODEL_938e044295dd42a7a13993ba40fc8539"
       ],
       "layout": "IPY_MODEL_7c0d9e99a90e4e5ca45335d055a00206"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
