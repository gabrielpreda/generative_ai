{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gpreda/rag-using-llama-2-langchain-and-chromadb?scriptVersionId=144059414\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Introduction\n\n<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F769452%2Fb18d0513200d426e556b2b7b7c825981%2FRAG.png?generation=1695504022336680&alt=media\"></img>\n\n## Objective\n\nUse Llama 2.0, Langchain and ChromaDB to create a Retrieval Augmented Generation (RAG) system. This will allow us to ask questions about our documents (that were not included in the training data), without fine-tunning the Large Language Model (LLM).\nWhen using RAG, if you are given a question, you first do a retrieval step to fetch any relevant documents from a special database, a vector database where these documents were indexed. \n\n## Definitions\n\n* LLM - Large Language Model  \n* Llama 2.0 - LLM from Meta \n* Langchain - a framework designed to simplify the creation of applications using LLMs\n* Vector database - a database that organizes data through high-dimmensional vectors  \n* ChromaDB - vector database  \n* RAG - Retrieval Augmented Generation \n\n## Model details\n\n* **Model**: Llama 2  \n* **Variation**: 7b-chat-hf  (7b: 7B dimm. hf: HuggingFace build)\n* **Version**: V1  \n* **Framework**: PyTorch  \n\nLlaMA 2 model is pretrained and fine-tuned with 2 Trillion tokens and 7 to 70 Billion parameters which makes it one of the powerful open source models. It is a highly improvement over LlaMA 1 model.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Installations, imports, utils","metadata":{}},{"cell_type":"code","source":"!pip install transformers accelerate einops langchain xformers bitsandbytes chromadb sentence_transformers","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-24T11:27:16.336361Z","iopub.execute_input":"2023-09-24T11:27:16.337009Z","iopub.status.idle":"2023-09-24T11:30:52.668755Z","shell.execute_reply.started":"2023-09-24T11:27:16.336972Z","shell.execute_reply":"2023-09-24T11:30:52.667418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import cuda, bfloat16\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer\nfrom time import time\nimport chromadb\nfrom chromadb.config import Settings\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-24T11:30:52.671176Z","iopub.execute_input":"2023-09-24T11:30:52.671539Z","iopub.status.idle":"2023-09-24T11:31:02.57404Z","shell.execute_reply.started":"2023-09-24T11:30:52.671499Z","shell.execute_reply":"2023-09-24T11:31:02.572932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialize model, tokenizer, query pipeline","metadata":{}},{"cell_type":"markdown","source":"Define the model, the device, and the `bitsandbytes` configuration.","metadata":{}},{"cell_type":"code","source":"model_id = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n# set quantization configuration to load large model with less GPU memory\n# this requires the `bitsandbytes` library\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-24T11:31:02.575426Z","iopub.execute_input":"2023-09-24T11:31:02.576071Z","iopub.status.idle":"2023-09-24T11:31:02.709912Z","shell.execute_reply.started":"2023-09-24T11:31:02.57603Z","shell.execute_reply":"2023-09-24T11:31:02.708905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prepare the model and the tokenizer.","metadata":{}},{"cell_type":"code","source":"time_1 = time()\nmodel_config = transformers.AutoConfig.from_pretrained(\n    model_id,\n)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n    device_map='auto',\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntime_2 = time()\nprint(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-24T11:31:02.712874Z","iopub.execute_input":"2023-09-24T11:31:02.713231Z","iopub.status.idle":"2023-09-24T11:34:25.391533Z","shell.execute_reply.started":"2023-09-24T11:31:02.713195Z","shell.execute_reply":"2023-09-24T11:34:25.390606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the query pipeline.","metadata":{}},{"cell_type":"code","source":"time_1 = time()\nquery_pipeline = transformers.pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",)\ntime_2 = time()\nprint(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-24T11:34:25.393127Z","iopub.execute_input":"2023-09-24T11:34:25.393544Z","iopub.status.idle":"2023-09-24T11:34:27.946285Z","shell.execute_reply.started":"2023-09-24T11:34:25.393507Z","shell.execute_reply":"2023-09-24T11:34:27.945304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We define a function for testing the pipeline.","metadata":{}},{"cell_type":"code","source":"def test_model(tokenizer, pipeline, prompt_to_test):\n    \"\"\"\n    Perform a query\n    print the result\n    Args:\n        tokenizer: the tokenizer\n        pipeline: the pipeline\n        prompt_to_test: the prompt\n    Returns\n        None\n    \"\"\"\n    # adapted from https://huggingface.co/blog/llama2#using-transformers\n    time_1 = time()\n    sequences = pipeline(\n        prompt_to_test,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        max_length=200,)\n    time_2 = time()\n    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n    for seq in sequences:\n        print(f\"Result: {seq['generated_text']}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-24T11:34:27.950559Z","iopub.execute_input":"2023-09-24T11:34:27.950939Z","iopub.status.idle":"2023-09-24T11:34:27.960805Z","shell.execute_reply.started":"2023-09-24T11:34:27.9509Z","shell.execute_reply":"2023-09-24T11:34:27.958917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test the query pipeline\n\nWe test the pipeline with a query about the meaning of State of the Union (SOTU).","metadata":{}},{"cell_type":"code","source":"test_model(tokenizer,\n           query_pipeline,\n           \"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-24T11:34:27.964942Z","iopub.execute_input":"2023-09-24T11:34:27.965217Z","iopub.status.idle":"2023-09-24T11:34:40.274764Z","shell.execute_reply.started":"2023-09-24T11:34:27.965192Z","shell.execute_reply":"2023-09-24T11:34:40.273475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Retrieval Augmented Generation","metadata":{}},{"cell_type":"markdown","source":"## Check the model with a HuggingFace pipeline\n\n\nWe check the model with a HF pipeline, using a query about the meaning of State of the Union (SOTU).","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:22:16.433666Z","iopub.execute_input":"2023-09-23T19:22:16.434937Z","iopub.status.idle":"2023-09-23T19:22:16.440864Z","shell.execute_reply.started":"2023-09-23T19:22:16.434891Z","shell.execute_reply":"2023-09-23T19:22:16.439217Z"}}},{"cell_type":"code","source":"llm = HuggingFacePipeline(pipeline=query_pipeline)\n# checking again that everything is working fine\nllm(prompt=\"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-24T11:34:40.277537Z","iopub.execute_input":"2023-09-24T11:34:40.278384Z","iopub.status.idle":"2023-09-24T11:34:45.497736Z","shell.execute_reply.started":"2023-09-24T11:34:40.27833Z","shell.execute_reply":"2023-09-24T11:34:45.496602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ingestion of data using Text loder\n\nWe will ingest the newest presidential address, from Jan 2023.","metadata":{}},{"cell_type":"code","source":"loader = TextLoader(\"/kaggle/input/president-bidens-state-of-the-union-2023/biden-sotu-2023-planned-official.txt\",\n                    encoding=\"utf8\")\ndocuments = loader.load()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-24T11:34:45.499156Z","iopub.execute_input":"2023-09-24T11:34:45.499774Z","iopub.status.idle":"2023-09-24T11:34:45.510252Z","shell.execute_reply.started":"2023-09-24T11:34:45.499734Z","shell.execute_reply":"2023-09-24T11:34:45.508979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split data in chunks\n\nWe split data in chunks using a recursive character text splitter.","metadata":{}},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\nall_splits = text_splitter.split_documents(documents)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-24T11:34:45.511882Z","iopub.execute_input":"2023-09-24T11:34:45.512413Z","iopub.status.idle":"2023-09-24T11:34:45.543059Z","shell.execute_reply.started":"2023-09-24T11:34:45.512379Z","shell.execute_reply":"2023-09-24T11:34:45.542209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Embeddings and Storing in Vector Store","metadata":{}},{"cell_type":"markdown","source":"Create the embeddings using Sentence Transformer and HuggingFace embeddings.","metadata":{}},{"cell_type":"code","source":"model_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\nembeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-24T11:34:45.544149Z","iopub.execute_input":"2023-09-24T11:34:45.544475Z","iopub.status.idle":"2023-09-24T11:34:53.204351Z","shell.execute_reply.started":"2023-09-24T11:34:45.544443Z","shell.execute_reply":"2023-09-24T11:34:53.203268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initialize ChromaDB with the document splits, the embeddings defined previously and with the option to persist it locally.","metadata":{}},{"cell_type":"code","source":"vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-24T11:34:53.209241Z","iopub.execute_input":"2023-09-24T11:34:53.209668Z","iopub.status.idle":"2023-09-24T11:34:54.842076Z","shell.execute_reply.started":"2023-09-24T11:34:53.209635Z","shell.execute_reply":"2023-09-24T11:34:54.841118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initialize chain","metadata":{}},{"cell_type":"code","source":"retriever = vectordb.as_retriever()\n\nqa = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    retriever=retriever, \n    verbose=True\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-24T11:34:54.843557Z","iopub.execute_input":"2023-09-24T11:34:54.843922Z","iopub.status.idle":"2023-09-24T11:34:54.852247Z","shell.execute_reply.started":"2023-09-24T11:34:54.843887Z","shell.execute_reply":"2023-09-24T11:34:54.85108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test the Retrieval-Augmented Generation \n\n\nWe define a test function, that will run the query and time it.","metadata":{}},{"cell_type":"code","source":"def test_rag(qa, query):\n    print(f\"Query: {query}\\n\")\n    time_1 = time()\n    result = qa.run(query)\n    time_2 = time()\n    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n    print(\"\\nResult: \", result)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-24T11:34:54.85463Z","iopub.execute_input":"2023-09-24T11:34:54.854961Z","iopub.status.idle":"2023-09-24T11:34:54.865233Z","shell.execute_reply.started":"2023-09-24T11:34:54.854935Z","shell.execute_reply":"2023-09-24T11:34:54.86406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check few queries.","metadata":{}},{"cell_type":"code","source":"query = \"What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\"\ntest_rag(qa, query)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-24T11:34:54.866703Z","iopub.execute_input":"2023-09-24T11:34:54.867062Z","iopub.status.idle":"2023-09-24T11:35:10.254328Z","shell.execute_reply.started":"2023-09-24T11:34:54.867031Z","shell.execute_reply":"2023-09-24T11:35:10.253314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"What is the nation economic status? Summarize. Keep it under 200 words.\"\ntest_rag(qa, query)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-24T11:35:10.255955Z","iopub.execute_input":"2023-09-24T11:35:10.256581Z","iopub.status.idle":"2023-09-24T11:35:24.62616Z","shell.execute_reply.started":"2023-09-24T11:35:10.256545Z","shell.execute_reply":"2023-09-24T11:35:24.625206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Document sources\n\nLet's check the documents sources, for the last query run.","metadata":{}},{"cell_type":"code","source":"docs = vectordb.similarity_search(query)\nprint(f\"Query: {query}\")\nprint(f\"Retrieved documents: {len(docs)}\")\nfor doc in docs:\n    doc_details = doc.to_json()['kwargs']\n    print(\"Source: \", doc_details['metadata']['source'])\n    print(\"Text: \", doc_details['page_content'], \"\\n\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-24T11:42:07.601993Z","iopub.execute_input":"2023-09-24T11:42:07.602359Z","iopub.status.idle":"2023-09-24T11:42:07.659413Z","shell.execute_reply.started":"2023-09-24T11:42:07.602331Z","shell.execute_reply":"2023-09-24T11:42:07.658367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions\n\n\nWe used Langchain, ChromaDB and Llama 2 as a LLM to build a Retrieval Augmented Generation solution. For testing, we were using the latest State of the Union address from Jan 2023.\n","metadata":{}},{"cell_type":"markdown","source":"# References  \n\n[1] Murtuza Kazmi, Using LLaMA 2.0, FAISS and LangChain for Question-Answering on Your Own Data, https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476","metadata":{}}]}