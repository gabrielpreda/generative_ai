{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206c4eab",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.010965,
     "end_time": "2024-05-04T11:36:54.393296",
     "exception": false,
     "start_time": "2024-05-04T11:36:54.382331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center><h1>Fine-tunning Gemma model with Kaggle Docs data</h1></center>\n",
    "\n",
    "<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This notebook will demonstrate three things:\n",
    "\n",
    "1. How to fine-tune Gemma model using LoRA\n",
    "2. Creation of a specialised class to query about Kaggle features\n",
    "3. Some results of querying about Kaggle Docs\n",
    "\n",
    "This work is largely based on previous work. Here I list the sources:\n",
    "\n",
    "1. Gemma Model Card, Kaggle Models, https://www.kaggle.com/models/google/gemma\n",
    "2. Kaggle QA with Gemma - KerasNLP Starter, Kaggle Code, https://www.kaggle.com/code/awsaf49/kaggle-qa-with-gemma-kerasnlp-starter (Version 11)  \n",
    "3. Fine-tune Gemma models in Keras using LoRA, Kaggle Code, https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora (Version 1)  \n",
    "4. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, LoRA: Low-Rank Adaptation of Large Language Models, ArXiv, https://arxiv.org/pdf/2106.09685.pdf\n",
    "5. Abheesht Sharma, Matthew Watson, Parameter-efficient fine-tuning of GPT-2 with LoRA, https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/\n",
    "6. Keras 3 API documentation / KerasNLP / Models / Gemma, https://keras.io/api/keras_nlp/models/gemma/\n",
    "7. Kaggle Docs, Kaggle Dataset, https://www.kaggle.com/datasets/awsaf49/kaggle-docs  \n",
    "\n",
    "\n",
    "**Let's go**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd791f6f",
   "metadata": {
    "papermill": {
     "duration": 0.010048,
     "end_time": "2024-05-04T11:36:54.414349",
     "exception": false,
     "start_time": "2024-05-04T11:36:54.404301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is Gemma?\n",
    "\n",
    "\n",
    "Gemma is a collection of lightweight source generative AI models designed to be used mostly by developers and researchers. Created by Google DeepMind research lab that also developed Gemini, Gemma is available in several versions, with 2B and 7B parameters, as following:\n",
    "\n",
    "\n",
    "| Model                  | Parameters      | Tuned versions    | Description                                    | Recomemnded target platforms       |\n",
    "|------------------------|-----------------|-------------------|------------------------------------------------|------------------------------------|\n",
    "| `gemma_2b_en`          | 2.51B           | Pretrained        | 18-layer Gemma model (Gemma with 2B parameters)|Mobile devices and laptops          |\n",
    "| `gemma_instruct_2b_en` | 2.51B           | Instruction tuned | 18-layer Gemma model (Gemma with 2B parameters)| Mobile devices and laptops         | \n",
    "| `gemma_7b_en`          | 8.54B           | Pretrained        | 28-layer Gemma model (Gemma with 7B parameters)| Desktop computers and small servers|\n",
    "| `gemma_instruct_7b_en` | 8.54B           | Instruction tuned | 28-layer Gemma model (Gemma with 7B parameters)| Desktop computers and small servers|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821a6f5",
   "metadata": {
    "papermill": {
     "duration": 0.010185,
     "end_time": "2024-05-04T11:36:54.434770",
     "exception": false,
     "start_time": "2024-05-04T11:36:54.424585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is LoRA?  \n",
    "\n",
    "LoRA stands for Low-Rank Adaptation. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to LoRA paper, this number decreases 10,000 times, and the computational resources size decreases 3 times. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e6e38",
   "metadata": {
    "papermill": {
     "duration": 0.010089,
     "end_time": "2024-05-04T11:36:54.456069",
     "exception": false,
     "start_time": "2024-05-04T11:36:54.445980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How we proceed?\n",
    "\n",
    "For fine-tunning with LoRA, we will follow the steps:\n",
    "\n",
    "1. Install prerequisites\n",
    "2. Load and process the data for fine-tuning\n",
    "3. Initialize the code for Gemma causal language model (Gemma Causal LM)\n",
    "4. Perform fine-tuning\n",
    "5. Test the fine-tunned model with questions from the data used for fine-tuning and with aditional questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a8af62",
   "metadata": {
    "papermill": {
     "duration": 0.010005,
     "end_time": "2024-05-04T11:36:54.476240",
     "exception": false,
     "start_time": "2024-05-04T11:36:54.466235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prerequisites\n",
    "\n",
    "\n",
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98784fd6",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-05-04T11:36:54.498391Z",
     "iopub.status.busy": "2024-05-04T11:36:54.498019Z",
     "iopub.status.idle": "2024-05-04T11:37:25.889430Z",
     "shell.execute_reply": "2024-05-04T11:37:25.888477Z"
    },
    "papermill": {
     "duration": 31.405224,
     "end_time": "2024-05-04T11:37:25.891716",
     "exception": false,
     "start_time": "2024-05-04T11:36:54.486492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras>=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f795347",
   "metadata": {
    "papermill": {
     "duration": 0.010522,
     "end_time": "2024-05-04T11:37:25.913331",
     "exception": false,
     "start_time": "2024-05-04T11:37:25.902809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b7ae417",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-05-04T11:37:25.935854Z",
     "iopub.status.busy": "2024-05-04T11:37:25.935542Z",
     "iopub.status.idle": "2024-05-04T11:37:40.435799Z",
     "shell.execute_reply": "2024-05-04T11:37:40.434795Z"
    },
    "papermill": {
     "duration": 14.514442,
     "end_time": "2024-05-04T11:37:40.438155",
     "exception": false,
     "start_time": "2024-05-04T11:37:25.923713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 11:37:30.279488: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-04 11:37:30.279612: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-04 11:37:30.409367: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"\"\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas() # progress bar for pandas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74575b2d",
   "metadata": {
    "papermill": {
     "duration": 0.010597,
     "end_time": "2024-05-04T11:37:40.459772",
     "exception": false,
     "start_time": "2024-05-04T11:37:40.449175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "807f14ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:37:40.482605Z",
     "iopub.status.busy": "2024-05-04T11:37:40.482094Z",
     "iopub.status.idle": "2024-05-04T11:37:40.487104Z",
     "shell.execute_reply": "2024-05-04T11:37:40.486217Z"
    },
    "papermill": {
     "duration": 0.018599,
     "end_time": "2024-05-04T11:37:40.489115",
     "exception": false,
     "start_time": "2024-05-04T11:37:40.470516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    seed = 42\n",
    "    dataset_path = \"/kaggle/input/kaggle-docs/questions_answers\"\n",
    "    preset = \"gemma_2b_en\" # name of pretrained Gemma\n",
    "    sequence_length = 512 # max size of input sequence for training\n",
    "    batch_size = 1 # size of the input batch in training, x 2 as two GPUs\n",
    "    lora_rank = 4 # rank for LoRA, higher means more trainable parameters\n",
    "    learning_rate=8e-5 # learning rate used in train\n",
    "    epochs = 15 # number of epochs to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa4ee72",
   "metadata": {
    "papermill": {
     "duration": 0.010256,
     "end_time": "2024-05-04T11:37:40.509884",
     "exception": false,
     "start_time": "2024-05-04T11:37:40.499628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Set a random seed for results reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1877ccb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:37:40.532080Z",
     "iopub.status.busy": "2024-05-04T11:37:40.531796Z",
     "iopub.status.idle": "2024-05-04T11:37:40.536047Z",
     "shell.execute_reply": "2024-05-04T11:37:40.535228Z"
    },
    "papermill": {
     "duration": 0.017535,
     "end_time": "2024-05-04T11:37:40.538021",
     "exception": false,
     "start_time": "2024-05-04T11:37:40.520486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(Config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e7a43d",
   "metadata": {
    "papermill": {
     "duration": 0.010317,
     "end_time": "2024-05-04T11:37:40.558854",
     "exception": false,
     "start_time": "2024-05-04T11:37:40.548537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "904fc673",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:37:40.581021Z",
     "iopub.status.busy": "2024-05-04T11:37:40.580737Z",
     "iopub.status.idle": "2024-05-04T11:37:40.608253Z",
     "shell.execute_reply": "2024-05-04T11:37:40.607463Z"
    },
    "papermill": {
     "duration": 0.040674,
     "end_time": "2024-05-04T11:37:40.610192",
     "exception": false,
     "start_time": "2024-05-04T11:37:40.569518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the different types of competitions a...</td>\n",
       "      <td># Types of Competitions\\n\\nKaggle Competitions...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the different competition formats on ...</td>\n",
       "      <td>There are handful of different formats competi...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to join a competition?</td>\n",
       "      <td>Before you start, navigate to the [Competition...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to form, manage, and disband teams in a co...</td>\n",
       "      <td>Everyone that competes in a Competition does s...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I make a submission in a competition?</td>\n",
       "      <td>You will need to submit your model predictions...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  What are the different types of competitions a...   \n",
       "1  What are the different competition formats on ...   \n",
       "2                         How to join a competition?   \n",
       "3  How to form, manage, and disband teams in a co...   \n",
       "4       How do I make a submission in a competition?   \n",
       "\n",
       "                                              Answer     Category  \n",
       "0  # Types of Competitions\\n\\nKaggle Competitions...  competition  \n",
       "1  There are handful of different formats competi...  competition  \n",
       "2  Before you start, navigate to the [Competition...  competition  \n",
       "3  Everyone that competes in a Competition does s...  competition  \n",
       "4  You will need to submit your model predictions...  competition  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{Config.dataset_path}/data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12527abb",
   "metadata": {
    "papermill": {
     "duration": 0.010559,
     "end_time": "2024-05-04T11:37:40.631774",
     "exception": false,
     "start_time": "2024-05-04T11:37:40.621215",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's check the total number of rows in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dfb3659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:37:40.654829Z",
     "iopub.status.busy": "2024-05-04T11:37:40.654149Z",
     "iopub.status.idle": "2024-05-04T11:37:40.659580Z",
     "shell.execute_reply": "2024-05-04T11:37:40.658732Z"
    },
    "papermill": {
     "duration": 0.019041,
     "end_time": "2024-05-04T11:37:40.661602",
     "exception": false,
     "start_time": "2024-05-04T11:37:40.642561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd260ea6",
   "metadata": {
    "papermill": {
     "duration": 0.010707,
     "end_time": "2024-05-04T11:37:40.683540",
     "exception": false,
     "start_time": "2024-05-04T11:37:40.672833",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For easiness, we will create the following template for QA: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e7d6bf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:37:40.706176Z",
     "iopub.status.busy": "2024-05-04T11:37:40.705910Z",
     "iopub.status.idle": "2024-05-04T11:37:40.715907Z",
     "shell.execute_reply": "2024-05-04T11:37:40.715049Z"
    },
    "papermill": {
     "duration": 0.02333,
     "end_time": "2024-05-04T11:37:40.717754",
     "exception": false,
     "start_time": "2024-05-04T11:37:40.694424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\\n\\nCategory:\\nkaggle-{Category}\\n\\nQuestion:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\n",
    "df[\"prompt\"] = df.apply(lambda row: template.format(Category=row.Category,\n",
    "                                                             Question=row.Question,\n",
    "                                                             Answer=row.Answer), axis=1)\n",
    "data = df.prompt.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c127731",
   "metadata": {
    "papermill": {
     "duration": 0.012057,
     "end_time": "2024-05-04T11:37:40.740716",
     "exception": false,
     "start_time": "2024-05-04T11:37:40.728659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Template utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a42d5989",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:37:40.763656Z",
     "iopub.status.busy": "2024-05-04T11:37:40.763413Z",
     "iopub.status.idle": "2024-05-04T11:37:40.767867Z",
     "shell.execute_reply": "2024-05-04T11:37:40.767039Z"
    },
    "papermill": {
     "duration": 0.018174,
     "end_time": "2024-05-04T11:37:40.769771",
     "exception": false,
     "start_time": "2024-05-04T11:37:40.751597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Category\", \"Question\", \"Answer\"], [\"blue\", \"red\", \"green\"]):\n",
    "        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26437150",
   "metadata": {
    "papermill": {
     "duration": 0.010955,
     "end_time": "2024-05-04T11:37:40.791558",
     "exception": false,
     "start_time": "2024-05-04T11:37:40.780603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Specialized class to query Gemma\n",
    "\n",
    "\n",
    "We define a specialized class to query Gemma. But first, we need to initialize an object of GemmaCausalLM class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75c5808",
   "metadata": {
    "papermill": {
     "duration": 0.010547,
     "end_time": "2024-05-04T11:37:40.813239",
     "exception": false,
     "start_time": "2024-05-04T11:37:40.802692",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initialize the code for Gemma Causal LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a539a5fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:37:40.837293Z",
     "iopub.status.busy": "2024-05-04T11:37:40.836693Z",
     "iopub.status.idle": "2024-05-04T11:38:47.071833Z",
     "shell.execute_reply": "2024-05-04T11:38:47.070955Z"
    },
    "papermill": {
     "duration": 66.249386,
     "end_time": "2024-05-04T11:38:47.073962",
     "exception": false,
     "start_time": "2024-05-04T11:37:40.824576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'task.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'preprocessor.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_causal_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
    "gemma_causal_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb95f0",
   "metadata": {
    "papermill": {
     "duration": 0.013069,
     "end_time": "2024-05-04T11:38:47.100566",
     "exception": false,
     "start_time": "2024-05-04T11:38:47.087497",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define the specialized class\n",
    "\n",
    "Here we define the special class `GemmaQA`. \n",
    "in the `__init__` we pass the `GemmaCausalLM` object created before.\n",
    "The `query` member function uses `GemmaCausalLM` member function `generate` to generate the answer, based on a prompt that includes the category and the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "049c9cca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:38:47.128673Z",
     "iopub.status.busy": "2024-05-04T11:38:47.128019Z",
     "iopub.status.idle": "2024-05-04T11:38:47.134062Z",
     "shell.execute_reply": "2024-05-04T11:38:47.133171Z"
    },
    "papermill": {
     "duration": 0.022065,
     "end_time": "2024-05-04T11:38:47.135966",
     "exception": false,
     "start_time": "2024-05-04T11:38:47.113901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GemmaQA:\n",
    "    def __init__(self, max_length=512):\n",
    "        self.max_length = max_length\n",
    "        self.prompt = template\n",
    "        self.gemma_causal_lm = gemma_causal_lm\n",
    "        \n",
    "    def query(self, category, question):\n",
    "        response = self.gemma_causal_lm.generate(\n",
    "            self.prompt.format(\n",
    "                Category=category,\n",
    "                Question=question,\n",
    "                Answer=\"\"), \n",
    "            max_length=self.max_length)\n",
    "        display(Markdown(colorize_text(response)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75501ba2",
   "metadata": {
    "papermill": {
     "duration": 0.013127,
     "end_time": "2024-05-04T11:38:47.162438",
     "exception": false,
     "start_time": "2024-05-04T11:38:47.149311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Gemma preprocessor\n",
    "\n",
    "\n",
    "This preprocessing layer will take in batches of strings, and return outputs in a ```(x, y, sample_weight)``` format, where the y label is the next token id in the x sequence.\n",
    "\n",
    "From the code below, we can see that, after the preprocessor, the data shape is ```(num_samples, sequence_length)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1aed511",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:38:47.190208Z",
     "iopub.status.busy": "2024-05-04T11:38:47.189933Z",
     "iopub.status.idle": "2024-05-04T11:38:47.539012Z",
     "shell.execute_reply": "2024-05-04T11:38:47.538144Z"
    },
    "papermill": {
     "duration": 0.365758,
     "end_time": "2024-05-04T11:38:47.541508",
     "exception": false,
     "start_time": "2024-05-04T11:38:47.175750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y, sample_weight = gemma_causal_lm.preprocessor(data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a51d5be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:38:47.572101Z",
     "iopub.status.busy": "2024-05-04T11:38:47.571713Z",
     "iopub.status.idle": "2024-05-04T11:38:47.579531Z",
     "shell.execute_reply": "2024-05-04T11:38:47.578238Z"
    },
    "papermill": {
     "duration": 0.025901,
     "end_time": "2024-05-04T11:38:47.581702",
     "exception": false,
     "start_time": "2024-05-04T11:38:47.555801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': Array([[   2,  109, 8606, ...,    0,    0,    0],\n",
      "       [   2,  109, 8606, ...,    0,    0,    0]], dtype=int32), 'padding_mask': Array([[ True,  True,  True, ..., False, False, False],\n",
      "       [ True,  True,  True, ..., False, False, False]], dtype=bool)} [[   109   8606 235292 ...      0      0      0]\n",
      " [   109   8606 235292 ...      0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb21996",
   "metadata": {
    "papermill": {
     "duration": 0.015849,
     "end_time": "2024-05-04T11:38:47.613657",
     "exception": false,
     "start_time": "2024-05-04T11:38:47.597808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Perform fine-tuning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a6bb6",
   "metadata": {
    "papermill": {
     "duration": 0.015995,
     "end_time": "2024-05-04T11:38:47.689621",
     "exception": false,
     "start_time": "2024-05-04T11:38:47.673626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Enable LoRA for the model\n",
    "\n",
    "LoRA rank is setting the number of trainable parameters. A larger rank will result in a larger number of parameters to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2fc5364",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:38:47.723413Z",
     "iopub.status.busy": "2024-05-04T11:38:47.722635Z",
     "iopub.status.idle": "2024-05-04T11:38:48.212476Z",
     "shell.execute_reply": "2024-05-04T11:38:48.211624Z"
    },
    "papermill": {
     "duration": 0.508671,
     "end_time": "2024-05-04T11:38:48.214418",
     "exception": false,
     "start_time": "2024-05-04T11:38:47.705747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to the lora_rank as set in Config (4).\n",
    "gemma_causal_lm.backbone.enable_lora(rank=Config.lora_rank)\n",
    "gemma_causal_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a3439a",
   "metadata": {
    "papermill": {
     "duration": 0.014879,
     "end_time": "2024-05-04T11:38:48.244452",
     "exception": false,
     "start_time": "2024-05-04T11:38:48.229573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We see that only a small part of the parameters are trainable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4350b13a",
   "metadata": {
    "papermill": {
     "duration": 0.014585,
     "end_time": "2024-05-04T11:38:48.274253",
     "exception": false,
     "start_time": "2024-05-04T11:38:48.259668",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run the training sequence\n",
    "\n",
    "We set the `sequence_length` for the `GemmaCausalLM` (from configuration, will be 512).\n",
    "We compile the model, with the loss, optimizer and metric.\n",
    "For the metric, it is used `SparseCategoricalAccuracy`. This metric calculates how often predictions match integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38fff13a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:38:48.306242Z",
     "iopub.status.busy": "2024-05-04T11:38:48.305935Z",
     "iopub.status.idle": "2024-05-04T11:50:48.254639Z",
     "shell.execute_reply": "2024-05-04T11:50:48.253603Z"
    },
    "papermill": {
     "duration": 719.967443,
     "end_time": "2024-05-04T11:50:48.256683",
     "exception": false,
     "start_time": "2024-05-04T11:38:48.289240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 735ms/step - loss: 1.7209 - sparse_categorical_accuracy: 0.5241\n",
      "Epoch 2/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.6869 - sparse_categorical_accuracy: 0.5313\n",
      "Epoch 3/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.6175 - sparse_categorical_accuracy: 0.5417\n",
      "Epoch 4/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.5770 - sparse_categorical_accuracy: 0.5509\n",
      "Epoch 5/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - loss: 1.5537 - sparse_categorical_accuracy: 0.5552\n",
      "Epoch 6/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.5304 - sparse_categorical_accuracy: 0.5568\n",
      "Epoch 7/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.5028 - sparse_categorical_accuracy: 0.5630\n",
      "Epoch 8/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.4733 - sparse_categorical_accuracy: 0.5682\n",
      "Epoch 9/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.4444 - sparse_categorical_accuracy: 0.5745\n",
      "Epoch 10/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.4025 - sparse_categorical_accuracy: 0.5873\n",
      "Epoch 11/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.3607 - sparse_categorical_accuracy: 0.5960\n",
      "Epoch 12/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.3163 - sparse_categorical_accuracy: 0.6079\n",
      "Epoch 13/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.2684 - sparse_categorical_accuracy: 0.6198\n",
      "Epoch 14/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.2149 - sparse_categorical_accuracy: 0.6325\n",
      "Epoch 15/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.1585 - sparse_categorical_accuracy: 0.6450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x79940db6cb20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set sequence length cf. config (512)\n",
    "gemma_causal_lm.preprocessor.sequence_length = Config.sequence_length \n",
    "\n",
    "# Compile the model with loss, optimizer, and metric\n",
    "gemma_causal_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=Config.learning_rate),\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "gemma_causal_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd6bb5",
   "metadata": {
    "papermill": {
     "duration": 0.086828,
     "end_time": "2024-05-04T11:50:48.431835",
     "exception": false,
     "start_time": "2024-05-04T11:50:48.345007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test the fine-tuned model\n",
    "\n",
    "We instantiate an object of class GemmaQA. Because `gemma_causal_lm` was fine-tuned using LoRA, `gemma_qa` defined here will use the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a16e8227",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:50:48.621657Z",
     "iopub.status.busy": "2024-05-04T11:50:48.620969Z",
     "iopub.status.idle": "2024-05-04T11:50:48.625518Z",
     "shell.execute_reply": "2024-05-04T11:50:48.624630Z"
    },
    "papermill": {
     "duration": 0.109128,
     "end_time": "2024-05-04T11:50:48.627364",
     "exception": false,
     "start_time": "2024-05-04T11:50:48.518236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_qa = GemmaQA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4824b5",
   "metadata": {
    "papermill": {
     "duration": 0.091726,
     "end_time": "2024-05-04T11:50:48.819682",
     "exception": false,
     "start_time": "2024-05-04T11:50:48.727956",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For start, we are testing the model with some of the data from the training set itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717d78c7",
   "metadata": {
    "papermill": {
     "duration": 0.087719,
     "end_time": "2024-05-04T11:50:48.995215",
     "exception": false,
     "start_time": "2024-05-04T11:50:48.907496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b41870f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:50:49.171700Z",
     "iopub.status.busy": "2024-05-04T11:50:49.170965Z",
     "iopub.status.idle": "2024-05-04T11:51:08.610909Z",
     "shell.execute_reply": "2024-05-04T11:51:08.609928Z"
    },
    "papermill": {
     "duration": 19.530189,
     "end_time": "2024-05-04T11:51:08.613037",
     "exception": false,
     "start_time": "2024-05-04T11:50:49.082848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-competition\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What are the different types of competitions available on Kaggle?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Types of Competitions\n",
       "\n",
       "There are two main types of competitions on Kaggle:\n",
       "\n",
       "- **Data science challenges**: These are typically data science challenges where the goal is to build a model that outperforms a provided baseline. Examples include building a model to predict the likelihood of a fraudulent transaction or building a model to predict the likelihood of a heart attack.\n",
       "\n",
       "- **Classification challenges**: These are typically classification challenges where the goal is to build a model that outperforms a provided baseline. Examples include building a model to classify emails as spam or ham or building a model to classify images as cats or dogs.\n",
       "\n",
       "## How Are the Kaggle Competitions Ranked?\n",
       "\n",
       "The rankings on Kaggle are based on a combination of two metrics:\n",
       "\n",
       "- **Submissions**: The total number of submissions made by a user.\n",
       "\n",
       "- **Leaderboard Rank**: The rank of a user on the leaderboard of a given competition.\n",
       "\n",
       "The leaderboard rank is calculated by taking the median rank of the top 100 users on the leaderboard. The median rank is calculated by first sorting the leaderboard by leaderboard rank and then taking the middle value of the leaderboard.\n",
       "\n",
       "The combination of these two metrics is designed to encourage users to make high-quality submissions and to reward users who are consistently at the top of the leaderboard."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[0]\n",
    "gemma_qa.query(row.Category,row.Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19983b28",
   "metadata": {
    "papermill": {
     "duration": 0.098831,
     "end_time": "2024-05-04T11:51:08.812853",
     "exception": false,
     "start_time": "2024-05-04T11:51:08.714022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e9c0774",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:51:08.992013Z",
     "iopub.status.busy": "2024-05-04T11:51:08.991304Z",
     "iopub.status.idle": "2024-05-04T11:51:22.386247Z",
     "shell.execute_reply": "2024-05-04T11:51:22.385289Z"
    },
    "papermill": {
     "duration": 13.486869,
     "end_time": "2024-05-04T11:51:22.388528",
     "exception": false,
     "start_time": "2024-05-04T11:51:08.901659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-tpu\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "How to load and save model on TPU?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Overview\n",
       "\n",
       "TPU models are saved in SavedModel format. To load a model from SavedModel, you can use the `tpu.Model.from_saved_model` method.\n",
       "\n",
       "To save a model, you can use the `tpu.Model.save_main_session()` method.\n",
       "\n",
       "## Example\n",
       "\n",
       "```python\n",
       "# Load a model from SavedModel\n",
       "model = tpu.Model.from_saved_model(tpu.saved_model_fn)\n",
       "```\n",
       "\n",
       "## TPU Model Format\n",
       "\n",
       "TPU models are saved in SavedModel format. To load a model from SavedModel, you can use the tpu.Model.from_saved_model method.\n",
       "\n",
       "To save a model, you can use the tpu.Model.save_main_session() method.\n",
       "\n",
       "## TPU Model Format Overview\n",
       "\n",
       "The SavedModel format is a model archive that contains all the information needed to run a model on TPU. It is a zip archive with the following structure:\n",
       "\n",
       "- `model.pb` is the TensorFlow graph that describes the model.\n",
       "- `checkpoint.index` is a binary file that contains the metadata for the checkpoints in the model.\n",
       "- `config.json` is a JSON file that contains the configuration for the model.\n",
       "- `variables.add_file_io_ops.json` is a JSON file that contains the metadata for the file I/O ops in the model.\n",
       "- `variables.add_file_io_ops.pb` is a binary file that contains the code for the file I/O ops in the model.\n",
       "- `variables.add_op_for_each_checkpoint.index` is a binary file that contains the metadata for the checkpoints in the model.\n",
       "- `variables.add_op_for_each_checkpoint.json` is a JSON file that contains the metadata for the checkpoints in the model.\n",
       "- `variables.add_op_for_each_checkpoint.pb` is a binary file that contains the code for the checkpoints in the model.\n",
       "- `variables.add_op_for_each_checkpoint.summary.tsv` is a tab-separated-values file that contains the summary of the checkpoints in the model.\n",
       "- `variables.add_op_for_each_checkpoint.summary.tsv` is a tab-separated-values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[15]\n",
    "gemma_qa.query(row.Category,row.Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eb28a8",
   "metadata": {
    "papermill": {
     "duration": 0.088994,
     "end_time": "2024-05-04T11:51:22.566579",
     "exception": false,
     "start_time": "2024-05-04T11:51:22.477585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea2f9c1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:51:22.746966Z",
     "iopub.status.busy": "2024-05-04T11:51:22.746031Z",
     "iopub.status.idle": "2024-05-04T11:51:36.032243Z",
     "shell.execute_reply": "2024-05-04T11:51:36.031346Z"
    },
    "papermill": {
     "duration": 13.379479,
     "end_time": "2024-05-04T11:51:36.034367",
     "exception": false,
     "start_time": "2024-05-04T11:51:22.654888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-noteboook\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What are the different types of notebooks available on Kaggle?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Notebooks\n",
       "\n",
       "Kaggle Notebooks are interactive markdown documents that allow users to run code and see the results. Notebooks are a key part of the Kaggle experience, and are used by the community to share data science workflows, discuss techniques, and explore new ideas.\n",
       "\n",
       "Kaggle Notebooks are hosted on nbviewer.com. You can access them from any web browser, or from the Kaggle mobile app.\n",
       "\n",
       "You can create a Notebook from the Kaggle website or from the command line using the kaggle notebook command.\n",
       "\n",
       "## Versions\n",
       "\n",
       "Each Notebook has multiple versions. Each version is a snapshot of the Notebook at a specific point in time. You can view and compare the versions of a Notebook to see how it has changed over time.\n",
       "\n",
       "You can access the version history of a Notebook from the Notebook editor.\n",
       "\n",
       "You can create a new version of a Notebook from the Notebook editor.\n",
       "\n",
       "You can delete a Notebook version from the Notebook editor.\n",
       "\n",
       "## Versions and Sharing\n",
       "\n",
       "Each Notebook version has a URL that looks like this:\n",
       "\n",
       "```\n",
       "https://www.kaggle.com/kylegordon/kaggle-noteboook-versions?version=1666666\n",
       "```\n",
       "\n",
       "You can share a Notebook version URL with others, and they will be prompted to view the Notebook as a guest.\n",
       "\n",
       "You can also create a Notebook version from a URL that someone else has shared with you.\n",
       "\n",
       "## Versions and Permissions\n",
       "\n",
       "Each Notebook version has a URL that looks like this:\n",
       "\n",
       "```\n",
       "https://www.kaggle.com/kylegordon/kaggle-noteboook-versions?version=1666666\n",
       "```\n",
       "\n",
       "You can view and compare the versions of a Notebook to see how it has changed over time.\n",
       "\n",
       "You can access the version history of a Notebook from the Notebook editor.\n",
       "\n",
       "You can create a new version of a Notebook from the Notebook editor.\n",
       "\n",
       "You can delete a Notebook version from the Notebook editor.\n",
       "\n",
       "You can also delete a Notebook version from the Versions tab of the Notebook editor.\n",
       "\n",
       "## Versions and Privacy\n",
       "\n",
       "Each Notebook version has a URL that looks like this:\n",
       "\n",
       "```\n",
       "https://www.kaggle.com/kylegordon/kaggle-noteboook-versions?version=1666666\n",
       "```\n",
       "\n",
       "You can share a Notebook version URL with others"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[25]\n",
    "gemma_qa.query(row.Category,row.Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7be228",
   "metadata": {
    "papermill": {
     "duration": 0.089572,
     "end_time": "2024-05-04T11:51:36.213509",
     "exception": false,
     "start_time": "2024-05-04T11:51:36.123937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Not seen question(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "132566c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:51:36.392095Z",
     "iopub.status.busy": "2024-05-04T11:51:36.391696Z",
     "iopub.status.idle": "2024-05-04T11:51:49.894116Z",
     "shell.execute_reply": "2024-05-04T11:51:49.893142Z"
    },
    "papermill": {
     "duration": 13.594261,
     "end_time": "2024-05-04T11:51:49.896104",
     "exception": false,
     "start_time": "2024-05-04T11:51:36.301843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-notebook\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "How to run a notebook?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Installation\n",
       "\n",
       "You can run notebooks on Kaggle without installing anything, but if you want to make use of the community-built add-ons marketplace or you have a team of users that you want to collaborate with on a notebook, then you should consider creating a notebook on Kaggle.\n",
       "\n",
       "To install Kaggle and Notebook features in your own conda environment, run:\n",
       "\n",
       "```\n",
       "kaggle datasets login-or-create <USERNAME>\n",
       "kaggle kernels --help\n",
       "```\n",
       "\n",
       "For a full list of command line arguments, run `kaggle kernels help <command` from the Kaggle command line.\n",
       "\n",
       "## Run Locally\n",
       "\n",
       "If you are running locally, you can run the notebook locally by running the following command from the notebook directory:\n",
       "\n",
       "```python\n",
       "!kaggle run --kernels /kaggle/notebooks/tutorial-saving-and-loading-models: Cells 1-X\n",
       "```\n",
       "\n",
       "The command will run the notebook from the first cell to the most recently executed cell. If you want to run a specific cell, you can specify the cell number after the `--kernels` flag.\n",
       "\n",
       "If you want to run the notebook from scratch, you can run the notebook locally by running the following command from the notebook directory:\n",
       "\n",
       "```python\n",
       "kaggle run --kernels <USERNAME>/notebooks/tutorial-saving-and-loading-models:latest\n",
       "```\n",
       "\n",
       "The command will run the notebook from the first cell to the most recently executed cell.\n",
       "\n",
       "If you want to run a specific cell, you can specify the cell number after the `--kernels` flag.\n",
       "\n",
       "If you want to run the notebook from scratch, you can run the notebook locally by running the following command from the notebook directory:\n",
       "\n",
       "```python\n",
       "kaggle run --kernels <USERNAME>/notebooks/tutorial-saving-and-loading-models:latest\n",
       "```\n",
       "\n",
       "## Run on Kaggle\n",
       "\n",
       "If you are running on Kaggle, you can run the notebook on Kaggle by clicking the “Run Notebook” button in the top right corner of the notebook editor.\n",
       "\n",
       "If you are running locally, you can upload the notebook to Kaggle and run the notebook locally by running the following command from the notebook directory:\n",
       "\n",
       "```python\n",
       "kaggle runnotebooks <USERNAME>/notebooks/tutorial-saving-and-loading-models:latest\n",
       "```\n",
       "\n",
       "If you are running on Kaggle, you can run the notebook"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category = \"notebook\"\n",
    "question = \"How to run a notebook?\"\n",
    "gemma_qa.query(category,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84d9273a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:51:50.074648Z",
     "iopub.status.busy": "2024-05-04T11:51:50.073733Z",
     "iopub.status.idle": "2024-05-04T11:52:03.548705Z",
     "shell.execute_reply": "2024-05-04T11:52:03.547725Z"
    },
    "papermill": {
     "duration": 13.566076,
     "end_time": "2024-05-04T11:52:03.551048",
     "exception": false,
     "start_time": "2024-05-04T11:51:49.984972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-discussions\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "How to create a discussion topic?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Creating a Discussion\n",
       "\n",
       "To create a new discussion topic, click on the \"Discussions\" tab and click on the \"New Discussion\" button.\n",
       "\n",
       "You can create a new discussion by either uploading a dataset or by pasting a URL. If you upload a dataset, the discussion will be associated with the dataset. If you paste a URL, the discussion will be associated with the notebook that URL points to.\n",
       "\n",
       "If you upload a dataset, the discussion will be associated with the dataset. If you paste a URL, the discussion will be associated with the notebook that URL points to.\n",
       "\n",
       "If you are creating a discussion associated with a dataset, the discussion will be public by default. If you are creating a discussion associated with a notebook, the discussion will be private by default.\n",
       "\n",
       "If you want to make the discussion public, click on the \"Privacy\" dropdown and select \"Public\".\n",
       "\n",
       "If you want to make the discussion private, click on the \"Privacy\" dropdown and select \"Private\".\n",
       "\n",
       "If you want to make the discussion a community discussion, click on the \"Privacy\" dropdown and select \"Community\".\n",
       "\n",
       "If you want to make the discussion a team discussion, click on the \"Team\" dropdown and select \"Team\".\n",
       "\n",
       "If you want to make the discussion a user-to-user conversation, click on the \"Conversation\" dropdown and select \"User to User\".\n",
       "\n",
       "If you want to make the discussion a notebook discussion, click on the \"Notebook\" dropdown and select \"Notebook\".\n",
       "\n",
       "If you want to create a discussion associated with a dataset, click on the \"Dataset\" dropdown and select the dataset you want to associate the discussion with.\n",
       "\n",
       "If you want to create a discussion associated with a notebook, click on the \"Notebook\" dropdown and select the notebook you want to associate the discussion with.\n",
       "\n",
       "If you are creating a discussion associated with a dataset, you will be presented with a list of all the datasets associated with the selected folder.\n",
       "\n",
       "If you are creating a discussion associated with a notebook, you will be presented with a list of all the notebooks associated with the selected folder.\n",
       "\n",
       "If you are creating a discussion associated with a dataset, you will be presented with a list of all the users who have access to the dataset.\n",
       "\n",
       "If you are creating a discussion associated with a notebook, you will be presented with a list of all the users who have access to the notebook.\n",
       "\n",
       "If you are creating"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category = \"discussions\"\n",
    "question = \"How to create a discussion topic?\"\n",
    "gemma_qa.query(category,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2cded7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:52:03.738626Z",
     "iopub.status.busy": "2024-05-04T11:52:03.737832Z",
     "iopub.status.idle": "2024-05-04T11:52:17.214186Z",
     "shell.execute_reply": "2024-05-04T11:52:17.213113Z"
    },
    "papermill": {
     "duration": 13.569291,
     "end_time": "2024-05-04T11:52:17.216331",
     "exception": false,
     "start_time": "2024-05-04T11:52:03.647040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-competitions\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a code competition?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Code competitions are a type of competition where participants are given a dataset and a set of tools to build machine learning models. The goal is to build the best model for a given task. Code competitions are a popular format for Kaggle Datasets because they encourage participants to use the tools and techniques available on Kaggle.\n",
       "\n",
       "Code competitions are a great way to learn new tools and techniques, and to see how they can be applied to a real-world problem. They also provide a structured way to evaluate and improve your model, as you'll be competing against other participants who are using the same dataset and tools.\n",
       "\n",
       "There are many benefits to participating in code competitions. First, they're a great way to learn how to use a dataset and a task to drive model development. Second, they're a great way to improve your model's performance. Third, they're a great way to meet other data scientists and machine learning enthusiasts. Fourth, they're a lot of fun!\n",
       "\n",
       "## Types of Code Competitions\n",
       "\n",
       "There are two main types of code competitions on Kaggle: public and private.\n",
       "\n",
       "Public competitions are open to anyone and everyone. They're a great way to see what other people are working on in the community, and to learn from others' experiences. Public competitions are a great way to get exposure and recognition for your work.\n",
       "\n",
       "Private competitions are invitation-only and typically invitees are part of a larger community or organization. They're a great way to collaborate with others who share your interest in the same topic. Private competitions are a great way to get feedback on your work from experts in the field.\n",
       "\n",
       "## How to Participate in a Code Competition\n",
       "\n",
       "To participate in a code competition, you'll need to create a competition notebook. This notebook will contain all of the code you use to build your model. You can then submit your notebook to the competition.\n",
       "\n",
       "The first step is to find a competition that interests you. You can browse all of the current competitions on Kaggle Datasets by visiting the [code competitions page](https://www.kaggle.com/competitions/code/overview).\n",
       "\n",
       "Once you've found a competition you'd like to enter, click on the \"Participate\" button. This will take you to a page where you can sign up for the competition.\n",
       "\n",
       "The next step is to familiarize yourself with the dataset. This is the collection"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category = \"competitions\"\n",
    "question = \"What is a code competition?\"\n",
    "gemma_qa.query(category,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cf99762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T11:52:17.402274Z",
     "iopub.status.busy": "2024-05-04T11:52:17.401883Z",
     "iopub.status.idle": "2024-05-04T11:52:30.769326Z",
     "shell.execute_reply": "2024-05-04T11:52:30.768314Z"
    },
    "papermill": {
     "duration": 13.460513,
     "end_time": "2024-05-04T11:52:30.771348",
     "exception": false,
     "start_time": "2024-05-04T11:52:17.310835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-datasets\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What are the steps to create a Kaggle dataset?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Introduction\n",
       "\n",
       "Kaggle Datasets is a repository of public datasets that are created by members of the Kaggle community. Datasets are used by the Kaggle community for a variety of purposes, including training machine learning models, evaluating models, and sharing data.\n",
       "\n",
       "Kaggle Datasets is a great resource for anyone working with data.\n",
       "\n",
       "## Getting Started\n",
       "\n",
       "To get started creating a new dataset, navigate to the \"Create Dataset\" button in the left-hand navigation bar.\n",
       "\n",
       "You can also access the \"Create Dataset\" page from your profile by clicking on your username in the top-right corner of any page on Kaggle and selecting \"Create Dataset\" from the drop-down menu.\n",
       "\n",
       "There are two ways to create a new dataset:\n",
       "\n",
       "- Upload a file: If you have a large dataset that you want to upload, this is the way to go. Simply browse to the location of the file on your computer and select \"Upload\" to begin the upload process.\n",
       "- Generate a new dataset: If you don't have a large dataset to upload, this is the way to go. You can choose from a variety of template datasets to get started, or you can start from scratch by selecting \"Create Blank Dataset\" and editing the dataset settings to your liking.\n",
       "\n",
       "Once you have created your dataset, you will be taken to the \"Manage Dataset\" page. Here, you can edit the metadata (e.g. title, description, tags), permissions (who can view and edit the dataset), and the dataset files themselves.\n",
       "\n",
       "## Analytics\n",
       "\n",
       "Analytics are available to help you understand how your dataset is being used. You can see the number of downloads, clones, and edits your dataset has received over time, as well as the number of unique users who have accessed the dataset.\n",
       "\n",
       "To access analytics, navigate to the \"Manage Dataset\" page and click on the \"Analytics\" tab.\n",
       "\n",
       "## Sharing\n",
       "\n",
       "Kaggle Datasets is a community of data scientists, machine learning engineers, and data enthusiasts. By sharing your datasets with others in the community, you can help advance the state of the art in machine learning and data science.\n",
       "\n",
       "To share your dataset, navigate to the \"Manage Dataset\" page and click on the \"Share\" tab. Here, you can choose from a variety of sharing options, including public, private, and unlisted.\n",
       "\n",
       "## Search\n",
       "\n",
       "Kaggle Datasets is a"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category = \"datasets\"\n",
    "question = \"What are the steps to create a Kaggle dataset?\"\n",
    "gemma_qa.query(category,question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba6915",
   "metadata": {
    "papermill": {
     "duration": 0.093187,
     "end_time": "2024-05-04T11:52:30.953748",
     "exception": false,
     "start_time": "2024-05-04T11:52:30.860561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea76959",
   "metadata": {
    "papermill": {
     "duration": 0.088401,
     "end_time": "2024-05-04T11:52:31.130839",
     "exception": false,
     "start_time": "2024-05-04T11:52:31.042438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We demonstated how to fine-tune a Gemma model using LoRA.   \n",
    "We also created a class to run queries to the Gemma model and tested it with some examples from the existing training data but also with some new, not seen questions."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7669720,
     "sourceId": 64148,
     "sourceType": "competition"
    },
    {
     "datasetId": 4484051,
     "sourceId": 7711309,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 5171,
     "sourceId": 11371,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 943.560823,
   "end_time": "2024-05-04T11:52:35.237524",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-04T11:36:51.676701",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
