{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f375d009",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.011972,
     "end_time": "2024-03-31T18:24:32.068349",
     "exception": false,
     "start_time": "2024-03-31T18:24:32.056377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center><h1>Fine-tunning Gemma model with Kaggle Docs data</h1></center>\n",
    "\n",
    "<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This notebook will demonstrate three things:\n",
    "\n",
    "1. How to fine-tune Gemma model using LoRA\n",
    "2. Creation of a specialised class to query about Kaggle features\n",
    "3. Some results of querying about Kaggle Docs\n",
    "\n",
    "This work is largely based on previous work. Here I list the sources:\n",
    "\n",
    "1. Gemma Model Card, Kaggle Models, https://www.kaggle.com/models/google/gemma\n",
    "2. Kaggle QA with Gemma - KerasNLP Starter, Kaggle Code, https://www.kaggle.com/code/awsaf49/kaggle-qa-with-gemma-kerasnlp-starter (Version 11)  \n",
    "3. Fine-tune Gemma models in Keras using LoRA, Kaggle Code, https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora (Version 1)  \n",
    "4. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, LoRA: Low-Rank Adaptation of Large Language Models, ArXiv, https://arxiv.org/pdf/2106.09685.pdf\n",
    "5. Abheesht Sharma, Matthew Watson, Parameter-efficient fine-tuning of GPT-2 with LoRA, https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/\n",
    "6. Keras 3 API documentation / KerasNLP / Models / Gemma, https://keras.io/api/keras_nlp/models/gemma/\n",
    "7. Kaggle Docs, Kaggle Dataset, https://www.kaggle.com/datasets/awsaf49/kaggle-docs\n",
    "\n",
    "**Let's go**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481850c4",
   "metadata": {
    "papermill": {
     "duration": 0.011453,
     "end_time": "2024-03-31T18:24:32.091748",
     "exception": false,
     "start_time": "2024-03-31T18:24:32.080295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is Gemma?\n",
    "\n",
    "\n",
    "Gemma is a collection of lightweight source generative AI models designed to be used mostly by developers and researchers. Created by Google DeepMind research lab that also developed Gemini, Gemma is available in several versions, with 2B and 7B parameters, as following:\n",
    "\n",
    "\n",
    "| Model                  | Parameters      | Tuned versions    | Description                                    | Recomemnded target platforms       |\n",
    "|------------------------|-----------------|-------------------|------------------------------------------------|------------------------------------|\n",
    "| `gemma_2b_en`          | 2.51B           | Pretrained        | 18-layer Gemma model (Gemma with 2B parameters)|Mobile devices and laptops          |\n",
    "| `gemma_instruct_2b_en` | 2.51B           | Instruction tuned | 18-layer Gemma model (Gemma with 2B parameters)| Mobile devices and laptops         | \n",
    "| `gemma_7b_en`          | 8.54B           | Pretrained        | 28-layer Gemma model (Gemma with 7B parameters)| Desktop computers and small servers|\n",
    "| `gemma_instruct_7b_en` | 8.54B           | Instruction tuned | 28-layer Gemma model (Gemma with 7B parameters)| Desktop computers and small servers|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a3f4d",
   "metadata": {
    "papermill": {
     "duration": 0.011535,
     "end_time": "2024-03-31T18:24:32.115056",
     "exception": false,
     "start_time": "2024-03-31T18:24:32.103521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is LoRA?  \n",
    "\n",
    "LoRA stands for Low-Rank Adaptation. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to LoRA paper, this number decreases 10,000 times, and the computational resources size decreases 3 times. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40be4e9",
   "metadata": {
    "papermill": {
     "duration": 0.011541,
     "end_time": "2024-03-31T18:24:32.139408",
     "exception": false,
     "start_time": "2024-03-31T18:24:32.127867",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How we proceed?\n",
    "\n",
    "For fine-tunning with LoRA, we will follow the steps:\n",
    "\n",
    "1. Install prerequisites\n",
    "2. Load and process the data for fine-tuning\n",
    "3. Initialize the code for Gemma causal language model (Gemma Causal LM)\n",
    "4. Perform fine-tuning\n",
    "5. Test the fine-tunned model with questions from the data used for fine-tuning and with aditional questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06f9f64",
   "metadata": {
    "papermill": {
     "duration": 0.011356,
     "end_time": "2024-03-31T18:24:32.162395",
     "exception": false,
     "start_time": "2024-03-31T18:24:32.151039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prerequisites\n",
    "\n",
    "\n",
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2cca4c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:24:32.188578Z",
     "iopub.status.busy": "2024-03-31T18:24:32.187616Z",
     "iopub.status.idle": "2024-03-31T18:25:05.871438Z",
     "shell.execute_reply": "2024-03-31T18:25:05.870414Z"
    },
    "papermill": {
     "duration": 33.699698,
     "end_time": "2024-03-31T18:25:05.874187",
     "exception": false,
     "start_time": "2024-03-31T18:24:32.174489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras>=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece3397",
   "metadata": {
    "papermill": {
     "duration": 0.011984,
     "end_time": "2024-03-31T18:25:05.898117",
     "exception": false,
     "start_time": "2024-03-31T18:25:05.886133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad49b62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:25:05.924879Z",
     "iopub.status.busy": "2024-03-31T18:25:05.924454Z",
     "iopub.status.idle": "2024-03-31T18:25:22.166990Z",
     "shell.execute_reply": "2024-03-31T18:25:22.166001Z"
    },
    "papermill": {
     "duration": 16.259279,
     "end_time": "2024-03-31T18:25:22.169596",
     "exception": false,
     "start_time": "2024-03-31T18:25:05.910317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 18:25:10.746069: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-31 18:25:10.746181: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-31 18:25:10.907272: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas() # progress bar for pandas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaee2cf",
   "metadata": {
    "papermill": {
     "duration": 0.011808,
     "end_time": "2024-03-31T18:25:22.193891",
     "exception": false,
     "start_time": "2024-03-31T18:25:22.182083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b249e4db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:25:22.220238Z",
     "iopub.status.busy": "2024-03-31T18:25:22.219077Z",
     "iopub.status.idle": "2024-03-31T18:25:22.224453Z",
     "shell.execute_reply": "2024-03-31T18:25:22.223500Z"
    },
    "papermill": {
     "duration": 0.020753,
     "end_time": "2024-03-31T18:25:22.226651",
     "exception": false,
     "start_time": "2024-03-31T18:25:22.205898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    seed = 42\n",
    "    dataset_path = \"/kaggle/input/kaggle-docs/questions_answers\"\n",
    "    preset = \"gemma_2b_en\" # name of pretrained Gemma\n",
    "    sequence_length = 512 # max size of input sequence for training\n",
    "    batch_size = 1 # size of the input batch in training, x 2 as two GPUs\n",
    "    epochs = 10 # number of epochs to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "316d8c45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:25:22.254724Z",
     "iopub.status.busy": "2024-03-31T18:25:22.253987Z",
     "iopub.status.idle": "2024-03-31T18:25:22.259247Z",
     "shell.execute_reply": "2024-03-31T18:25:22.258208Z"
    },
    "papermill": {
     "duration": 0.021944,
     "end_time": "2024-03-31T18:25:22.261665",
     "exception": false,
     "start_time": "2024-03-31T18:25:22.239721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(Config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7be8ca",
   "metadata": {
    "papermill": {
     "duration": 0.012438,
     "end_time": "2024-03-31T18:25:22.287664",
     "exception": false,
     "start_time": "2024-03-31T18:25:22.275226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utils\n",
    "\n",
    "This is an utility function that we will include in our class for QA to format the answer to our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3796aa78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:25:22.315695Z",
     "iopub.status.busy": "2024-03-31T18:25:22.315341Z",
     "iopub.status.idle": "2024-03-31T18:25:22.321161Z",
     "shell.execute_reply": "2024-03-31T18:25:22.320200Z"
    },
    "papermill": {
     "duration": 0.022402,
     "end_time": "2024-03-31T18:25:22.323330",
     "exception": false,
     "start_time": "2024-03-31T18:25:22.300928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Category\", \"Question\", \"Answer\"], [\"blue\", \"red\", \"green\"]):\n",
    "        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b4ab41",
   "metadata": {
    "papermill": {
     "duration": 0.013377,
     "end_time": "2024-03-31T18:25:22.351026",
     "exception": false,
     "start_time": "2024-03-31T18:25:22.337649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28ffaf13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:25:22.379506Z",
     "iopub.status.busy": "2024-03-31T18:25:22.378785Z",
     "iopub.status.idle": "2024-03-31T18:25:22.411180Z",
     "shell.execute_reply": "2024-03-31T18:25:22.410070Z"
    },
    "papermill": {
     "duration": 0.049421,
     "end_time": "2024-03-31T18:25:22.414113",
     "exception": false,
     "start_time": "2024-03-31T18:25:22.364692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the different types of competitions a...</td>\n",
       "      <td># Types of Competitions\\n\\nKaggle Competitions...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the different competition formats on ...</td>\n",
       "      <td>There are handful of different formats competi...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to join a competition?</td>\n",
       "      <td>Before you start, navigate to the [Competition...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to form, manage, and disband teams in a co...</td>\n",
       "      <td>Everyone that competes in a Competition does s...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I make a submission in a competition?</td>\n",
       "      <td>You will need to submit your model predictions...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  What are the different types of competitions a...   \n",
       "1  What are the different competition formats on ...   \n",
       "2                         How to join a competition?   \n",
       "3  How to form, manage, and disband teams in a co...   \n",
       "4       How do I make a submission in a competition?   \n",
       "\n",
       "                                              Answer     Category  \n",
       "0  # Types of Competitions\\n\\nKaggle Competitions...  competition  \n",
       "1  There are handful of different formats competi...  competition  \n",
       "2  Before you start, navigate to the [Competition...  competition  \n",
       "3  Everyone that competes in a Competition does s...  competition  \n",
       "4  You will need to submit your model predictions...  competition  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{Config.dataset_path}/data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4111c3b",
   "metadata": {
    "papermill": {
     "duration": 0.012946,
     "end_time": "2024-03-31T18:25:22.442012",
     "exception": false,
     "start_time": "2024-03-31T18:25:22.429066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's check the total number of rows in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a998af1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:25:22.469866Z",
     "iopub.status.busy": "2024-03-31T18:25:22.469450Z",
     "iopub.status.idle": "2024-03-31T18:25:22.476378Z",
     "shell.execute_reply": "2024-03-31T18:25:22.475411Z"
    },
    "papermill": {
     "duration": 0.023342,
     "end_time": "2024-03-31T18:25:22.478559",
     "exception": false,
     "start_time": "2024-03-31T18:25:22.455217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe41ac",
   "metadata": {
    "papermill": {
     "duration": 0.012736,
     "end_time": "2024-03-31T18:25:22.504658",
     "exception": false,
     "start_time": "2024-03-31T18:25:22.491922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For easiness, we will create the following template for QA: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a227840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:25:22.532020Z",
     "iopub.status.busy": "2024-03-31T18:25:22.531620Z",
     "iopub.status.idle": "2024-03-31T18:25:22.563105Z",
     "shell.execute_reply": "2024-03-31T18:25:22.562029Z"
    },
    "papermill": {
     "duration": 0.050543,
     "end_time": "2024-03-31T18:25:22.568083",
     "exception": false,
     "start_time": "2024-03-31T18:25:22.517540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70f7e689a6d46ac9419fa4de63a4460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "template = \"\\n\\nCategory:\\nkaggle-{Category}\\n\\nQuestion:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\n",
    "df[\"prompt\"] = df.progress_apply(lambda row: template.format(Category=row.Category,\n",
    "                                                             Question=row.Question,\n",
    "                                                             Answer=row.Answer), axis=1)\n",
    "data = df.prompt.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965e07e",
   "metadata": {
    "papermill": {
     "duration": 0.012888,
     "end_time": "2024-03-31T18:25:22.594337",
     "exception": false,
     "start_time": "2024-03-31T18:25:22.581449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Template utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86599c7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:25:22.621813Z",
     "iopub.status.busy": "2024-03-31T18:25:22.621398Z",
     "iopub.status.idle": "2024-03-31T18:25:22.626847Z",
     "shell.execute_reply": "2024-03-31T18:25:22.625853Z"
    },
    "papermill": {
     "duration": 0.021863,
     "end_time": "2024-03-31T18:25:22.628927",
     "exception": false,
     "start_time": "2024-03-31T18:25:22.607064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Category\", \"Question\", \"Answer\"], [\"blue\", \"red\", \"green\"]):\n",
    "        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9559f0",
   "metadata": {
    "papermill": {
     "duration": 0.012805,
     "end_time": "2024-03-31T18:25:22.655000",
     "exception": false,
     "start_time": "2024-03-31T18:25:22.642195",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Specialized class to query Gemma\n",
    "\n",
    "\n",
    "We define a specialized class to query Gemma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af29c9d8",
   "metadata": {
    "papermill": {
     "duration": 0.012375,
     "end_time": "2024-03-31T18:25:22.679994",
     "exception": false,
     "start_time": "2024-03-31T18:25:22.667619",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initialize the code for Gemma Causal LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "666939c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:25:22.706594Z",
     "iopub.status.busy": "2024-03-31T18:25:22.705950Z",
     "iopub.status.idle": "2024-03-31T18:26:17.358346Z",
     "shell.execute_reply": "2024-03-31T18:26:17.357355Z"
    },
    "papermill": {
     "duration": 54.667988,
     "end_time": "2024-03-31T18:26:17.360579",
     "exception": false,
     "start_time": "2024-03-31T18:25:22.692591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_causal_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
    "gemma_causal_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bb9214",
   "metadata": {
    "papermill": {
     "duration": 0.014477,
     "end_time": "2024-03-31T18:26:17.389474",
     "exception": false,
     "start_time": "2024-03-31T18:26:17.374997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define the specialized class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a0a6f37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:26:17.420121Z",
     "iopub.status.busy": "2024-03-31T18:26:17.419668Z",
     "iopub.status.idle": "2024-03-31T18:26:17.426590Z",
     "shell.execute_reply": "2024-03-31T18:26:17.425522Z"
    },
    "papermill": {
     "duration": 0.024643,
     "end_time": "2024-03-31T18:26:17.428810",
     "exception": false,
     "start_time": "2024-03-31T18:26:17.404167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GemmaQA:\n",
    "    def __init__(self, max_length=512):\n",
    "        self.max_length = max_length\n",
    "        self.prompt = template\n",
    "        self.gemma_causal_lm = gemma_causal_lm\n",
    "        \n",
    "    def query(self, category, question):\n",
    "        response = self.gemma_causal_lm.generate(\n",
    "            self.prompt.format(\n",
    "                Category=category,\n",
    "                Question=question,\n",
    "                Answer=\"\"), \n",
    "            max_length=self.max_length)\n",
    "        display(Markdown(colorize_text(response)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ec723",
   "metadata": {
    "papermill": {
     "duration": 0.014192,
     "end_time": "2024-03-31T18:26:17.457492",
     "exception": false,
     "start_time": "2024-03-31T18:26:17.443300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test the GemmaQA class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a1a2785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:26:17.488220Z",
     "iopub.status.busy": "2024-03-31T18:26:17.487741Z",
     "iopub.status.idle": "2024-03-31T18:26:42.586914Z",
     "shell.execute_reply": "2024-03-31T18:26:42.585904Z"
    },
    "papermill": {
     "duration": 25.117147,
     "end_time": "2024-03-31T18:26:42.589140",
     "exception": false,
     "start_time": "2024-03-31T18:26:17.471993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is Kaggle?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Kaggle is a platform for data scientists to compete with each other.\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a kernel?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "A kernel is a piece of code that you write and submit to Kaggle.\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a leaderboard?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "A leaderboard is a list of the top performers on a Kaggle competition.\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a dataset?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "A dataset is a collection of data that you can use to train your model.\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a submission?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "A submission is the result of your model on a Kaggle competition.\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a kernel?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "A kernel is a piece of code that you write and submit to Kaggle.\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a leaderboard?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "A leaderboard is a list of the top performers on a Kaggle competition.\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a dataset?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "A dataset is a collection of data that you can use to train your model.\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a submission?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "A submission is the result of your model on a Kaggle competition.\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a kernel?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "A kernel is a piece of code that you write and submit to Kaggle.\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a leaderboard?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "A leaderboard is a list of the top performers on a Kaggle competition.\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a dataset?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "A dataset is a collection of data that you can use to train your model.\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a submission?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "A submission is the result of your model on a Kaggle competition.\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a kernel?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "A kernel is a piece of code that you write and submit to Kaggle.\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a leaderboard?\n",
       "\n",
       "Answer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_qa = GemmaQA()\n",
    "category=\"\"\n",
    "question=\"What is Kaggle?\"\n",
    "gemma_qa.query(category, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add72b09",
   "metadata": {
    "papermill": {
     "duration": 0.014277,
     "end_time": "2024-03-31T18:26:42.618311",
     "exception": false,
     "start_time": "2024-03-31T18:26:42.604034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Gemma preprocessor\n",
    "\n",
    "\n",
    "This preprocessing layer will take in batches of strings, and return outputs in a ```(x, y, sample_weight)``` format, where the y label is the next token id in the x sequence.\n",
    "\n",
    "From the code below, we can see that, after the preprocessor, the data shape is ```(num_samples, sequence_length)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a76c400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:26:42.710362Z",
     "iopub.status.busy": "2024-03-31T18:26:42.709993Z",
     "iopub.status.idle": "2024-03-31T18:26:43.046722Z",
     "shell.execute_reply": "2024-03-31T18:26:43.045762Z"
    },
    "papermill": {
     "duration": 0.415775,
     "end_time": "2024-03-31T18:26:43.049289",
     "exception": false,
     "start_time": "2024-03-31T18:26:42.633514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y, sample_weight = gemma_causal_lm.preprocessor(data[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79061b71",
   "metadata": {
    "papermill": {
     "duration": 0.01512,
     "end_time": "2024-03-31T18:26:43.079868",
     "exception": false,
     "start_time": "2024-03-31T18:26:43.064748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Perform fine-tuning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d75e982",
   "metadata": {
    "papermill": {
     "duration": 0.014986,
     "end_time": "2024-03-31T18:26:43.109970",
     "exception": false,
     "start_time": "2024-03-31T18:26:43.094984",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Enable LoRA for the model\n",
    "\n",
    "LoRA rank is setting the number of trainable parameters. A larger rank will result in a larger number of parameters to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1db7a63e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:26:43.142180Z",
     "iopub.status.busy": "2024-03-31T18:26:43.141800Z",
     "iopub.status.idle": "2024-03-31T18:26:43.655372Z",
     "shell.execute_reply": "2024-03-31T18:26:43.654355Z"
    },
    "papermill": {
     "duration": 0.532566,
     "end_time": "2024-03-31T18:26:43.657600",
     "exception": false,
     "start_time": "2024-03-31T18:26:43.125034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to 4.\n",
    "gemma_causal_lm.backbone.enable_lora(rank=4)\n",
    "gemma_causal_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd3b141",
   "metadata": {
    "papermill": {
     "duration": 0.015266,
     "end_time": "2024-03-31T18:26:43.688466",
     "exception": false,
     "start_time": "2024-03-31T18:26:43.673200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run the training sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7b4c564",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:26:43.722649Z",
     "iopub.status.busy": "2024-03-31T18:26:43.721738Z",
     "iopub.status.idle": "2024-03-31T18:35:08.492366Z",
     "shell.execute_reply": "2024-03-31T18:35:08.491296Z"
    },
    "papermill": {
     "duration": 504.790032,
     "end_time": "2024-03-31T18:35:08.494602",
     "exception": false,
     "start_time": "2024-03-31T18:26:43.704570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 736ms/step - loss: 1.7209 - sparse_categorical_accuracy: 0.5241\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 731ms/step - loss: 1.6869 - sparse_categorical_accuracy: 0.5313\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 732ms/step - loss: 1.6175 - sparse_categorical_accuracy: 0.5417\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 731ms/step - loss: 1.5770 - sparse_categorical_accuracy: 0.5509\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 732ms/step - loss: 1.5537 - sparse_categorical_accuracy: 0.5552\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - loss: 1.5304 - sparse_categorical_accuracy: 0.5568\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 731ms/step - loss: 1.5028 - sparse_categorical_accuracy: 0.5630\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 732ms/step - loss: 1.4733 - sparse_categorical_accuracy: 0.5682\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 732ms/step - loss: 1.4444 - sparse_categorical_accuracy: 0.5747\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 732ms/step - loss: 1.4025 - sparse_categorical_accuracy: 0.5873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x79a11455df60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit the input sequence length to 512 (to control memory usage).\n",
    "gemma_causal_lm.preprocessor.sequence_length = Config.sequence_length \n",
    "\n",
    "# Compile the model with loss, optimizer, and metric\n",
    "gemma_causal_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=8e-5),\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "gemma_causal_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3567570",
   "metadata": {
    "papermill": {
     "duration": 0.071746,
     "end_time": "2024-03-31T18:35:08.637966",
     "exception": false,
     "start_time": "2024-03-31T18:35:08.566220",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c8d4a0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:35:08.781384Z",
     "iopub.status.busy": "2024-03-31T18:35:08.780986Z",
     "iopub.status.idle": "2024-03-31T18:35:08.785839Z",
     "shell.execute_reply": "2024-03-31T18:35:08.784845Z"
    },
    "papermill": {
     "duration": 0.07872,
     "end_time": "2024-03-31T18:35:08.788022",
     "exception": false,
     "start_time": "2024-03-31T18:35:08.709302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_qa = GemmaQA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02371fd",
   "metadata": {
    "papermill": {
     "duration": 0.071705,
     "end_time": "2024-03-31T18:35:08.929804",
     "exception": false,
     "start_time": "2024-03-31T18:35:08.858099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ebff1e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:35:09.077905Z",
     "iopub.status.busy": "2024-03-31T18:35:09.077499Z",
     "iopub.status.idle": "2024-03-31T18:35:36.813318Z",
     "shell.execute_reply": "2024-03-31T18:35:36.812215Z"
    },
    "papermill": {
     "duration": 27.812091,
     "end_time": "2024-03-31T18:35:36.815499",
     "exception": false,
     "start_time": "2024-03-31T18:35:09.003408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-competition\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What are the different types of competitions available on Kaggle?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "There are several competition types on Kaggle.\n",
       "\n",
       "## Competitions\n",
       "\n",
       "This is the type of competition you will most likely be interested in if you are a new user looking to participate in a competition. Competitions are organized by the Kaggle team and can be public or invite-only. They are typically open for 2-4 weeks and can be either public or invite-only.\n",
       "\n",
       "You can participate in the competition as an individual or as a team. You can join a public team or create your own. You can also join an existing team, but it’s more likely that you’ll find a team that you want to join.\n",
       "\n",
       "If you are a team, you’ll need to invite one or more teammates to the competition. Once they have accepted, the team is created and ready to go.\n",
       "\n",
       "If you join a public team, it’ll show up on your Team Leaderboard and Team Leaderboard Page, which you can visit to track how you’re doing as a team. You can see your individual leaderboard as well, which shows how you’re performing compared to everyone else who has joined the competition as an individual.\n",
       "\n",
       "If you’re a public team member, you’ll also see your team’s leaderboard and leaderboard page on Kaggle.\n",
       "\n",
       "If you create a private team, only you and your teammates will see your team’s leaderboard and leaderboard page on Kaggle.\n",
       "\n",
       "You can find out more about teams and how they work by reading the Team Guide.\n",
       "\n",
       "## Datasets\n",
       "\n",
       "Kaggle datasets are a great way to share your models and data with other Kaggle users. They can be anything from data sets to notebooks to competitions. You can find datasets by searching on Kaggle or by browsing the Datasets Directory.\n",
       "\n",
       "## Challenges\n",
       "\n",
       "Kaggle challenges are a new way to engage in the community. Anyone can create a challenge to invite other users to collaborate on a project. Challenges are typically open-ended and open-source.\n",
       "\n",
       "If you’re interested in participating in a challenge, we recommend reading through the challenge description and rules before joining. You should also check out the other users who have already signed up to see how they’re approaching it.\n",
       "\n",
       "## Competitions with Datasets\n",
       "\n",
       "If you have a dataset you want to share in a competition, you can do that too! You can create a dataset in your profile and make it public or invite other Kag"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[0]\n",
    "gemma_qa.query(row.Category,row.Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d35f443",
   "metadata": {
    "papermill": {
     "duration": 0.088786,
     "end_time": "2024-03-31T18:35:36.977295",
     "exception": false,
     "start_time": "2024-03-31T18:35:36.888509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0beab53a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:35:37.144433Z",
     "iopub.status.busy": "2024-03-31T18:35:37.143974Z",
     "iopub.status.idle": "2024-03-31T18:35:40.685917Z",
     "shell.execute_reply": "2024-03-31T18:35:40.684886Z"
    },
    "papermill": {
     "duration": 3.622698,
     "end_time": "2024-03-31T18:35:40.688371",
     "exception": false,
     "start_time": "2024-03-31T18:35:37.065673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-tpu\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "How to load and save model on TPU?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "TpuModel.load_model()\n",
       "\n",
       "## Load a model\n",
       "\n",
       "The model can be loaded using `load_model()` function. The following code loads the MobileNetV2 model from a SavedModel checkpoint.\n",
       "\n",
       "The `load_model()` function is also available for loading from a SavedModel file (.pb). This is useful for loading models with TPUv2, as SavedModel files cannot be loaded on TPUv2.\n",
       "\n",
       "## Save a model\n",
       "\n",
       "The model can be saved to disk using the `save_model()` and `save_checkpoint()` function:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[15]\n",
    "gemma_qa.query(row.Category,row.Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21185bd",
   "metadata": {
    "papermill": {
     "duration": 0.070058,
     "end_time": "2024-03-31T18:35:40.838370",
     "exception": false,
     "start_time": "2024-03-31T18:35:40.768312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79dbf2b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:35:40.981214Z",
     "iopub.status.busy": "2024-03-31T18:35:40.980332Z",
     "iopub.status.idle": "2024-03-31T18:35:44.645715Z",
     "shell.execute_reply": "2024-03-31T18:35:44.644612Z"
    },
    "papermill": {
     "duration": 3.739529,
     "end_time": "2024-03-31T18:35:44.648073",
     "exception": false,
     "start_time": "2024-03-31T18:35:40.908544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-noteboook\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What are the different types of notebooks available on Kaggle?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "There are two main types of notebooks on Kaggle: Public or Shared Notebooks, and Private Notebooks.\n",
       "\n",
       "## Public or Shared Notebooks\n",
       "\n",
       "Public or Shared Notebooks are available for everyone. These notebooks can either be created and owned by Kaggle users, or shared by other Kaggle users. Anyone can view these notebooks, but you need to be logged in to Kaggle in order to run them.\n",
       "\n",
       "## Private Notebooks\n",
       "\n",
       "Private notebooks are owned by Kaggle users and are only available to those with the appropriate permission. These can either be created by Kaggle users, or shared by other Kaggle users."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[25]\n",
    "gemma_qa.query(row.Category,row.Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba9688f",
   "metadata": {
    "papermill": {
     "duration": 0.073482,
     "end_time": "2024-03-31T18:35:44.795517",
     "exception": false,
     "start_time": "2024-03-31T18:35:44.722035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Not seen question(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ae53c5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:35:44.939989Z",
     "iopub.status.busy": "2024-03-31T18:35:44.939256Z",
     "iopub.status.idle": "2024-03-31T18:35:47.860276Z",
     "shell.execute_reply": "2024-03-31T18:35:47.859175Z"
    },
    "papermill": {
     "duration": 2.994859,
     "end_time": "2024-03-31T18:35:47.862440",
     "exception": false,
     "start_time": "2024-03-31T18:35:44.867581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-notebook\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "How to run a notebook?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "When you click on the \"Run\" button, you can choose whether to run the entire notebook or just a single cell. If you select \"Entire notebook\", then all of the cells of the notebook will be executed.\n",
       "\n",
       "You can also use the \"Run cell\" button to only run a single cell in a notebook.\n",
       "\n",
       "If you have any questions about using notebooks, you can also use the “Ask question” button in the bottom left corner of the notebook."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category = \"notebook\"\n",
    "question = \"How to run a notebook?\"\n",
    "gemma_qa.query(category,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af093ff7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:35:48.011478Z",
     "iopub.status.busy": "2024-03-31T18:35:48.010579Z",
     "iopub.status.idle": "2024-03-31T18:35:52.005663Z",
     "shell.execute_reply": "2024-03-31T18:35:52.004605Z"
    },
    "papermill": {
     "duration": 4.074314,
     "end_time": "2024-03-31T18:35:52.008445",
     "exception": false,
     "start_time": "2024-03-31T18:35:47.934131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-discussions\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "How to create a discussion topic?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "To create a discussion topic, you can follow these steps:\n",
       "\n",
       "1. Click on “Discussions”.\n",
       "2. In the “Topic” dropdown, select “Create”.\n",
       "3. In the pop-up window, enter a title for your discussion.\n",
       "4. Select the appropriate “Type” and \"Visibility\" for your discussion.\n",
       "5. Add tags to the end of the title to help others find the topic.\n",
       "6. Select the appropriate \"Tags\" for your discussion.\n",
       "7. Click \"Save\" to create the discussion.\n",
       "\n",
       "\n",
       "Note that you can edit your discussion title, type and tags at any time after it has been created."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category = \"discussions\"\n",
    "question = \"How to create a discussion topic?\"\n",
    "gemma_qa.query(category,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "258fa638",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T18:35:52.164043Z",
     "iopub.status.busy": "2024-03-31T18:35:52.163199Z",
     "iopub.status.idle": "2024-03-31T18:36:04.065743Z",
     "shell.execute_reply": "2024-03-31T18:36:04.064766Z"
    },
    "papermill": {
     "duration": 11.979898,
     "end_time": "2024-03-31T18:36:04.068143",
     "exception": false,
     "start_time": "2024-03-31T18:35:52.088245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-competitions\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a code competition?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Code competitions are a popular format for Kaggle's community to compete and collaborate. Code competitions can range from small, focused competitions with only a few participants to large, multi-week competitions with hundreds of participants.\n",
       "\n",
       "## Types of Code Competitions\n",
       "\n",
       "## Data Science\n",
       "\n",
       "Data science competitions on Kaggle are often the first to test the skills of newcomers. Data science competitions often require a deeper understanding of the data and its structure, as well as more advanced programming skills.\n",
       "\n",
       "## Machine Learning\n",
       "\n",
       "Machine learning competitions are often the most popular on Kaggle, as they offer a wide range of interesting and challenging problems to work on. These competitions often require a combination of both data and programming skills, as machine learning is an interdisciplinary field.\n",
       "\n",
       "## Vision (Image Classification)\n",
       "\n",
       "Image classification competitions are popular on Kaggle and are typically data science competitions. The data used for vision competitions often consists of images from various categories, such as cars, dogs, or faces. The goal of these competitions is to classify each image as belonging to one of those categories.\n",
       "\n",
       "## Regression\n",
       "\n",
       "Regression competitions are also popular on Kaggle. These competitions often involve predicting a quantitative value from a set of inputs, typically numerical variables such as time or money. Regression competitions can be used to make predictions about anything from stock prices to customer behavior.\n",
       "\n",
       "## Time Series\n",
       "\n",
       "Time series competitions are competitions that involve predicting time-series data. Time series can be thought of as data that changes over time. For example, the number of tweets per hour over the last week could be a time series. The goal of time series competitions is to predict the values of the time series for future time periods.\n",
       "\n",
       "## Text Classification\n",
       "\n",
       "Text classification competitions are competitions that involve classifying text into one of several categories. For example, you might be given a set of news articles and asked to classify each one as belonging to one of several categories, such as “politics” or “entertainment.” Text classification competitions are popular on Kaggle, as they offer a way to use machine learning to solve real-world problems related to natural language processing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category = \"competitions\"\n",
    "question = \"What is a code competition?\"\n",
    "gemma_qa.query(category,question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3a5b7",
   "metadata": {
    "papermill": {
     "duration": 0.069699,
     "end_time": "2024-03-31T18:36:04.209030",
     "exception": false,
     "start_time": "2024-03-31T18:36:04.139331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e91e57",
   "metadata": {
    "papermill": {
     "duration": 0.073038,
     "end_time": "2024-03-31T18:36:04.354533",
     "exception": false,
     "start_time": "2024-03-31T18:36:04.281495",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We demonstated how to fine-tune a Gemma model using LoRA. \n",
    "We also created a class to run queries to the Gemma model and tested it with some examples from the existing training data but also with some new, not seen questions."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7669720,
     "sourceId": 64148,
     "sourceType": "competition"
    },
    {
     "datasetId": 4484051,
     "sourceId": 7711309,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 5171,
     "sourceId": 11371,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 699.671021,
   "end_time": "2024-03-31T18:36:08.732964",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-31T18:24:29.061943",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "09bfb99b59284c588a373743cc4a874a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3135e2343c5541249618b7009b3f5664": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_34b42cc324e84b999321d4f7a79044a9",
       "placeholder": "​",
       "style": "IPY_MODEL_09bfb99b59284c588a373743cc4a874a",
       "value": "100%"
      }
     },
     "34b42cc324e84b999321d4f7a79044a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5bd3834b138148b78d80b51719c66087": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "63fa5ee234ee4f4aac1e99923302d63a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7adc897eac494a59821838a122caa76b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8ef1094cff7b48838ace93dbe3a77f53": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f8ae7ff10d214e64aa018669f9c9ba2a",
       "max": 60.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_63fa5ee234ee4f4aac1e99923302d63a",
       "value": 60.0
      }
     },
     "97e9a6496ae346fb844d04ccb2ece549": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b70f7e689a6d46ac9419fa4de63a4460": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3135e2343c5541249618b7009b3f5664",
        "IPY_MODEL_8ef1094cff7b48838ace93dbe3a77f53",
        "IPY_MODEL_d5b608766b224c7fa992d5cc9d517562"
       ],
       "layout": "IPY_MODEL_7adc897eac494a59821838a122caa76b"
      }
     },
     "d5b608766b224c7fa992d5cc9d517562": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5bd3834b138148b78d80b51719c66087",
       "placeholder": "​",
       "style": "IPY_MODEL_97e9a6496ae346fb844d04ccb2ece549",
       "value": " 60/60 [00:00&lt;00:00, 3162.17it/s]"
      }
     },
     "f8ae7ff10d214e64aa018669f9c9ba2a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
