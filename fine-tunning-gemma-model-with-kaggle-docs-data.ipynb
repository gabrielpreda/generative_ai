{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d77045",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.010799,
     "end_time": "2024-04-13T18:56:47.919043",
     "exception": false,
     "start_time": "2024-04-13T18:56:47.908244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center><h1>Fine-tunning Gemma model with Kaggle Docs data</h1></center>\n",
    "\n",
    "<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This notebook will demonstrate three things:\n",
    "\n",
    "1. How to fine-tune Gemma model using LoRA\n",
    "2. Creation of a specialised class to query about Kaggle features\n",
    "3. Some results of querying about Kaggle Docs\n",
    "\n",
    "This work is largely based on previous work. Here I list the sources:\n",
    "\n",
    "1. Gemma Model Card, Kaggle Models, https://www.kaggle.com/models/google/gemma\n",
    "2. Kaggle QA with Gemma - KerasNLP Starter, Kaggle Code, https://www.kaggle.com/code/awsaf49/kaggle-qa-with-gemma-kerasnlp-starter (Version 11)  \n",
    "3. Fine-tune Gemma models in Keras using LoRA, Kaggle Code, https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora (Version 1)  \n",
    "4. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, LoRA: Low-Rank Adaptation of Large Language Models, ArXiv, https://arxiv.org/pdf/2106.09685.pdf\n",
    "5. Abheesht Sharma, Matthew Watson, Parameter-efficient fine-tuning of GPT-2 with LoRA, https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/\n",
    "6. Keras 3 API documentation / KerasNLP / Models / Gemma, https://keras.io/api/keras_nlp/models/gemma/\n",
    "7. Kaggle Docs, Kaggle Dataset, https://www.kaggle.com/datasets/awsaf49/kaggle-docs  \n",
    "\n",
    "\n",
    "**Let's go**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e548bb35",
   "metadata": {
    "papermill": {
     "duration": 0.010278,
     "end_time": "2024-04-13T18:56:47.939869",
     "exception": false,
     "start_time": "2024-04-13T18:56:47.929591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is Gemma?\n",
    "\n",
    "\n",
    "Gemma is a collection of lightweight source generative AI models designed to be used mostly by developers and researchers. Created by Google DeepMind research lab that also developed Gemini, Gemma is available in several versions, with 2B and 7B parameters, as following:\n",
    "\n",
    "\n",
    "| Model                  | Parameters      | Tuned versions    | Description                                    | Recomemnded target platforms       |\n",
    "|------------------------|-----------------|-------------------|------------------------------------------------|------------------------------------|\n",
    "| `gemma_2b_en`          | 2.51B           | Pretrained        | 18-layer Gemma model (Gemma with 2B parameters)|Mobile devices and laptops          |\n",
    "| `gemma_instruct_2b_en` | 2.51B           | Instruction tuned | 18-layer Gemma model (Gemma with 2B parameters)| Mobile devices and laptops         | \n",
    "| `gemma_7b_en`          | 8.54B           | Pretrained        | 28-layer Gemma model (Gemma with 7B parameters)| Desktop computers and small servers|\n",
    "| `gemma_instruct_7b_en` | 8.54B           | Instruction tuned | 28-layer Gemma model (Gemma with 7B parameters)| Desktop computers and small servers|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ba6d35",
   "metadata": {
    "papermill": {
     "duration": 0.010149,
     "end_time": "2024-04-13T18:56:47.960346",
     "exception": false,
     "start_time": "2024-04-13T18:56:47.950197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is LoRA?  \n",
    "\n",
    "LoRA stands for Low-Rank Adaptation. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to LoRA paper, this number decreases 10,000 times, and the computational resources size decreases 3 times. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f431f",
   "metadata": {
    "papermill": {
     "duration": 0.010043,
     "end_time": "2024-04-13T18:56:47.983033",
     "exception": false,
     "start_time": "2024-04-13T18:56:47.972990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How we proceed?\n",
    "\n",
    "For fine-tunning with LoRA, we will follow the steps:\n",
    "\n",
    "1. Install prerequisites\n",
    "2. Load and process the data for fine-tuning\n",
    "3. Initialize the code for Gemma causal language model (Gemma Causal LM)\n",
    "4. Perform fine-tuning\n",
    "5. Test the fine-tunned model with questions from the data used for fine-tuning and with aditional questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529ed095",
   "metadata": {
    "papermill": {
     "duration": 0.010043,
     "end_time": "2024-04-13T18:56:48.003314",
     "exception": false,
     "start_time": "2024-04-13T18:56:47.993271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prerequisites\n",
    "\n",
    "\n",
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "695c64b2",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-04-13T18:56:48.025763Z",
     "iopub.status.busy": "2024-04-13T18:56:48.025045Z",
     "iopub.status.idle": "2024-04-13T18:57:19.822493Z",
     "shell.execute_reply": "2024-04-13T18:57:19.821321Z"
    },
    "papermill": {
     "duration": 31.811448,
     "end_time": "2024-04-13T18:57:19.825183",
     "exception": false,
     "start_time": "2024-04-13T18:56:48.013735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras>=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f07a6f",
   "metadata": {
    "papermill": {
     "duration": 0.010724,
     "end_time": "2024-04-13T18:57:19.847148",
     "exception": false,
     "start_time": "2024-04-13T18:57:19.836424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee9b71ee",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-04-13T18:57:19.870611Z",
     "iopub.status.busy": "2024-04-13T18:57:19.870300Z",
     "iopub.status.idle": "2024-04-13T18:57:34.551379Z",
     "shell.execute_reply": "2024-04-13T18:57:34.550575Z"
    },
    "papermill": {
     "duration": 14.695612,
     "end_time": "2024-04-13T18:57:34.553654",
     "exception": false,
     "start_time": "2024-04-13T18:57:19.858042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 18:57:24.394399: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-13 18:57:24.394500: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-13 18:57:24.534501: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"\"\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas() # progress bar for pandas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c875a02",
   "metadata": {
    "papermill": {
     "duration": 0.010517,
     "end_time": "2024-04-13T18:57:34.575254",
     "exception": false,
     "start_time": "2024-04-13T18:57:34.564737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b33ca1be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T18:57:34.597650Z",
     "iopub.status.busy": "2024-04-13T18:57:34.597186Z",
     "iopub.status.idle": "2024-04-13T18:57:34.601774Z",
     "shell.execute_reply": "2024-04-13T18:57:34.600949Z"
    },
    "papermill": {
     "duration": 0.017731,
     "end_time": "2024-04-13T18:57:34.603583",
     "exception": false,
     "start_time": "2024-04-13T18:57:34.585852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    seed = 42\n",
    "    dataset_path = \"/kaggle/input/kaggle-docs/questions_answers\"\n",
    "    preset = \"gemma_2b_en\" # name of pretrained Gemma\n",
    "    sequence_length = 512 # max size of input sequence for training\n",
    "    batch_size = 1 # size of the input batch in training, x 2 as two GPUs\n",
    "    epochs = 15 # number of epochs to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df82b8f",
   "metadata": {
    "papermill": {
     "duration": 0.010307,
     "end_time": "2024-04-13T18:57:34.624482",
     "exception": false,
     "start_time": "2024-04-13T18:57:34.614175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Set a random seed for results reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d1f4e42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T18:57:34.647882Z",
     "iopub.status.busy": "2024-04-13T18:57:34.647291Z",
     "iopub.status.idle": "2024-04-13T18:57:34.651466Z",
     "shell.execute_reply": "2024-04-13T18:57:34.650636Z"
    },
    "papermill": {
     "duration": 0.017982,
     "end_time": "2024-04-13T18:57:34.653313",
     "exception": false,
     "start_time": "2024-04-13T18:57:34.635331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(Config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150965ed",
   "metadata": {
    "papermill": {
     "duration": 0.010373,
     "end_time": "2024-04-13T18:57:34.674475",
     "exception": false,
     "start_time": "2024-04-13T18:57:34.664102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "125f372a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T18:57:34.697032Z",
     "iopub.status.busy": "2024-04-13T18:57:34.696423Z",
     "iopub.status.idle": "2024-04-13T18:57:34.723735Z",
     "shell.execute_reply": "2024-04-13T18:57:34.722846Z"
    },
    "papermill": {
     "duration": 0.04061,
     "end_time": "2024-04-13T18:57:34.725731",
     "exception": false,
     "start_time": "2024-04-13T18:57:34.685121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the different types of competitions a...</td>\n",
       "      <td># Types of Competitions\\n\\nKaggle Competitions...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the different competition formats on ...</td>\n",
       "      <td>There are handful of different formats competi...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to join a competition?</td>\n",
       "      <td>Before you start, navigate to the [Competition...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to form, manage, and disband teams in a co...</td>\n",
       "      <td>Everyone that competes in a Competition does s...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I make a submission in a competition?</td>\n",
       "      <td>You will need to submit your model predictions...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  What are the different types of competitions a...   \n",
       "1  What are the different competition formats on ...   \n",
       "2                         How to join a competition?   \n",
       "3  How to form, manage, and disband teams in a co...   \n",
       "4       How do I make a submission in a competition?   \n",
       "\n",
       "                                              Answer     Category  \n",
       "0  # Types of Competitions\\n\\nKaggle Competitions...  competition  \n",
       "1  There are handful of different formats competi...  competition  \n",
       "2  Before you start, navigate to the [Competition...  competition  \n",
       "3  Everyone that competes in a Competition does s...  competition  \n",
       "4  You will need to submit your model predictions...  competition  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{Config.dataset_path}/data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93279b4",
   "metadata": {
    "papermill": {
     "duration": 0.010552,
     "end_time": "2024-04-13T18:57:34.747204",
     "exception": false,
     "start_time": "2024-04-13T18:57:34.736652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's check the total number of rows in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba418e10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T18:57:34.770139Z",
     "iopub.status.busy": "2024-04-13T18:57:34.769669Z",
     "iopub.status.idle": "2024-04-13T18:57:34.774997Z",
     "shell.execute_reply": "2024-04-13T18:57:34.774139Z"
    },
    "papermill": {
     "duration": 0.018748,
     "end_time": "2024-04-13T18:57:34.776906",
     "exception": false,
     "start_time": "2024-04-13T18:57:34.758158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad423f5b",
   "metadata": {
    "papermill": {
     "duration": 0.010746,
     "end_time": "2024-04-13T18:57:34.798744",
     "exception": false,
     "start_time": "2024-04-13T18:57:34.787998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For easiness, we will create the following template for QA: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab4b700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T18:57:34.821925Z",
     "iopub.status.busy": "2024-04-13T18:57:34.821488Z",
     "iopub.status.idle": "2024-04-13T18:57:34.830773Z",
     "shell.execute_reply": "2024-04-13T18:57:34.829954Z"
    },
    "papermill": {
     "duration": 0.022907,
     "end_time": "2024-04-13T18:57:34.832761",
     "exception": false,
     "start_time": "2024-04-13T18:57:34.809854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\\n\\nCategory:\\nkaggle-{Category}\\n\\nQuestion:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\n",
    "df[\"prompt\"] = df.apply(lambda row: template.format(Category=row.Category,\n",
    "                                                             Question=row.Question,\n",
    "                                                             Answer=row.Answer), axis=1)\n",
    "data = df.prompt.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a06dad1",
   "metadata": {
    "papermill": {
     "duration": 0.01076,
     "end_time": "2024-04-13T18:57:34.854801",
     "exception": false,
     "start_time": "2024-04-13T18:57:34.844041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Template utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "977123da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T18:57:34.878161Z",
     "iopub.status.busy": "2024-04-13T18:57:34.877613Z",
     "iopub.status.idle": "2024-04-13T18:57:34.882018Z",
     "shell.execute_reply": "2024-04-13T18:57:34.881238Z"
    },
    "papermill": {
     "duration": 0.018097,
     "end_time": "2024-04-13T18:57:34.883989",
     "exception": false,
     "start_time": "2024-04-13T18:57:34.865892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Category\", \"Question\", \"Answer\"], [\"blue\", \"red\", \"green\"]):\n",
    "        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b1f88b",
   "metadata": {
    "papermill": {
     "duration": 0.010819,
     "end_time": "2024-04-13T18:57:34.906162",
     "exception": false,
     "start_time": "2024-04-13T18:57:34.895343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Specialized class to query Gemma\n",
    "\n",
    "\n",
    "We define a specialized class to query Gemma. But first, we need to initialize an object of GemmaCausalLM class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d56f60d",
   "metadata": {
    "papermill": {
     "duration": 0.010691,
     "end_time": "2024-04-13T18:57:34.927828",
     "exception": false,
     "start_time": "2024-04-13T18:57:34.917137",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initialize the code for Gemma Causal LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b05d06e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T18:57:34.951323Z",
     "iopub.status.busy": "2024-04-13T18:57:34.950742Z",
     "iopub.status.idle": "2024-04-13T18:58:29.204558Z",
     "shell.execute_reply": "2024-04-13T18:58:29.203676Z"
    },
    "papermill": {
     "duration": 54.267617,
     "end_time": "2024-04-13T18:58:29.206521",
     "exception": false,
     "start_time": "2024-04-13T18:57:34.938904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_causal_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
    "gemma_causal_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c8030c",
   "metadata": {
    "papermill": {
     "duration": 0.012722,
     "end_time": "2024-04-13T18:58:29.232183",
     "exception": false,
     "start_time": "2024-04-13T18:58:29.219461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define the specialized class\n",
    "\n",
    "Here we define the special class `GemmaQA`. \n",
    "in the `__init__` we pass the `GemmaCausalLM` object created before.\n",
    "The `query` member function uses `GemmaCausalLM` member function `generate` to generate the answer, based on a prompt that includes the category and the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f5382ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T18:58:29.259601Z",
     "iopub.status.busy": "2024-04-13T18:58:29.259295Z",
     "iopub.status.idle": "2024-04-13T18:58:29.265295Z",
     "shell.execute_reply": "2024-04-13T18:58:29.264468Z"
    },
    "papermill": {
     "duration": 0.022309,
     "end_time": "2024-04-13T18:58:29.267216",
     "exception": false,
     "start_time": "2024-04-13T18:58:29.244907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GemmaQA:\n",
    "    def __init__(self, max_length=512):\n",
    "        self.max_length = max_length\n",
    "        self.prompt = template\n",
    "        self.gemma_causal_lm = gemma_causal_lm\n",
    "        \n",
    "    def query(self, category, question):\n",
    "        response = self.gemma_causal_lm.generate(\n",
    "            self.prompt.format(\n",
    "                Category=category,\n",
    "                Question=question,\n",
    "                Answer=\"\"), \n",
    "            max_length=self.max_length)\n",
    "        display(Markdown(colorize_text(response)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5fd09",
   "metadata": {
    "papermill": {
     "duration": 0.012605,
     "end_time": "2024-04-13T18:58:29.292681",
     "exception": false,
     "start_time": "2024-04-13T18:58:29.280076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Gemma preprocessor\n",
    "\n",
    "\n",
    "This preprocessing layer will take in batches of strings, and return outputs in a ```(x, y, sample_weight)``` format, where the y label is the next token id in the x sequence.\n",
    "\n",
    "From the code below, we can see that, after the preprocessor, the data shape is ```(num_samples, sequence_length)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "184dbf55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T18:58:29.319374Z",
     "iopub.status.busy": "2024-04-13T18:58:29.319004Z",
     "iopub.status.idle": "2024-04-13T18:58:29.660406Z",
     "shell.execute_reply": "2024-04-13T18:58:29.659461Z"
    },
    "papermill": {
     "duration": 0.357384,
     "end_time": "2024-04-13T18:58:29.662718",
     "exception": false,
     "start_time": "2024-04-13T18:58:29.305334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y, sample_weight = gemma_causal_lm.preprocessor(data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c94e18d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T18:58:29.690569Z",
     "iopub.status.busy": "2024-04-13T18:58:29.689775Z",
     "iopub.status.idle": "2024-04-13T18:58:29.696265Z",
     "shell.execute_reply": "2024-04-13T18:58:29.695376Z"
    },
    "papermill": {
     "duration": 0.022322,
     "end_time": "2024-04-13T18:58:29.698192",
     "exception": false,
     "start_time": "2024-04-13T18:58:29.675870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': Array([[   2,  109, 8606, ...,    0,    0,    0],\n",
      "       [   2,  109, 8606, ...,    0,    0,    0]], dtype=int32), 'padding_mask': Array([[ True,  True,  True, ..., False, False, False],\n",
      "       [ True,  True,  True, ..., False, False, False]], dtype=bool)} [[   109   8606 235292 ...      0      0      0]\n",
      " [   109   8606 235292 ...      0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc3ba3b",
   "metadata": {
    "papermill": {
     "duration": 0.0127,
     "end_time": "2024-04-13T18:58:29.725158",
     "exception": false,
     "start_time": "2024-04-13T18:58:29.712458",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Perform fine-tuning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68099b5f",
   "metadata": {
    "papermill": {
     "duration": 0.01308,
     "end_time": "2024-04-13T18:58:29.795463",
     "exception": false,
     "start_time": "2024-04-13T18:58:29.782383",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Enable LoRA for the model\n",
    "\n",
    "LoRA rank is setting the number of trainable parameters. A larger rank will result in a larger number of parameters to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2c80f35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T18:58:29.823824Z",
     "iopub.status.busy": "2024-04-13T18:58:29.822947Z",
     "iopub.status.idle": "2024-04-13T18:58:30.284484Z",
     "shell.execute_reply": "2024-04-13T18:58:30.283647Z"
    },
    "papermill": {
     "duration": 0.478073,
     "end_time": "2024-04-13T18:58:30.286384",
     "exception": false,
     "start_time": "2024-04-13T18:58:29.808311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to 4.\n",
    "gemma_causal_lm.backbone.enable_lora(rank=4)\n",
    "gemma_causal_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d77a4b",
   "metadata": {
    "papermill": {
     "duration": 0.014682,
     "end_time": "2024-04-13T18:58:30.316951",
     "exception": false,
     "start_time": "2024-04-13T18:58:30.302269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We see that only a small part of the parameters are trainable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeefc3d",
   "metadata": {
    "papermill": {
     "duration": 0.015046,
     "end_time": "2024-04-13T18:58:30.347782",
     "exception": false,
     "start_time": "2024-04-13T18:58:30.332736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run the training sequence\n",
    "\n",
    "We set the `sequence_length` for the `GemmaCausalLM` (from configuration, will be 512).\n",
    "We compile the model, with the loss, optimizer and metric.\n",
    "For the metric, it is used `SparseCategoricalAccuracy`. This metric calculates how often predictions match integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "481397ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T18:58:30.378775Z",
     "iopub.status.busy": "2024-04-13T18:58:30.378460Z",
     "iopub.status.idle": "2024-04-13T19:10:30.589682Z",
     "shell.execute_reply": "2024-04-13T19:10:30.588704Z"
    },
    "papermill": {
     "duration": 720.228711,
     "end_time": "2024-04-13T19:10:30.591565",
     "exception": false,
     "start_time": "2024-04-13T18:58:30.362854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 735ms/step - loss: 1.7209 - sparse_categorical_accuracy: 0.5241\n",
      "Epoch 2/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.6869 - sparse_categorical_accuracy: 0.5313\n",
      "Epoch 3/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.6175 - sparse_categorical_accuracy: 0.5417\n",
      "Epoch 4/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.5770 - sparse_categorical_accuracy: 0.5509\n",
      "Epoch 5/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - loss: 1.5537 - sparse_categorical_accuracy: 0.5552\n",
      "Epoch 6/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.5304 - sparse_categorical_accuracy: 0.5568\n",
      "Epoch 7/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.5028 - sparse_categorical_accuracy: 0.5630\n",
      "Epoch 8/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.4733 - sparse_categorical_accuracy: 0.5682\n",
      "Epoch 9/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.4444 - sparse_categorical_accuracy: 0.5745\n",
      "Epoch 10/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.4025 - sparse_categorical_accuracy: 0.5873\n",
      "Epoch 11/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.3607 - sparse_categorical_accuracy: 0.5961\n",
      "Epoch 12/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.3163 - sparse_categorical_accuracy: 0.6079\n",
      "Epoch 13/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.2684 - sparse_categorical_accuracy: 0.6198\n",
      "Epoch 14/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.2149 - sparse_categorical_accuracy: 0.6325\n",
      "Epoch 15/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.1585 - sparse_categorical_accuracy: 0.6450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x78f0e04f7880>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_causal_lm.preprocessor.sequence_length = Config.sequence_length \n",
    "\n",
    "# Compile the model with loss, optimizer, and metric\n",
    "gemma_causal_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=8e-5),\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "gemma_causal_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac63f6e",
   "metadata": {
    "papermill": {
     "duration": 0.090735,
     "end_time": "2024-04-13T19:10:30.774872",
     "exception": false,
     "start_time": "2024-04-13T19:10:30.684137",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test the fine-tuned model\n",
    "\n",
    "We instantiate an object of class GemmaQA. Because `gemma_causal_lm` was fine-tuned using LoRA, `gemma_qa` defined here will use the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73112611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:10:30.954044Z",
     "iopub.status.busy": "2024-04-13T19:10:30.953270Z",
     "iopub.status.idle": "2024-04-13T19:10:30.957778Z",
     "shell.execute_reply": "2024-04-13T19:10:30.956781Z"
    },
    "papermill": {
     "duration": 0.097451,
     "end_time": "2024-04-13T19:10:30.959994",
     "exception": false,
     "start_time": "2024-04-13T19:10:30.862543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_qa = GemmaQA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0e56b2",
   "metadata": {
    "papermill": {
     "duration": 0.086082,
     "end_time": "2024-04-13T19:10:31.135530",
     "exception": false,
     "start_time": "2024-04-13T19:10:31.049448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For start, we are testing the model with some of the data from the training set itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52000aff",
   "metadata": {
    "papermill": {
     "duration": 0.086618,
     "end_time": "2024-04-13T19:10:31.310862",
     "exception": false,
     "start_time": "2024-04-13T19:10:31.224244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0034a092",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:10:31.488784Z",
     "iopub.status.busy": "2024-04-13T19:10:31.487841Z",
     "iopub.status.idle": "2024-04-13T19:10:50.950828Z",
     "shell.execute_reply": "2024-04-13T19:10:50.949922Z"
    },
    "papermill": {
     "duration": 19.554467,
     "end_time": "2024-04-13T19:10:50.953772",
     "exception": false,
     "start_time": "2024-04-13T19:10:31.399305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-competition\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What are the different types of competitions available on Kaggle?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Types of Competitions\n",
       "\n",
       "There are two main types of competitions on Kaggle:\n",
       "\n",
       "- **Data science challenges**: These are typically data science challenges where the goal is to build a model that outperforms a provided baseline. Examples include building a model to predict the likelihood of a fraudulent transaction or building a model to predict the likelihood of a heart attack.\n",
       "\n",
       "- **Classification challenges**: These are typically classification challenges where the goal is to build a model that outperforms a provided baseline. Examples include building a model to classify emails as spam or ham or building a model to classify images as cats or dogs.\n",
       "\n",
       "## How Are the Kaggle Competitions Ranked?\n",
       "\n",
       "The rankings on Kaggle are based on a combination of two metrics:\n",
       "\n",
       "- **Submissions**: The total number of submissions made by a user.\n",
       "\n",
       "- **Leaderboard Rank**: The rank of a user on the leaderboard of a given competition.\n",
       "\n",
       "The leaderboard rank is calculated by taking the median rank of the top 100 users on the leaderboard. The median rank is calculated by first sorting the leaderboard by leaderboard rank and then taking the middle value of the leaderboard.\n",
       "\n",
       "The combination of these two metrics is designed to encourage users to make high-quality submissions and to reward users who are consistently at the top of the leaderboard."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Let's check what is the correct answer: \n",
      " # Types of Competitions\n",
      "\n",
      "Kaggle Competitions are designed to provide challenges for competitors at all different stages of their machine learning careers. As a result, they are very diverse, with a range of broad types.\n",
      "\n",
      "## Featured\n",
      "\n",
      "Featured competitions are the types of competitions that Kaggle is probably best known for. These are full-scale machine learning challenges which pose difficult, generally commercially-purposed prediction problems. For example, past featured competitions have included:\n",
      "\n",
      "- [Allstate Claim Prediction Challenge](https://www.kaggle.com/c/allstate-purchase-prediction-challenge) - Use customers’ shopping history to predict which insurance policy they purchase\n",
      "- [Jigsaw Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) - Predict the existence and type of toxic comments on Wikipedia\n",
      "- [Zillow Prize](https://www.kaggle.com/c/zillow-prize-1) - Build a machine learning algorithm that can challenge Zestimates, the Zillow real estate price estimation algorithm\n",
      "\n",
      "Featured competitions attract some of the most formidable experts, and offer prize pools going as high as a million dollars. However, they remain accessible to anyone and everyone. Whether you’re an expert in the field or a complete novice, featured competitions are a valuable opportunity to learn skills and techniques from the very best in the field.\n",
      "\n",
      "## Research\n",
      "\n",
      "Research competitions are another common type of competition on Kaggle. Research competitions feature problems which are more experimental than featured competition problems. For example, some past research competitions have included:\n",
      "\n",
      "- [Google Landmark Retrieval Challenge](https://www.kaggle.com/c/landmark-retrieval-challenge) - Given an image, can you find all the same landmarks in a dataset?\n",
      "- [Right Whale Recognition](https://www.kaggle.com/c/noaa-right-whale-recognition) - Identify endangered right whales in aerial photographs\n",
      "- [Large Scale Hierarchical Text Classification](https://www.kaggle.com/c/lshtc) - Classify Wikipedia documents into one of ~300,000 categories\n",
      "\n",
      "Research competitions do not usually offer prizes or points due to their experimental nature. But they offer an opportunity to work on problems which may not have a clean or easy solution and which are integral to a specific domain or area in a slightly less competitive environment.\n",
      "\n",
      "## Getting Started\n",
      "\n",
      "Getting Started competitions are the easiest, most approachable competitions on Kaggle. These are semi-permanent competitions that are meant to be used by new users just getting their foot in the door in the field of machine learning. They offer no prizes or points. Because of their long-running nature, Getting Started competitions are perhaps the most heavily tutorialized problems in machine learning - just what a newcomer needs to get started!\n",
      "\n",
      "- [Digit Recognizer](https://www.kaggle.com/c/digit-recognizer)\n",
      "- [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic) - Predict survival on the Titanic\n",
      "- [Housing Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)\n",
      "\n",
      "Getting Started competitions have two-month rolling leaderboards. Once a submission is more than two months old, it is automatically invalidated and no longer counts towards the leaderboard. Similarly, your team will drop from the leaderboard if all its submissions are older than two months. This gives new Kagglers the opportunity to see how their scores stack up against a cohort of competitors, rather than many tens of thousands of users. If your team is removed from a Getting Started competition due to the rolling expiry and wishes to rejoin, creating a new submission will cause it to show again on the leaderboard.\n",
      "\n",
      "Additionally, the Kaggle [Learn platform](https://www.kaggle.com/learn/overview) has several tracks for beginners interested in free hands-on data science learning from pandas to deep learning. Lessons within a track are separated into easily digestible chunks and contain Notebook exercises for you to practise building models and new techniques. You’ll learn all the skills you need to dive into Kaggle Competitions.\n",
      "\n",
      "## Playground\n",
      "\n",
      "Playground competitions are a “for fun” type of Kaggle competition that is one step above Getting Started in difficulty. These are competitions which often provide relatively simple machine learning tasks, and are similarly targeted at newcomers or Kagglers interested in practicing a new type of problem in a lower-stakes setting. Prizes range from kudos to small cash prizes. Some examples of Playground competitions are:\n",
      "\n",
      "- [Dogs versus Cats](https://www.kaggle.com/c/dogs-vs-cats) - Create an algorithm to distinguish dogs from cats\n",
      "- [Leaf Classification](https://www.kaggle.com/c/leaf-classification) - Can you see the random forest for the leaves?\n",
      "- [New York City Taxi Trip Duration](https://www.kaggle.com/c/nyc-taxi-trip-duration) - Share code and data to improve ride time predictions\n"
     ]
    }
   ],
   "source": [
    "row = df.iloc[0]\n",
    "gemma_qa.query(row.Category,row.Question)\n",
    "print(\"\\nLet's check what is the correct answer: \\n\", row.Answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f2e83d",
   "metadata": {
    "papermill": {
     "duration": 0.087197,
     "end_time": "2024-04-13T19:10:51.130502",
     "exception": false,
     "start_time": "2024-04-13T19:10:51.043305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe282383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:10:51.308264Z",
     "iopub.status.busy": "2024-04-13T19:10:51.307520Z",
     "iopub.status.idle": "2024-04-13T19:11:04.700139Z",
     "shell.execute_reply": "2024-04-13T19:11:04.699250Z"
    },
    "papermill": {
     "duration": 13.483848,
     "end_time": "2024-04-13T19:11:04.702706",
     "exception": false,
     "start_time": "2024-04-13T19:10:51.218858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-tpu\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "How to load and save model on TPU?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Overview\n",
       "\n",
       "TPU models are saved in SavedModel format. To load a model from SavedModel, you can use the `tpu.Model.from_saved_model` method.\n",
       "\n",
       "To save a model, you can use the `tpu.Model.save_main_session()` method.\n",
       "\n",
       "## Example\n",
       "\n",
       "```python\n",
       "# Load a model from SavedModel\n",
       "model = tpu.Model.from_saved_model(tpu.saved_model_fn)\n",
       "```\n",
       "\n",
       "## TPU Model Format\n",
       "\n",
       "TPU models are saved in SavedModel format. To load a model from SavedModel, you can use the tpu.Model.from_saved_model method.\n",
       "\n",
       "To save a model, you can use the tpu.Model.save_main_session() method.\n",
       "\n",
       "## TPU Model Format Overview\n",
       "\n",
       "The SavedModel format is a model archive that contains all the information needed to run a model on TPU. It is a zip archive with the following structure:\n",
       "\n",
       "- `model.pb` is the TensorFlow graph that describes the model.\n",
       "- `checkpoint.index` is a binary file that contains the metadata for the checkpoints in the model.\n",
       "- `config.json` is a JSON file that contains the configuration for the model.\n",
       "- `variables.add_file_io_ops.json` is a JSON file that contains the metadata for the file I/O ops in the model.\n",
       "- `variables.add_file_io_ops.pb` is a binary file that contains the code for the file I/O ops in the model.\n",
       "- `variables.add_op_for_each_checkpoint.index` is a binary file that contains the metadata for the checkpoints in the model.\n",
       "- `variables.add_op_for_each_checkpoint.json` is a JSON file that contains the metadata for the checkpoints in the model.\n",
       "- `variables.add_op_for_each_checkpoint.pb` is a binary file that contains the code for the checkpoints in the model.\n",
       "- `variables.add_op_for_each_checkpoint.summary.tsv` is a tab-separated-values file that contains the summary of the checkpoints in the model.\n",
       "- `variables.add_op_for_each_checkpoint.summary.tsv` is a tab-separated-values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Let's check what is the correct answer: \n",
      " When loading and saving TPU models from/to the local disk, the `experimental_io_device` option must be used. The technical explanation is at the end of this section. It can be omitted if writing to GCS because TPUs have direct access to GCS. This option does nothing on GPUs.\n",
      "\n",
      "## Saving a TPU model locally\n",
      "\n",
      "```python\n",
      "save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
      "model.save('./model', options=save_locally) # saving in Tensorflow's \"SavedModel\" format\n",
      "```\n",
      "\n",
      "## Loading a TPU model from local disk\n",
      "\n",
      "```python\n",
      "with strategy.scope():\n",
      "    load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
      "    model = tf.keras.models.load_model('./model', options=load_locally) # loading in Tensorflow's \"SavedModel\" format\n",
      "```\n",
      "\n",
      "## Writing checkpoints locally from a TPU model\n",
      "\n",
      "```python\n",
      "save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
      "checkpoints_cb = tf.keras.callbacks.ModelCheckpoint('./checkpoints', options=save_locally)\n",
      "model.fit(…, callbacks=[checkpoints_cb])\n",
      "```\n",
      "\n",
      "## Loading a model from Tensorflow Hub to TPU directly\n",
      "\n",
      "```python\n",
      "import tensorflow_hub as hub\n",
      "with strategy.scope():\n",
      "    load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
      "    pretrained_model = hub.KerasLayer('https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1', trainable=True, input_shape=[512,512,3], load_options=load_locally)\n",
      "```\n",
      "\n",
      "Example in this [EfficientNetB7 Notebook](https://www.kaggle.com/mgornergoogle/efficientnetb7-on-100-flowers#Model).\n",
      "\n",
      "## `experimental_io_device` explained\n",
      "\n",
      "To understand what the `experimental_io_device='/job:localhost'` flag does, some background info is needed first. TPU users will remember that in order to train a model on TPU, you have to instantiate the model in a `TPUStrategy` scope. Like this:\n",
      "\n",
      "```python\n",
      "# connect to a TPU and instantiate a distribution strategy\n",
      "tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\n",
      "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
      "tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
      "\n",
      "# instantiate the model in the strategy scope\n",
      "with tpu_strategy.scope():\n",
      "    model = tf.keras.Sequential( … )\n",
      "```\n",
      "\n",
      "This boilerplate code actually does 2 things:\n",
      "\n",
      "1. The strategy scope instructs Tensorflow to instantiate all the variables of the model in the memory of the TPU.\n",
      "2. The `TPUClusterResolver.connect()` call automatically enters the TPU device scope which instructs Tensorflow to run Tensorflow operations on the TPU.\n",
      "\n",
      "Now if you call `model.save('./model')` when you are connected to a TPU, Tensorflow will try to run the save operations on the TPU and since the TPU is a network-connected accelerator that has no access to your local disk, the operation will fail. Notice that saving to GCS will work though. The TPU does have access to GCS.\n",
      "\n",
      "If you want to save a TPU model to your local disk, you need to run the saving operation on your local machine and that is what the `experimental_io_device='/job:localhost'` flag does.\n"
     ]
    }
   ],
   "source": [
    "row = df.iloc[15]\n",
    "gemma_qa.query(row.Category,row.Question)\n",
    "print(\"\\nLet's check what is the correct answer: \\n\", row.Answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e45b8c7",
   "metadata": {
    "papermill": {
     "duration": 0.08796,
     "end_time": "2024-04-13T19:11:04.880149",
     "exception": false,
     "start_time": "2024-04-13T19:11:04.792189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e4f4028",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:11:05.059855Z",
     "iopub.status.busy": "2024-04-13T19:11:05.059075Z",
     "iopub.status.idle": "2024-04-13T19:11:18.344626Z",
     "shell.execute_reply": "2024-04-13T19:11:18.343665Z"
    },
    "papermill": {
     "duration": 13.379813,
     "end_time": "2024-04-13T19:11:18.347859",
     "exception": false,
     "start_time": "2024-04-13T19:11:04.968046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-noteboook\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What are the different types of notebooks available on Kaggle?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Notebooks\n",
       "\n",
       "Kaggle Notebooks are interactive markdown documents that allow users to run code and see the results. Notebooks are a key part of the Kaggle experience, and are used by the community to share data science workflows, discuss techniques, and explore new ideas.\n",
       "\n",
       "Kaggle Notebooks are hosted on nbviewer.com. You can access them from any web browser, or from the Kaggle mobile app.\n",
       "\n",
       "You can create a Notebook from the Kaggle website or from the command line using the kaggle notebook command.\n",
       "\n",
       "## Versions\n",
       "\n",
       "Each Notebook has multiple versions. Each version is a snapshot of the Notebook at a specific point in time. You can view and compare the versions of a Notebook to see how it has changed over time.\n",
       "\n",
       "You can access the version history of a Notebook from the Notebook editor.\n",
       "\n",
       "You can create a new version of a Notebook from the Notebook editor.\n",
       "\n",
       "You can delete a Notebook version from the Notebook editor.\n",
       "\n",
       "## Versions and Sharing\n",
       "\n",
       "Each Notebook version has a URL that looks like this:\n",
       "\n",
       "```\n",
       "https://www.kaggle.com/kylegordon/kaggle-noteboook-versions?version=1666666\n",
       "```\n",
       "\n",
       "You can share a Notebook version URL with others, and they will be prompted to view the Notebook as a guest.\n",
       "\n",
       "You can also create a Notebook version from a URL that someone else has shared with you.\n",
       "\n",
       "## Versions and Permissions\n",
       "\n",
       "Each Notebook version has a URL that looks like this:\n",
       "\n",
       "```\n",
       "https://www.kaggle.com/kylegordon/kaggle-noteboook-versions?version=1666666\n",
       "```\n",
       "\n",
       "You can view and compare the versions of a Notebook to see how it has changed over time.\n",
       "\n",
       "You can access the version history of a Notebook from the Notebook editor.\n",
       "\n",
       "You can create a new version of a Notebook from the Notebook editor.\n",
       "\n",
       "You can delete a Notebook version from the Notebook editor.\n",
       "\n",
       "You can also delete a Notebook version from the Versions tab of the Notebook editor.\n",
       "\n",
       "## Versions and Privacy\n",
       "\n",
       "Each Notebook version has a URL that looks like this:\n",
       "\n",
       "```\n",
       "https://www.kaggle.com/kylegordon/kaggle-noteboook-versions?version=1666666\n",
       "```\n",
       "\n",
       "You can share a Notebook version URL with others"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Let's check what is the correct answer: \n",
      " There are two different types of Notebooks on Kaggle.\n",
      "\n",
      "## Scripts\n",
      "\n",
      "The first type is a script. Scripts are files that execute everything as code sequentially. To start a script, click on “Create Notebook” and select “Script”. This will open the Scripts editing interface.\n",
      "\n",
      "From here you may select what type of script you would like to execute. You may write scripts in R or in Python.\n",
      "\n",
      "You can also execute selected lines of code by highlighting the code in the editor interface and clicking the “Run” button or hitting shift-enter. Any results will be printed to the console.\n",
      "\n",
      "“[Deep Learning Support [.9663]](https://www.kaggle.com/alexanderkireev/deep-learning-support-9663)” from the [TalkingData AdTracking Fraud Detection Challenge](https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection) is a great example of a Script-type.\n",
      "\n",
      "### RMarkdown Scripts\n",
      "\n",
      "RMarkdown scripts are a special type of script that executes not just R code, but RMarkdown code. This is a combination of R code and Markdown editing syntax that is preferred by most R authors in our community.\n",
      "\n",
      "The RMarkdown editor is the same one used for basic R or Python scripts, except that it uses the special RMarkdown syntax. To start editing an RMarkdown script, click on “Create Notebook”, navigate to the “Scripts” pane, and click on that. Then, in the language dropdown, click on “RMarkdown”.\n",
      "\n",
      "“[Head Start for Data Science](https://www.kaggle.com/hiteshp/head-start-for-data-scientist)” is a great example of a RMarkdown Script-type.\n",
      "\n",
      "## Notebooks\n",
      "\n",
      "The last type is Jupyter notebooks (usually just “notebooks”). Jupyter notebooks consist of a sequence of cells, where each cell is formatted in either Markdown (for writing text) or in a programming language of your choice (for writing code). To start a notebook, click on “Create Notebook”, and select “Notebook”. This will open the Notebooks editing interface.\n",
      "\n",
      "Notebooks may be written in either R or Python.\n",
      "\n",
      "“[Comprehensive data exploration with Python](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python)” is a great example of a Python Jupyter Notebook-type. “[How to Become a Data Scientist](https://www.kaggle.com/jackcook/how-to-become-a-data-scientist)” is a great example of an R Jupyter Notebook-type.\n"
     ]
    }
   ],
   "source": [
    "row = df.iloc[25]\n",
    "gemma_qa.query(row.Category,row.Question)\n",
    "print(\"\\nLet's check what is the correct answer: \\n\", row.Answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bfad3e",
   "metadata": {
    "papermill": {
     "duration": 0.089122,
     "end_time": "2024-04-13T19:11:18.533159",
     "exception": false,
     "start_time": "2024-04-13T19:11:18.444037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Not seen question(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb7e439a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:11:18.710846Z",
     "iopub.status.busy": "2024-04-13T19:11:18.710523Z",
     "iopub.status.idle": "2024-04-13T19:11:32.210704Z",
     "shell.execute_reply": "2024-04-13T19:11:32.209688Z"
    },
    "papermill": {
     "duration": 13.591621,
     "end_time": "2024-04-13T19:11:32.212851",
     "exception": false,
     "start_time": "2024-04-13T19:11:18.621230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-notebook\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "How to run a notebook?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Installation\n",
       "\n",
       "You can run notebooks on Kaggle without installing anything, but if you want to make use of the community-built add-ons marketplace or you have a team of users that you want to collaborate with on a notebook, then you should consider creating a notebook on Kaggle.\n",
       "\n",
       "To install Kaggle and Notebook features in your own conda environment, run:\n",
       "\n",
       "```\n",
       "kaggle datasets login-or-create <USERNAME>\n",
       "kaggle kernels --help\n",
       "```\n",
       "\n",
       "For a full list of command line arguments, run `kaggle kernels help <command` from the Kaggle command line.\n",
       "\n",
       "## Run Locally\n",
       "\n",
       "If you are running locally, you can run the notebook locally by running the following command from the notebook directory:\n",
       "\n",
       "```python\n",
       "!kaggle run --kernels /kaggle/notebooks/tutorial-saving-and-loading-models: Cells 1-X\n",
       "```\n",
       "\n",
       "The command will run the notebook from the first cell to the most recently executed cell. If you want to run a specific cell, you can specify the cell number after the `--kernels` flag.\n",
       "\n",
       "If you want to run the notebook from scratch, you can run the notebook locally by running the following command from the notebook directory:\n",
       "\n",
       "```python\n",
       "kaggle run --kernels <USERNAME>/notebooks/tutorial-saving-and-loading-models:latest\n",
       "```\n",
       "\n",
       "The command will run the notebook from the first cell to the most recently executed cell.\n",
       "\n",
       "If you want to run a specific cell, you can specify the cell number after the `--kernels` flag.\n",
       "\n",
       "If you want to run the notebook from scratch, you can run the notebook locally by running the following command from the notebook directory:\n",
       "\n",
       "```python\n",
       "kaggle run --kernels <USERNAME>/notebooks/tutorial-saving-and-loading-models:latest\n",
       "```\n",
       "\n",
       "## Run on Kaggle\n",
       "\n",
       "If you are running on Kaggle, you can run the notebook on Kaggle by clicking the “Run Notebook” button in the top right corner of the notebook editor.\n",
       "\n",
       "If you are running locally, you can upload the notebook to Kaggle and run the notebook locally by running the following command from the notebook directory:\n",
       "\n",
       "```python\n",
       "kaggle runnotebooks <USERNAME>/notebooks/tutorial-saving-and-loading-models:latest\n",
       "```\n",
       "\n",
       "If you are running on Kaggle, you can run the notebook"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category = \"notebook\"\n",
    "question = \"How to run a notebook?\"\n",
    "gemma_qa.query(category,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f17f0e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:11:32.397185Z",
     "iopub.status.busy": "2024-04-13T19:11:32.396841Z",
     "iopub.status.idle": "2024-04-13T19:11:45.868705Z",
     "shell.execute_reply": "2024-04-13T19:11:45.867789Z"
    },
    "papermill": {
     "duration": 13.56548,
     "end_time": "2024-04-13T19:11:45.870960",
     "exception": false,
     "start_time": "2024-04-13T19:11:32.305480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-discussions\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "How to create a discussion topic?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Creating a Discussion\n",
       "\n",
       "To create a new discussion topic, click on the \"Discussions\" tab and click on the \"New Discussion\" button.\n",
       "\n",
       "You can create a new discussion by either uploading a dataset or by pasting a URL. If you upload a dataset, the discussion will be associated with the dataset. If you paste a URL, the discussion will be associated with the notebook that URL points to.\n",
       "\n",
       "If you upload a dataset, the discussion will be associated with the dataset. If you paste a URL, the discussion will be associated with the notebook that URL points to.\n",
       "\n",
       "If you are creating a discussion associated with a dataset, the discussion will be public by default. If you are creating a discussion associated with a notebook, the discussion will be private by default.\n",
       "\n",
       "If you want to make the discussion public, click on the \"Privacy\" dropdown and select \"Public\".\n",
       "\n",
       "If you want to make the discussion private, click on the \"Privacy\" dropdown and select \"Private\".\n",
       "\n",
       "If you want to make the discussion a community discussion, click on the \"Privacy\" dropdown and select \"Community\".\n",
       "\n",
       "If you want to make the discussion a team discussion, click on the \"Team\" dropdown and select \"Team\".\n",
       "\n",
       "If you want to make the discussion a user-to-user conversation, click on the \"Conversation\" dropdown and select \"User to User\".\n",
       "\n",
       "If you want to make the discussion a notebook discussion, click on the \"Notebook\" dropdown and select \"Notebook\".\n",
       "\n",
       "If you want to create a discussion associated with a dataset, click on the \"Dataset\" dropdown and select the dataset you want to associate the discussion with.\n",
       "\n",
       "If you want to create a discussion associated with a notebook, click on the \"Notebook\" dropdown and select the notebook you want to associate the discussion with.\n",
       "\n",
       "If you are creating a discussion associated with a dataset, you will be presented with a list of all the datasets associated with the selected folder.\n",
       "\n",
       "If you are creating a discussion associated with a notebook, you will be presented with a list of all the notebooks associated with the selected folder.\n",
       "\n",
       "If you are creating a discussion associated with a dataset, you will be presented with a list of all the users who have access to the dataset.\n",
       "\n",
       "If you are creating a discussion associated with a notebook, you will be presented with a list of all the users who have access to the notebook.\n",
       "\n",
       "If you are creating"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category = \"discussions\"\n",
    "question = \"How to create a discussion topic?\"\n",
    "gemma_qa.query(category,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "547dc567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:11:46.057597Z",
     "iopub.status.busy": "2024-04-13T19:11:46.057249Z",
     "iopub.status.idle": "2024-04-13T19:11:59.530341Z",
     "shell.execute_reply": "2024-04-13T19:11:59.529199Z"
    },
    "papermill": {
     "duration": 13.568965,
     "end_time": "2024-04-13T19:11:59.532560",
     "exception": false,
     "start_time": "2024-04-13T19:11:45.963595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-competitions\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a code competition?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Code competitions are a type of competition where participants are given a dataset and a set of tools to build machine learning models. The goal is to build the best model for a given task. Code competitions are a popular format for Kaggle Datasets because they encourage participants to use the tools and techniques available on Kaggle.\n",
       "\n",
       "Code competitions are a great way to learn new tools and techniques, and to see how they can be applied to a real-world problem. They also provide a structured way to evaluate and improve your model, as you'll be competing against other participants who are using the same dataset and tools.\n",
       "\n",
       "There are many benefits to participating in code competitions. First, they're a great way to learn how to use a dataset and a task to drive model development. Second, they're a great way to improve your model's performance. Third, they're a great way to meet other data scientists and machine learning enthusiasts. Fourth, they're a lot of fun!\n",
       "\n",
       "## Types of Code Competitions\n",
       "\n",
       "There are two main types of code competitions on Kaggle: public and private.\n",
       "\n",
       "Public competitions are open to anyone and everyone. They're a great way to see what other people are working on in the community, and to learn from others' experiences. Public competitions are a great way to get exposure and recognition for your work.\n",
       "\n",
       "Private competitions are invitation-only and typically invitees are part of a larger community or organization. They're a great way to collaborate with others who share your interest in the same topic. Private competitions are a great way to get feedback on your work from experts in the field.\n",
       "\n",
       "## How to Participate in a Code Competition\n",
       "\n",
       "To participate in a code competition, you'll need to create a competition notebook. This notebook will contain all of the code you use to build your model. You can then submit your notebook to the competition.\n",
       "\n",
       "The first step is to find a competition that interests you. You can browse all of the current competitions on Kaggle Datasets by visiting the [code competitions page](https://www.kaggle.com/competitions/code/overview).\n",
       "\n",
       "Once you've found a competition you'd like to enter, click on the \"Participate\" button. This will take you to a page where you can sign up for the competition.\n",
       "\n",
       "The next step is to familiarize yourself with the dataset. This is the collection"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category = \"competitions\"\n",
    "question = \"What is a code competition?\"\n",
    "gemma_qa.query(category,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18d4bfa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:11:59.718124Z",
     "iopub.status.busy": "2024-04-13T19:11:59.717778Z",
     "iopub.status.idle": "2024-04-13T19:12:13.081121Z",
     "shell.execute_reply": "2024-04-13T19:12:13.080215Z"
    },
    "papermill": {
     "duration": 13.456993,
     "end_time": "2024-04-13T19:12:13.083311",
     "exception": false,
     "start_time": "2024-04-13T19:11:59.626318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-datasets\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What are the steps to create a Kaggle dataset?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Introduction\n",
       "\n",
       "Kaggle Datasets is a repository of public datasets that are created by members of the Kaggle community. Datasets are used by the Kaggle community for a variety of purposes, including training machine learning models, evaluating models, and sharing data.\n",
       "\n",
       "Kaggle Datasets is a great resource for anyone working with data.\n",
       "\n",
       "## Getting Started\n",
       "\n",
       "To get started creating a new dataset, navigate to the \"Create Dataset\" button in the left-hand navigation bar.\n",
       "\n",
       "You can also access the \"Create Dataset\" page from your profile by clicking on your username in the top-right corner of any page on Kaggle and selecting \"Create Dataset\" from the drop-down menu.\n",
       "\n",
       "There are two ways to create a new dataset:\n",
       "\n",
       "- Upload a file: If you have a large dataset that you want to upload, this is the way to go. Simply browse to the location of the file on your computer and select \"Upload\" to begin the upload process.\n",
       "- Generate a new dataset: If you don't have a large dataset to upload, this is the way to go. You can choose from a variety of template datasets to get started, or you can start from scratch by selecting \"Create Blank Dataset\" and editing the dataset settings to your liking.\n",
       "\n",
       "Once you have created your dataset, you will be taken to the \"Manage Dataset\" page. Here, you can edit the metadata (e.g. title, description, tags), permissions (who can view and edit the dataset), and the dataset files themselves.\n",
       "\n",
       "## Analytics\n",
       "\n",
       "Analytics are available to help you understand how your dataset is being used. You can see the number of downloads, clones, and edits your dataset has received over time, as well as the number of unique users who have accessed the dataset.\n",
       "\n",
       "To access analytics, navigate to the \"Manage Dataset\" page and click on the \"Analytics\" tab.\n",
       "\n",
       "## Sharing\n",
       "\n",
       "Kaggle Datasets is a community of data scientists, machine learning engineers, and data enthusiasts. By sharing your datasets with others in the community, you can help advance the state of the art in machine learning and data science.\n",
       "\n",
       "To share your dataset, navigate to the \"Manage Dataset\" page and click on the \"Share\" tab. Here, you can choose from a variety of sharing options, including public, private, and unlisted.\n",
       "\n",
       "## Search\n",
       "\n",
       "Kaggle Datasets is a"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category = \"datasets\"\n",
    "question = \"What are the steps to create a Kaggle dataset?\"\n",
    "gemma_qa.query(category,question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdabf61",
   "metadata": {
    "papermill": {
     "duration": 0.098326,
     "end_time": "2024-04-13T19:12:13.283972",
     "exception": false,
     "start_time": "2024-04-13T19:12:13.185646",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c5869",
   "metadata": {
    "papermill": {
     "duration": 0.088929,
     "end_time": "2024-04-13T19:12:13.466205",
     "exception": false,
     "start_time": "2024-04-13T19:12:13.377276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We demonstated how to fine-tune a Gemma model using LoRA.   \n",
    "We also created a class to run queries to the Gemma model and tested it with some examples from the existing training data but also with some new, not seen questions."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7669720,
     "sourceId": 64148,
     "sourceType": "competition"
    },
    {
     "datasetId": 4484051,
     "sourceId": 7711309,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 5171,
     "sourceId": 11371,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 931.602374,
   "end_time": "2024-04-13T19:12:16.732878",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-13T18:56:45.130504",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
