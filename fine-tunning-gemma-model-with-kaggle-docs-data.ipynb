{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a4e46f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.010258,
     "end_time": "2024-04-01T14:11:50.470263",
     "exception": false,
     "start_time": "2024-04-01T14:11:50.460005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center><h1>Fine-tunning Gemma model with Kaggle Docs data</h1></center>\n",
    "\n",
    "<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This notebook will demonstrate three things:\n",
    "\n",
    "1. How to fine-tune Gemma model using LoRA\n",
    "2. Creation of a specialised class to query about Kaggle features\n",
    "3. Some results of querying about Kaggle Docs\n",
    "\n",
    "This work is largely based on previous work. Here I list the sources:\n",
    "\n",
    "1. Gemma Model Card, Kaggle Models, https://www.kaggle.com/models/google/gemma\n",
    "2. Kaggle QA with Gemma - KerasNLP Starter, Kaggle Code, https://www.kaggle.com/code/awsaf49/kaggle-qa-with-gemma-kerasnlp-starter (Version 11)  \n",
    "3. Fine-tune Gemma models in Keras using LoRA, Kaggle Code, https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora (Version 1)  \n",
    "4. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, LoRA: Low-Rank Adaptation of Large Language Models, ArXiv, https://arxiv.org/pdf/2106.09685.pdf\n",
    "5. Abheesht Sharma, Matthew Watson, Parameter-efficient fine-tuning of GPT-2 with LoRA, https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/\n",
    "6. Keras 3 API documentation / KerasNLP / Models / Gemma, https://keras.io/api/keras_nlp/models/gemma/\n",
    "7. Kaggle Docs, Kaggle Dataset, https://www.kaggle.com/datasets/awsaf49/kaggle-docs  \n",
    "8. TPUs in Keras, Kaggle Docs, https://www.kaggle.com/docs/tpu  \n",
    "\n",
    "**Let's go**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaebb4a",
   "metadata": {
    "papermill": {
     "duration": 0.009826,
     "end_time": "2024-04-01T14:11:50.490191",
     "exception": false,
     "start_time": "2024-04-01T14:11:50.480365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is Gemma?\n",
    "\n",
    "\n",
    "Gemma is a collection of lightweight source generative AI models designed to be used mostly by developers and researchers. Created by Google DeepMind research lab that also developed Gemini, Gemma is available in several versions, with 2B and 7B parameters, as following:\n",
    "\n",
    "\n",
    "| Model                  | Parameters      | Tuned versions    | Description                                    | Recomemnded target platforms       |\n",
    "|------------------------|-----------------|-------------------|------------------------------------------------|------------------------------------|\n",
    "| `gemma_2b_en`          | 2.51B           | Pretrained        | 18-layer Gemma model (Gemma with 2B parameters)|Mobile devices and laptops          |\n",
    "| `gemma_instruct_2b_en` | 2.51B           | Instruction tuned | 18-layer Gemma model (Gemma with 2B parameters)| Mobile devices and laptops         | \n",
    "| `gemma_7b_en`          | 8.54B           | Pretrained        | 28-layer Gemma model (Gemma with 7B parameters)| Desktop computers and small servers|\n",
    "| `gemma_instruct_7b_en` | 8.54B           | Instruction tuned | 28-layer Gemma model (Gemma with 7B parameters)| Desktop computers and small servers|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc66fda",
   "metadata": {
    "papermill": {
     "duration": 0.009501,
     "end_time": "2024-04-01T14:11:50.509548",
     "exception": false,
     "start_time": "2024-04-01T14:11:50.500047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is LoRA?  \n",
    "\n",
    "LoRA stands for Low-Rank Adaptation. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to LoRA paper, this number decreases 10,000 times, and the computational resources size decreases 3 times. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb96b6",
   "metadata": {
    "papermill": {
     "duration": 0.009786,
     "end_time": "2024-04-01T14:11:50.529896",
     "exception": false,
     "start_time": "2024-04-01T14:11:50.520110",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How we proceed?\n",
    "\n",
    "For fine-tunning with LoRA, we will follow the steps:\n",
    "\n",
    "1. Install prerequisites\n",
    "2. Load and process the data for fine-tuning\n",
    "3. Initialize the code for Gemma causal language model (Gemma Causal LM)\n",
    "4. Perform fine-tuning\n",
    "5. Test the fine-tunned model with questions from the data used for fine-tuning and with aditional questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f671122b",
   "metadata": {
    "papermill": {
     "duration": 0.010712,
     "end_time": "2024-04-01T14:11:50.550400",
     "exception": false,
     "start_time": "2024-04-01T14:11:50.539688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prerequisites\n",
    "\n",
    "\n",
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb13e3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:11:50.571207Z",
     "iopub.status.busy": "2024-04-01T14:11:50.570868Z",
     "iopub.status.idle": "2024-04-01T14:12:21.126685Z",
     "shell.execute_reply": "2024-04-01T14:12:21.125599Z"
    },
    "papermill": {
     "duration": 30.568993,
     "end_time": "2024-04-01T14:12:21.129105",
     "exception": false,
     "start_time": "2024-04-01T14:11:50.560112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras>=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314abb4a",
   "metadata": {
    "papermill": {
     "duration": 0.009757,
     "end_time": "2024-04-01T14:12:21.149043",
     "exception": false,
     "start_time": "2024-04-01T14:12:21.139286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd92c68e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:12:21.170216Z",
     "iopub.status.busy": "2024-04-01T14:12:21.169923Z",
     "iopub.status.idle": "2024-04-01T14:12:35.821628Z",
     "shell.execute_reply": "2024-04-01T14:12:35.820778Z"
    },
    "papermill": {
     "duration": 14.665309,
     "end_time": "2024-04-01T14:12:35.824127",
     "exception": false,
     "start_time": "2024-04-01T14:12:21.158818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-01 14:12:25.559428: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-01 14:12:25.559528: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-01 14:12:25.697509: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"\"\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas() # progress bar for pandas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abff457e",
   "metadata": {
    "papermill": {
     "duration": 0.00982,
     "end_time": "2024-04-01T14:12:35.844446",
     "exception": false,
     "start_time": "2024-04-01T14:12:35.834626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30486f22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:12:35.865669Z",
     "iopub.status.busy": "2024-04-01T14:12:35.865164Z",
     "iopub.status.idle": "2024-04-01T14:12:35.870049Z",
     "shell.execute_reply": "2024-04-01T14:12:35.869177Z"
    },
    "papermill": {
     "duration": 0.017538,
     "end_time": "2024-04-01T14:12:35.871928",
     "exception": false,
     "start_time": "2024-04-01T14:12:35.854390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    seed = 42\n",
    "    dataset_path = \"/kaggle/input/kaggle-docs/questions_answers\"\n",
    "    preset = \"gemma_2b_en\" # name of pretrained Gemma\n",
    "    sequence_length = 512 # max size of input sequence for training\n",
    "    batch_size = 1 # size of the input batch in training, x 2 as two GPUs\n",
    "    epochs = 15 # number of epochs to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a787da",
   "metadata": {
    "papermill": {
     "duration": 0.009639,
     "end_time": "2024-04-01T14:12:35.891624",
     "exception": false,
     "start_time": "2024-04-01T14:12:35.881985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Initialize the TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2386d373",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:12:35.912735Z",
     "iopub.status.busy": "2024-04-01T14:12:35.912475Z",
     "iopub.status.idle": "2024-04-01T14:12:35.916998Z",
     "shell.execute_reply": "2024-04-01T14:12:35.916132Z"
    },
    "papermill": {
     "duration": 0.017194,
     "end_time": "2024-04-01T14:12:35.918792",
     "exception": false,
     "start_time": "2024-04-01T14:12:35.901598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(Config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dcb4df",
   "metadata": {
    "papermill": {
     "duration": 0.009643,
     "end_time": "2024-04-01T14:12:35.938368",
     "exception": false,
     "start_time": "2024-04-01T14:12:35.928725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34b1c0bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:12:35.959375Z",
     "iopub.status.busy": "2024-04-01T14:12:35.959109Z",
     "iopub.status.idle": "2024-04-01T14:12:35.996113Z",
     "shell.execute_reply": "2024-04-01T14:12:35.995270Z"
    },
    "papermill": {
     "duration": 0.049657,
     "end_time": "2024-04-01T14:12:35.998018",
     "exception": false,
     "start_time": "2024-04-01T14:12:35.948361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the different types of competitions a...</td>\n",
       "      <td># Types of Competitions\\n\\nKaggle Competitions...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the different competition formats on ...</td>\n",
       "      <td>There are handful of different formats competi...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to join a competition?</td>\n",
       "      <td>Before you start, navigate to the [Competition...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to form, manage, and disband teams in a co...</td>\n",
       "      <td>Everyone that competes in a Competition does s...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I make a submission in a competition?</td>\n",
       "      <td>You will need to submit your model predictions...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  What are the different types of competitions a...   \n",
       "1  What are the different competition formats on ...   \n",
       "2                         How to join a competition?   \n",
       "3  How to form, manage, and disband teams in a co...   \n",
       "4       How do I make a submission in a competition?   \n",
       "\n",
       "                                              Answer     Category  \n",
       "0  # Types of Competitions\\n\\nKaggle Competitions...  competition  \n",
       "1  There are handful of different formats competi...  competition  \n",
       "2  Before you start, navigate to the [Competition...  competition  \n",
       "3  Everyone that competes in a Competition does s...  competition  \n",
       "4  You will need to submit your model predictions...  competition  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{Config.dataset_path}/data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997e81bd",
   "metadata": {
    "papermill": {
     "duration": 0.01006,
     "end_time": "2024-04-01T14:12:36.018497",
     "exception": false,
     "start_time": "2024-04-01T14:12:36.008437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's check the total number of rows in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34b59395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:12:36.039986Z",
     "iopub.status.busy": "2024-04-01T14:12:36.039700Z",
     "iopub.status.idle": "2024-04-01T14:12:36.045118Z",
     "shell.execute_reply": "2024-04-01T14:12:36.044224Z"
    },
    "papermill": {
     "duration": 0.01825,
     "end_time": "2024-04-01T14:12:36.046914",
     "exception": false,
     "start_time": "2024-04-01T14:12:36.028664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0214a4",
   "metadata": {
    "papermill": {
     "duration": 0.009999,
     "end_time": "2024-04-01T14:12:36.067224",
     "exception": false,
     "start_time": "2024-04-01T14:12:36.057225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For easiness, we will create the following template for QA: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dccb3a8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:12:36.088904Z",
     "iopub.status.busy": "2024-04-01T14:12:36.088648Z",
     "iopub.status.idle": "2024-04-01T14:12:36.098343Z",
     "shell.execute_reply": "2024-04-01T14:12:36.097581Z"
    },
    "papermill": {
     "duration": 0.022656,
     "end_time": "2024-04-01T14:12:36.100218",
     "exception": false,
     "start_time": "2024-04-01T14:12:36.077562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\\n\\nCategory:\\nkaggle-{Category}\\n\\nQuestion:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\n",
    "df[\"prompt\"] = df.apply(lambda row: template.format(Category=row.Category,\n",
    "                                                             Question=row.Question,\n",
    "                                                             Answer=row.Answer), axis=1)\n",
    "data = df.prompt.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133e3a9e",
   "metadata": {
    "papermill": {
     "duration": 0.010122,
     "end_time": "2024-04-01T14:12:36.120752",
     "exception": false,
     "start_time": "2024-04-01T14:12:36.110630",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Template utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e6a8a4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:12:36.142630Z",
     "iopub.status.busy": "2024-04-01T14:12:36.142358Z",
     "iopub.status.idle": "2024-04-01T14:12:36.147083Z",
     "shell.execute_reply": "2024-04-01T14:12:36.146217Z"
    },
    "papermill": {
     "duration": 0.017804,
     "end_time": "2024-04-01T14:12:36.148978",
     "exception": false,
     "start_time": "2024-04-01T14:12:36.131174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Category\", \"Question\", \"Answer\"], [\"blue\", \"red\", \"green\"]):\n",
    "        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d64f4",
   "metadata": {
    "papermill": {
     "duration": 0.010007,
     "end_time": "2024-04-01T14:12:36.169329",
     "exception": false,
     "start_time": "2024-04-01T14:12:36.159322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Specialized class to query Gemma\n",
    "\n",
    "\n",
    "We define a specialized class to query Gemma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada33da",
   "metadata": {
    "papermill": {
     "duration": 0.010187,
     "end_time": "2024-04-01T14:12:36.189809",
     "exception": false,
     "start_time": "2024-04-01T14:12:36.179622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initialize the code for Gemma Causal LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "661e9366",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:12:36.211550Z",
     "iopub.status.busy": "2024-04-01T14:12:36.211266Z",
     "iopub.status.idle": "2024-04-01T14:13:30.488990Z",
     "shell.execute_reply": "2024-04-01T14:13:30.488114Z"
    },
    "papermill": {
     "duration": 54.290673,
     "end_time": "2024-04-01T14:13:30.490895",
     "exception": false,
     "start_time": "2024-04-01T14:12:36.200222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_causal_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
    "gemma_causal_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f63b18",
   "metadata": {
    "papermill": {
     "duration": 0.011886,
     "end_time": "2024-04-01T14:13:30.515012",
     "exception": false,
     "start_time": "2024-04-01T14:13:30.503126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define the specialized class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d382d0a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:13:30.541498Z",
     "iopub.status.busy": "2024-04-01T14:13:30.540881Z",
     "iopub.status.idle": "2024-04-01T14:13:30.547215Z",
     "shell.execute_reply": "2024-04-01T14:13:30.546172Z"
    },
    "papermill": {
     "duration": 0.022235,
     "end_time": "2024-04-01T14:13:30.549235",
     "exception": false,
     "start_time": "2024-04-01T14:13:30.527000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GemmaQA:\n",
    "    def __init__(self, max_length=512):\n",
    "        self.max_length = max_length\n",
    "        self.prompt = template\n",
    "        self.gemma_causal_lm = gemma_causal_lm\n",
    "        \n",
    "    def query(self, category, question):\n",
    "        response = self.gemma_causal_lm.generate(\n",
    "            self.prompt.format(\n",
    "                Category=category,\n",
    "                Question=question,\n",
    "                Answer=\"\"), \n",
    "            max_length=self.max_length)\n",
    "        display(Markdown(colorize_text(response)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d9a5d8",
   "metadata": {
    "papermill": {
     "duration": 0.011853,
     "end_time": "2024-04-01T14:13:30.573190",
     "exception": false,
     "start_time": "2024-04-01T14:13:30.561337",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Gemma preprocessor\n",
    "\n",
    "\n",
    "This preprocessing layer will take in batches of strings, and return outputs in a ```(x, y, sample_weight)``` format, where the y label is the next token id in the x sequence.\n",
    "\n",
    "From the code below, we can see that, after the preprocessor, the data shape is ```(num_samples, sequence_length)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51120314",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:13:30.598738Z",
     "iopub.status.busy": "2024-04-01T14:13:30.598467Z",
     "iopub.status.idle": "2024-04-01T14:13:30.937906Z",
     "shell.execute_reply": "2024-04-01T14:13:30.937058Z"
    },
    "papermill": {
     "duration": 0.355379,
     "end_time": "2024-04-01T14:13:30.940562",
     "exception": false,
     "start_time": "2024-04-01T14:13:30.585183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y, sample_weight = gemma_causal_lm.preprocessor(data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e226e79f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:13:30.967191Z",
     "iopub.status.busy": "2024-04-01T14:13:30.966883Z",
     "iopub.status.idle": "2024-04-01T14:13:30.973012Z",
     "shell.execute_reply": "2024-04-01T14:13:30.972104Z"
    },
    "papermill": {
     "duration": 0.021637,
     "end_time": "2024-04-01T14:13:30.975073",
     "exception": false,
     "start_time": "2024-04-01T14:13:30.953436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': Array([[   2,  109, 8606, ...,    0,    0,    0],\n",
      "       [   2,  109, 8606, ...,    0,    0,    0]], dtype=int32), 'padding_mask': Array([[ True,  True,  True, ..., False, False, False],\n",
      "       [ True,  True,  True, ..., False, False, False]], dtype=bool)} [[   109   8606 235292 ...      0      0      0]\n",
      " [   109   8606 235292 ...      0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328c61f",
   "metadata": {
    "papermill": {
     "duration": 0.012449,
     "end_time": "2024-04-01T14:13:31.000543",
     "exception": false,
     "start_time": "2024-04-01T14:13:30.988094",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Perform fine-tuning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744f7ecc",
   "metadata": {
    "papermill": {
     "duration": 0.016489,
     "end_time": "2024-04-01T14:13:31.074524",
     "exception": false,
     "start_time": "2024-04-01T14:13:31.058035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Enable LoRA for the model\n",
    "\n",
    "LoRA rank is setting the number of trainable parameters. A larger rank will result in a larger number of parameters to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff5a3fd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:13:31.110050Z",
     "iopub.status.busy": "2024-04-01T14:13:31.109316Z",
     "iopub.status.idle": "2024-04-01T14:13:31.614849Z",
     "shell.execute_reply": "2024-04-01T14:13:31.613908Z"
    },
    "papermill": {
     "duration": 0.525611,
     "end_time": "2024-04-01T14:13:31.617283",
     "exception": false,
     "start_time": "2024-04-01T14:13:31.091672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to 4.\n",
    "gemma_causal_lm.backbone.enable_lora(rank=4)\n",
    "gemma_causal_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b7b7b9",
   "metadata": {
    "papermill": {
     "duration": 0.013763,
     "end_time": "2024-04-01T14:13:31.648755",
     "exception": false,
     "start_time": "2024-04-01T14:13:31.634992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run the training sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79fb68c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:13:31.678162Z",
     "iopub.status.busy": "2024-04-01T14:13:31.677285Z",
     "iopub.status.idle": "2024-04-01T14:25:30.580915Z",
     "shell.execute_reply": "2024-04-01T14:25:30.580013Z"
    },
    "papermill": {
     "duration": 718.920199,
     "end_time": "2024-04-01T14:25:30.582751",
     "exception": false,
     "start_time": "2024-04-01T14:13:31.662552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 734ms/step - loss: 1.7209 - sparse_categorical_accuracy: 0.5241\n",
      "Epoch 2/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.6869 - sparse_categorical_accuracy: 0.5313\n",
      "Epoch 3/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.6175 - sparse_categorical_accuracy: 0.5417\n",
      "Epoch 4/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.5770 - sparse_categorical_accuracy: 0.5509\n",
      "Epoch 5/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.5537 - sparse_categorical_accuracy: 0.5552\n",
      "Epoch 6/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.5304 - sparse_categorical_accuracy: 0.5568\n",
      "Epoch 7/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.5028 - sparse_categorical_accuracy: 0.5630\n",
      "Epoch 8/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.4733 - sparse_categorical_accuracy: 0.5682\n",
      "Epoch 9/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.4444 - sparse_categorical_accuracy: 0.5747\n",
      "Epoch 10/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.4025 - sparse_categorical_accuracy: 0.5873\n",
      "Epoch 11/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.3606 - sparse_categorical_accuracy: 0.5960\n",
      "Epoch 12/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - loss: 1.3162 - sparse_categorical_accuracy: 0.6079\n",
      "Epoch 13/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.2683 - sparse_categorical_accuracy: 0.6199\n",
      "Epoch 14/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 728ms/step - loss: 1.2149 - sparse_categorical_accuracy: 0.6325\n",
      "Epoch 15/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.1584 - sparse_categorical_accuracy: 0.6452\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7cc610176080>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_causal_lm.preprocessor.sequence_length = Config.sequence_length \n",
    "\n",
    "# Compile the model with loss, optimizer, and metric\n",
    "gemma_causal_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=8e-5),\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "gemma_causal_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47a8d0",
   "metadata": {
    "papermill": {
     "duration": 0.08592,
     "end_time": "2024-04-01T14:25:30.755535",
     "exception": false,
     "start_time": "2024-04-01T14:25:30.669615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66fe8020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:25:30.929773Z",
     "iopub.status.busy": "2024-04-01T14:25:30.929391Z",
     "iopub.status.idle": "2024-04-01T14:25:30.934904Z",
     "shell.execute_reply": "2024-04-01T14:25:30.934057Z"
    },
    "papermill": {
     "duration": 0.0953,
     "end_time": "2024-04-01T14:25:30.936919",
     "exception": false,
     "start_time": "2024-04-01T14:25:30.841619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_qa = GemmaQA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31caf749",
   "metadata": {
    "papermill": {
     "duration": 0.085497,
     "end_time": "2024-04-01T14:25:31.108505",
     "exception": false,
     "start_time": "2024-04-01T14:25:31.023008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a32325c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:25:31.282774Z",
     "iopub.status.busy": "2024-04-01T14:25:31.282079Z",
     "iopub.status.idle": "2024-04-01T14:25:49.181481Z",
     "shell.execute_reply": "2024-04-01T14:25:49.180561Z"
    },
    "papermill": {
     "duration": 17.988944,
     "end_time": "2024-04-01T14:25:49.183497",
     "exception": false,
     "start_time": "2024-04-01T14:25:31.194553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-competition\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What are the different types of competitions available on Kaggle?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Datasets\n",
       "\n",
       "The Dataset page describes what a dataset is (essentially an archive of files), how to use it (e.g., download, upload and share datasets), and what to do with it (e.g., create a new notebook, run a model on it, and evaluate your model).\n",
       "\n",
       "## Competitions\n",
       "\n",
       "The Competition page describes what a competition is (essentially a timed Kaggle Dataset) in terms of the rules of the competition and the data provided to the contestants.\n",
       "\n",
       "## Tracks\n",
       "\n",
       "If you’re competing in a multi-track competition, Tracks is the page to find your track(s). Tracks are the individual components of the competition.\n",
       "\n",
       "## Participants\n",
       "\n",
       "Participants is the page of your competition participants.\n",
       "\n",
       "## Results\n",
       "\n",
       "Results is the page of all the results for your competition.\n",
       "\n",
       "## Rules\n",
       "\n",
       "Finally, the Competition page has an “Rules as a Files” section. This is where competitions can publish their rules.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[0]\n",
    "gemma_qa.query(row.Category,row.Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c209adca",
   "metadata": {
    "papermill": {
     "duration": 0.086308,
     "end_time": "2024-04-01T14:25:49.356252",
     "exception": false,
     "start_time": "2024-04-01T14:25:49.269944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92373d36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:25:49.532129Z",
     "iopub.status.busy": "2024-04-01T14:25:49.531291Z",
     "iopub.status.idle": "2024-04-01T14:25:55.434219Z",
     "shell.execute_reply": "2024-04-01T14:25:55.433354Z"
    },
    "papermill": {
     "duration": 5.994514,
     "end_time": "2024-04-01T14:25:55.436321",
     "exception": false,
     "start_time": "2024-04-01T14:25:49.441807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-tpu\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "How to load and save model on TPU?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "You can load and save models on TPU devices by using TPUSharing.\n",
       "\n",
       "## Getting Started\n",
       "\n",
       "TPUSharing can be accessed via the [TPUSharing Console](https://www.kaggle.com/tpusher/tpu).\n",
       "\n",
       "Once you have created the model on TPU device you can download the model from the console. You can then upload the model to the Kaggle Model Hub or download it from the hub.\n",
       "\n",
       "## Saving a TPU Model\n",
       "\n",
       "To save a model to a Tensor Processing Unit (TPU) device, follow these steps.\n",
       "\n",
       "First, make sure you are connected to the internet and have a network connection that supports your network and model size.\n",
       "\n",
       "Next, navigate to the \"Upload Model\" tab in the left-hand menu. Select the model file to upload, wait for the upload to finish, and you are ready to run the model on TPU!\n",
       "\n",
       "\n",
       "## Loading a TPU Model\n",
       "\n",
       "To load a saved model from TPU, follow these steps."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[15]\n",
    "gemma_qa.query(row.Category,row.Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a299c945",
   "metadata": {
    "papermill": {
     "duration": 0.087736,
     "end_time": "2024-04-01T14:25:55.610994",
     "exception": false,
     "start_time": "2024-04-01T14:25:55.523258",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56827afc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:25:55.790476Z",
     "iopub.status.busy": "2024-04-01T14:25:55.790138Z",
     "iopub.status.idle": "2024-04-01T14:26:09.302354Z",
     "shell.execute_reply": "2024-04-01T14:26:09.301405Z"
    },
    "papermill": {
     "duration": 13.605518,
     "end_time": "2024-04-01T14:26:09.304615",
     "exception": false,
     "start_time": "2024-04-01T14:25:55.699097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-noteboook\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What are the different types of notebooks available on Kaggle?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## What are Notebooks?\n",
       "\n",
       "Kaggle notebooks are an integral aspect of the data science community on Kaggle. Notebooks are where you write and execute code to analyze data and create insights.\n",
       "\n",
       "## Notebook types\n",
       "\n",
       "The types of notebooks available on Kaggle are:\n",
       "\n",
       "- A standard notebook that you can share privately or publicly.\n",
       "- A notebook published to Kaggle. Public notebooks are accessible to anyone with the notebook URL; Kaggle notebooks are visible to all Kagglers. Kaggle notebooks are a great way to collaborate on projects or host your own data science challenge. You can learn more about using Kaggle notebooks in the rest of the notebook editor. Public Kaggle notebooks are visible to anyone with the notebook URL and can be forked. For more details see the \"Who Can See This Notebook?\" section below.\n",
       "- A private Kaggle notebook. Private Kaggle notebooks are accessible only to those with the invite-only URL. This can be useful if you'd like to share an early version of a notebook project with feedback or collaborators.\n",
       "\n",
       "## Notebook sharing\n",
       "\n",
       "There are three types of sharing:\n",
       "\n",
       "- A notebook is private if you share it with only your username.\n",
       "- A notebook is publicly available on Kaggle if you share it with your username (this is the default behavior). Public Kaggle notebooks are visible to anyone with the notebook URL and can be forked. For details and to set up access permissions, see “What are the different types of notebooks available on Kaggle?” below.\n",
       "- A notebook is a private notebook. You can share private notebooks with your username, with a public URL that is not shareable.\n",
       "\n",
       "## What are the different types of notebooks available on Kaggle?\n",
       "\n",
       "- **Public** notebooks can be accessed by everyone with the notebook's URL. This can be useful if you'd like to share an early version of a notebook project with feedback or collaborators.\n",
       "- **Private** notebooks can be accessed by the username the notebook was created with. You can share private notebooks with your username, with a URL that is not shareable. This is useful if you'd like to share an early version of a notebook project with only your username, or with a small group of people.\n",
       "- **Public Kaggle notebooks** can be accessed by everyone with Kaggle. This can be useful if you'd like to share an"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[25]\n",
    "gemma_qa.query(row.Category,row.Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d523f428",
   "metadata": {
    "papermill": {
     "duration": 0.086358,
     "end_time": "2024-04-01T14:26:09.479756",
     "exception": false,
     "start_time": "2024-04-01T14:26:09.393398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Not seen question(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78a756e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:26:09.656716Z",
     "iopub.status.busy": "2024-04-01T14:26:09.655849Z",
     "iopub.status.idle": "2024-04-01T14:26:16.874447Z",
     "shell.execute_reply": "2024-04-01T14:26:16.873444Z"
    },
    "papermill": {
     "duration": 7.310001,
     "end_time": "2024-04-01T14:26:16.876555",
     "exception": false,
     "start_time": "2024-04-01T14:26:09.566554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-notebook\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "How to run a notebook?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "When a notebook is created, it's publicly available through Kaggle's website. To access a notebook, navigate to the relevant Competition or Dataset page. The code in the notebook will then run in a sandbox and any resulting files (e.g. models, predictions, or metrics) will be saved to S3.\n",
       "\n",
       "## Run a notebook from a saved notebook URL\n",
       "\n",
       "You can share a URL of any notebook and other people will be able to run that notebook. If you share the URL of a Notebook on a public Dataset, then other people who have access to the Dataset will be able to run that notebook.\n",
       "\n",
       "## Run a notebook in a notebook sandbox from a saved notebook URL\n",
       "\n",
       "If you want to run a Notebook without sharing a URL, you can run it in the notebook sandbox. The easiest way to access the notebook sandbox is to go to the website of a Dataset or Competition and click on \"View Notebook\" to open the notebook associated with that Competition or Dataset in a new tab. You can then copy and paste your notebook’s code there. However, if you want to run a Notebook that is not associated with a Dataset or Competition (e.g. you have it locally), you need to use a URL."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category = \"notebook\"\n",
    "question = \"How to run a notebook?\"\n",
    "gemma_qa.query(category,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07b03dbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:26:17.055194Z",
     "iopub.status.busy": "2024-04-01T14:26:17.054825Z",
     "iopub.status.idle": "2024-04-01T14:26:20.015983Z",
     "shell.execute_reply": "2024-04-01T14:26:20.014966Z"
    },
    "papermill": {
     "duration": 3.051382,
     "end_time": "2024-04-01T14:26:20.018108",
     "exception": false,
     "start_time": "2024-04-01T14:26:16.966726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-discussions\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "How to create a discussion topic?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Create a Discussion Topic\n",
       "\n",
       "To start a discussion topic on Kaggle, you can either post a new one in the [Discussions](https://www.kaggle.com/discussions) section or upvote an existing one on the site.\n",
       "\n",
       "You can also search for existing Discussions in the sidebar.\n",
       "\n",
       "You can search by tag (e.g. \"python\"), keyword (e.g. \"model transfer learning\"), or user (e.g. \"Kaggle\")."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category = \"discussions\"\n",
    "question = \"How to create a discussion topic?\"\n",
    "gemma_qa.query(category,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad63b753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T14:26:20.196043Z",
     "iopub.status.busy": "2024-04-01T14:26:20.195178Z",
     "iopub.status.idle": "2024-04-01T14:26:24.391732Z",
     "shell.execute_reply": "2024-04-01T14:26:24.390763Z"
    },
    "papermill": {
     "duration": 4.287999,
     "end_time": "2024-04-01T14:26:24.393768",
     "exception": false,
     "start_time": "2024-04-01T14:26:20.105769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-competitions\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a code competition?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Code competitions are an important part of Kaggle Datasets and Competitions. Code competitions provide an opportunity for participants to demonstrate the utility of the code they’ve written in a time limit format.\n",
       "\n",
       "There are three types of code competitions: notebooks, models, and challenges. Notebooks competitions and models competitions are very similar, and both provide an opportunity for participants to demonstrate the utility of the code they’ve written in a time limit format. The primary difference is that models competitions allow participants to use models written for the competition in addition to the code that participants submit. Challenges competitions are a more limited version of notebooks competitions. They have limited functionality and are designed to be more lightweight and accessible for newcomers to data science."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category = \"competitions\"\n",
    "question = \"What is a code competition?\"\n",
    "gemma_qa.query(category,question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73149064",
   "metadata": {
    "papermill": {
     "duration": 0.088593,
     "end_time": "2024-04-01T14:26:24.569814",
     "exception": false,
     "start_time": "2024-04-01T14:26:24.481221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cdd280",
   "metadata": {
    "papermill": {
     "duration": 0.104026,
     "end_time": "2024-04-01T14:26:24.761822",
     "exception": false,
     "start_time": "2024-04-01T14:26:24.657796",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We demonstated how to fine-tune a Gemma model using LoRA.   \n",
    "We also created a class to run queries to the Gemma model and tested it with some examples from the existing training data but also with some new, not seen questions."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7669720,
     "sourceId": 64148,
     "sourceType": "competition"
    },
    {
     "datasetId": 4484051,
     "sourceId": 7711309,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 5171,
     "sourceId": 11371,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 880.636153,
   "end_time": "2024-04-01T14:26:28.344516",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-01T14:11:47.708363",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
