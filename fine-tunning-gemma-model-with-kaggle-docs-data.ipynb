{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9dbea0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.01052,
     "end_time": "2024-04-10T11:02:01.258268",
     "exception": false,
     "start_time": "2024-04-10T11:02:01.247748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center><h1>Fine-tunning Gemma model with Kaggle Docs data</h1></center>\n",
    "\n",
    "<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This notebook will demonstrate three things:\n",
    "\n",
    "1. How to fine-tune Gemma model using LoRA\n",
    "2. Creation of a specialised class to query about Kaggle features\n",
    "3. Some results of querying about Kaggle Docs\n",
    "\n",
    "This work is largely based on previous work. Here I list the sources:\n",
    "\n",
    "1. Gemma Model Card, Kaggle Models, https://www.kaggle.com/models/google/gemma\n",
    "2. Kaggle QA with Gemma - KerasNLP Starter, Kaggle Code, https://www.kaggle.com/code/awsaf49/kaggle-qa-with-gemma-kerasnlp-starter (Version 11)  \n",
    "3. Fine-tune Gemma models in Keras using LoRA, Kaggle Code, https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora (Version 1)  \n",
    "4. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, LoRA: Low-Rank Adaptation of Large Language Models, ArXiv, https://arxiv.org/pdf/2106.09685.pdf\n",
    "5. Abheesht Sharma, Matthew Watson, Parameter-efficient fine-tuning of GPT-2 with LoRA, https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/\n",
    "6. Keras 3 API documentation / KerasNLP / Models / Gemma, https://keras.io/api/keras_nlp/models/gemma/\n",
    "7. Kaggle Docs, Kaggle Dataset, https://www.kaggle.com/datasets/awsaf49/kaggle-docs  \n",
    "8. TPUs in Keras, Kaggle Docs, https://www.kaggle.com/docs/tpu  \n",
    "\n",
    "**Let's go**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f2612",
   "metadata": {
    "papermill": {
     "duration": 0.009729,
     "end_time": "2024-04-10T11:02:01.278174",
     "exception": false,
     "start_time": "2024-04-10T11:02:01.268445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is Gemma?\n",
    "\n",
    "\n",
    "Gemma is a collection of lightweight source generative AI models designed to be used mostly by developers and researchers. Created by Google DeepMind research lab that also developed Gemini, Gemma is available in several versions, with 2B and 7B parameters, as following:\n",
    "\n",
    "\n",
    "| Model                  | Parameters      | Tuned versions    | Description                                    | Recomemnded target platforms       |\n",
    "|------------------------|-----------------|-------------------|------------------------------------------------|------------------------------------|\n",
    "| `gemma_2b_en`          | 2.51B           | Pretrained        | 18-layer Gemma model (Gemma with 2B parameters)|Mobile devices and laptops          |\n",
    "| `gemma_instruct_2b_en` | 2.51B           | Instruction tuned | 18-layer Gemma model (Gemma with 2B parameters)| Mobile devices and laptops         | \n",
    "| `gemma_7b_en`          | 8.54B           | Pretrained        | 28-layer Gemma model (Gemma with 7B parameters)| Desktop computers and small servers|\n",
    "| `gemma_instruct_7b_en` | 8.54B           | Instruction tuned | 28-layer Gemma model (Gemma with 7B parameters)| Desktop computers and small servers|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6632952a",
   "metadata": {
    "papermill": {
     "duration": 0.009724,
     "end_time": "2024-04-10T11:02:01.297909",
     "exception": false,
     "start_time": "2024-04-10T11:02:01.288185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is LoRA?  \n",
    "\n",
    "LoRA stands for Low-Rank Adaptation. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to LoRA paper, this number decreases 10,000 times, and the computational resources size decreases 3 times. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28560c1",
   "metadata": {
    "papermill": {
     "duration": 0.009709,
     "end_time": "2024-04-10T11:02:01.318491",
     "exception": false,
     "start_time": "2024-04-10T11:02:01.308782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How we proceed?\n",
    "\n",
    "For fine-tunning with LoRA, we will follow the steps:\n",
    "\n",
    "1. Install prerequisites\n",
    "2. Load and process the data for fine-tuning\n",
    "3. Initialize the code for Gemma causal language model (Gemma Causal LM)\n",
    "4. Perform fine-tuning\n",
    "5. Test the fine-tunned model with questions from the data used for fine-tuning and with aditional questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3fdd90",
   "metadata": {
    "papermill": {
     "duration": 0.009697,
     "end_time": "2024-04-10T11:02:01.338003",
     "exception": false,
     "start_time": "2024-04-10T11:02:01.328306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prerequisites\n",
    "\n",
    "\n",
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d295dd82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:02:01.359841Z",
     "iopub.status.busy": "2024-04-10T11:02:01.359157Z",
     "iopub.status.idle": "2024-04-10T11:02:33.183900Z",
     "shell.execute_reply": "2024-04-10T11:02:33.182702Z"
    },
    "papermill": {
     "duration": 31.838626,
     "end_time": "2024-04-10T11:02:33.186532",
     "exception": false,
     "start_time": "2024-04-10T11:02:01.347906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras>=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c8fb0",
   "metadata": {
    "papermill": {
     "duration": 0.010106,
     "end_time": "2024-04-10T11:02:33.207132",
     "exception": false,
     "start_time": "2024-04-10T11:02:33.197026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88cfb2eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:02:33.229054Z",
     "iopub.status.busy": "2024-04-10T11:02:33.228732Z",
     "iopub.status.idle": "2024-04-10T11:02:47.833964Z",
     "shell.execute_reply": "2024-04-10T11:02:47.833026Z"
    },
    "papermill": {
     "duration": 14.618911,
     "end_time": "2024-04-10T11:02:47.836179",
     "exception": false,
     "start_time": "2024-04-10T11:02:33.217268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 11:02:37.753587: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-10 11:02:37.753684: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-10 11:02:37.877928: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"\"\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas() # progress bar for pandas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904cd49",
   "metadata": {
    "papermill": {
     "duration": 0.010304,
     "end_time": "2024-04-10T11:02:47.857599",
     "exception": false,
     "start_time": "2024-04-10T11:02:47.847295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91092762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:02:47.880030Z",
     "iopub.status.busy": "2024-04-10T11:02:47.879468Z",
     "iopub.status.idle": "2024-04-10T11:02:47.890448Z",
     "shell.execute_reply": "2024-04-10T11:02:47.889290Z"
    },
    "papermill": {
     "duration": 0.025749,
     "end_time": "2024-04-10T11:02:47.893713",
     "exception": false,
     "start_time": "2024-04-10T11:02:47.867964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    seed = 42\n",
    "    dataset_path = \"/kaggle/input/kaggle-docs/questions_answers\"\n",
    "    preset = \"gemma_2b_en\" # name of pretrained Gemma\n",
    "    sequence_length = 512 # max size of input sequence for training\n",
    "    batch_size = 1 # size of the input batch in training, x 2 as two GPUs\n",
    "    epochs = 15 # number of epochs to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492902db",
   "metadata": {
    "papermill": {
     "duration": 0.010061,
     "end_time": "2024-04-10T11:02:47.916928",
     "exception": false,
     "start_time": "2024-04-10T11:02:47.906867",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Initialize the TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db7fc5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:02:47.939742Z",
     "iopub.status.busy": "2024-04-10T11:02:47.938865Z",
     "iopub.status.idle": "2024-04-10T11:02:47.944172Z",
     "shell.execute_reply": "2024-04-10T11:02:47.943269Z"
    },
    "papermill": {
     "duration": 0.018764,
     "end_time": "2024-04-10T11:02:47.946144",
     "exception": false,
     "start_time": "2024-04-10T11:02:47.927380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(Config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a788a77",
   "metadata": {
    "papermill": {
     "duration": 0.010483,
     "end_time": "2024-04-10T11:02:47.967046",
     "exception": false,
     "start_time": "2024-04-10T11:02:47.956563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "917bf0db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:02:47.989926Z",
     "iopub.status.busy": "2024-04-10T11:02:47.988953Z",
     "iopub.status.idle": "2024-04-10T11:02:48.031108Z",
     "shell.execute_reply": "2024-04-10T11:02:48.030185Z"
    },
    "papermill": {
     "duration": 0.055996,
     "end_time": "2024-04-10T11:02:48.033500",
     "exception": false,
     "start_time": "2024-04-10T11:02:47.977504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the different types of competitions a...</td>\n",
       "      <td># Types of Competitions\\n\\nKaggle Competitions...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the different competition formats on ...</td>\n",
       "      <td>There are handful of different formats competi...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to join a competition?</td>\n",
       "      <td>Before you start, navigate to the [Competition...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to form, manage, and disband teams in a co...</td>\n",
       "      <td>Everyone that competes in a Competition does s...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I make a submission in a competition?</td>\n",
       "      <td>You will need to submit your model predictions...</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  What are the different types of competitions a...   \n",
       "1  What are the different competition formats on ...   \n",
       "2                         How to join a competition?   \n",
       "3  How to form, manage, and disband teams in a co...   \n",
       "4       How do I make a submission in a competition?   \n",
       "\n",
       "                                              Answer     Category  \n",
       "0  # Types of Competitions\\n\\nKaggle Competitions...  competition  \n",
       "1  There are handful of different formats competi...  competition  \n",
       "2  Before you start, navigate to the [Competition...  competition  \n",
       "3  Everyone that competes in a Competition does s...  competition  \n",
       "4  You will need to submit your model predictions...  competition  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{Config.dataset_path}/data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950cac68",
   "metadata": {
    "papermill": {
     "duration": 0.01402,
     "end_time": "2024-04-10T11:02:48.062679",
     "exception": false,
     "start_time": "2024-04-10T11:02:48.048659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's check the total number of rows in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1e22ea8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:02:48.090526Z",
     "iopub.status.busy": "2024-04-10T11:02:48.089313Z",
     "iopub.status.idle": "2024-04-10T11:02:48.096871Z",
     "shell.execute_reply": "2024-04-10T11:02:48.095744Z"
    },
    "papermill": {
     "duration": 0.022239,
     "end_time": "2024-04-10T11:02:48.099624",
     "exception": false,
     "start_time": "2024-04-10T11:02:48.077385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ecb651",
   "metadata": {
    "papermill": {
     "duration": 0.011238,
     "end_time": "2024-04-10T11:02:48.125865",
     "exception": false,
     "start_time": "2024-04-10T11:02:48.114627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For easiness, we will create the following template for QA: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e113f5ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:02:48.149374Z",
     "iopub.status.busy": "2024-04-10T11:02:48.148662Z",
     "iopub.status.idle": "2024-04-10T11:02:48.161167Z",
     "shell.execute_reply": "2024-04-10T11:02:48.160174Z"
    },
    "papermill": {
     "duration": 0.026688,
     "end_time": "2024-04-10T11:02:48.163364",
     "exception": false,
     "start_time": "2024-04-10T11:02:48.136676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\\n\\nCategory:\\nkaggle-{Category}\\n\\nQuestion:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\n",
    "df[\"prompt\"] = df.apply(lambda row: template.format(Category=row.Category,\n",
    "                                                             Question=row.Question,\n",
    "                                                             Answer=row.Answer), axis=1)\n",
    "data = df.prompt.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a63c8e9",
   "metadata": {
    "papermill": {
     "duration": 0.010532,
     "end_time": "2024-04-10T11:02:48.184694",
     "exception": false,
     "start_time": "2024-04-10T11:02:48.174162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Template utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fda30580",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:02:48.208945Z",
     "iopub.status.busy": "2024-04-10T11:02:48.208665Z",
     "iopub.status.idle": "2024-04-10T11:02:48.213515Z",
     "shell.execute_reply": "2024-04-10T11:02:48.212615Z"
    },
    "papermill": {
     "duration": 0.018746,
     "end_time": "2024-04-10T11:02:48.215455",
     "exception": false,
     "start_time": "2024-04-10T11:02:48.196709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Category\", \"Question\", \"Answer\"], [\"blue\", \"red\", \"green\"]):\n",
    "        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d46aaf",
   "metadata": {
    "papermill": {
     "duration": 0.010402,
     "end_time": "2024-04-10T11:02:48.236539",
     "exception": false,
     "start_time": "2024-04-10T11:02:48.226137",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Specialized class to query Gemma\n",
    "\n",
    "\n",
    "We define a specialized class to query Gemma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c564e9e",
   "metadata": {
    "papermill": {
     "duration": 0.010569,
     "end_time": "2024-04-10T11:02:48.257727",
     "exception": false,
     "start_time": "2024-04-10T11:02:48.247158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initialize the code for Gemma Causal LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afa48e18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:02:48.280552Z",
     "iopub.status.busy": "2024-04-10T11:02:48.280226Z",
     "iopub.status.idle": "2024-04-10T11:03:49.678711Z",
     "shell.execute_reply": "2024-04-10T11:03:49.677809Z"
    },
    "papermill": {
     "duration": 61.41227,
     "end_time": "2024-04-10T11:03:49.680743",
     "exception": false,
     "start_time": "2024-04-10T11:02:48.268473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_causal_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
    "gemma_causal_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c20af6",
   "metadata": {
    "papermill": {
     "duration": 0.012116,
     "end_time": "2024-04-10T11:03:49.705630",
     "exception": false,
     "start_time": "2024-04-10T11:03:49.693514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define the specialized class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccdd675f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:03:49.732137Z",
     "iopub.status.busy": "2024-04-10T11:03:49.731786Z",
     "iopub.status.idle": "2024-04-10T11:03:49.737721Z",
     "shell.execute_reply": "2024-04-10T11:03:49.736877Z"
    },
    "papermill": {
     "duration": 0.021307,
     "end_time": "2024-04-10T11:03:49.739595",
     "exception": false,
     "start_time": "2024-04-10T11:03:49.718288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GemmaQA:\n",
    "    def __init__(self, max_length=512):\n",
    "        self.max_length = max_length\n",
    "        self.prompt = template\n",
    "        self.gemma_causal_lm = gemma_causal_lm\n",
    "        \n",
    "    def query(self, category, question):\n",
    "        response = self.gemma_causal_lm.generate(\n",
    "            self.prompt.format(\n",
    "                Category=category,\n",
    "                Question=question,\n",
    "                Answer=\"\"), \n",
    "            max_length=self.max_length)\n",
    "        display(Markdown(colorize_text(response)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a919fb76",
   "metadata": {
    "papermill": {
     "duration": 0.012227,
     "end_time": "2024-04-10T11:03:49.764235",
     "exception": false,
     "start_time": "2024-04-10T11:03:49.752008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Gemma preprocessor\n",
    "\n",
    "\n",
    "This preprocessing layer will take in batches of strings, and return outputs in a ```(x, y, sample_weight)``` format, where the y label is the next token id in the x sequence.\n",
    "\n",
    "From the code below, we can see that, after the preprocessor, the data shape is ```(num_samples, sequence_length)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6bfe709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:03:49.790115Z",
     "iopub.status.busy": "2024-04-10T11:03:49.789810Z",
     "iopub.status.idle": "2024-04-10T11:03:50.163573Z",
     "shell.execute_reply": "2024-04-10T11:03:50.162646Z"
    },
    "papermill": {
     "duration": 0.389498,
     "end_time": "2024-04-10T11:03:50.166000",
     "exception": false,
     "start_time": "2024-04-10T11:03:49.776502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y, sample_weight = gemma_causal_lm.preprocessor(data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06c211fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:03:50.193563Z",
     "iopub.status.busy": "2024-04-10T11:03:50.193144Z",
     "iopub.status.idle": "2024-04-10T11:03:50.201099Z",
     "shell.execute_reply": "2024-04-10T11:03:50.200190Z"
    },
    "papermill": {
     "duration": 0.024612,
     "end_time": "2024-04-10T11:03:50.203580",
     "exception": false,
     "start_time": "2024-04-10T11:03:50.178968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': Array([[   2,  109, 8606, ...,    0,    0,    0],\n",
      "       [   2,  109, 8606, ...,    0,    0,    0]], dtype=int32), 'padding_mask': Array([[ True,  True,  True, ..., False, False, False],\n",
      "       [ True,  True,  True, ..., False, False, False]], dtype=bool)} [[   109   8606 235292 ...      0      0      0]\n",
      " [   109   8606 235292 ...      0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20530afe",
   "metadata": {
    "papermill": {
     "duration": 0.012391,
     "end_time": "2024-04-10T11:03:50.229975",
     "exception": false,
     "start_time": "2024-04-10T11:03:50.217584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Perform fine-tuning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635aa908",
   "metadata": {
    "papermill": {
     "duration": 0.01633,
     "end_time": "2024-04-10T11:03:50.305704",
     "exception": false,
     "start_time": "2024-04-10T11:03:50.289374",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Enable LoRA for the model\n",
    "\n",
    "LoRA rank is setting the number of trainable parameters. A larger rank will result in a larger number of parameters to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8f8e449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:03:50.333542Z",
     "iopub.status.busy": "2024-04-10T11:03:50.333157Z",
     "iopub.status.idle": "2024-04-10T11:03:50.826358Z",
     "shell.execute_reply": "2024-04-10T11:03:50.825378Z"
    },
    "papermill": {
     "duration": 0.50922,
     "end_time": "2024-04-10T11:03:50.828385",
     "exception": false,
     "start_time": "2024-04-10T11:03:50.319165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to 4.\n",
    "gemma_causal_lm.backbone.enable_lora(rank=4)\n",
    "gemma_causal_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640037a",
   "metadata": {
    "papermill": {
     "duration": 0.013542,
     "end_time": "2024-04-10T11:03:50.855907",
     "exception": false,
     "start_time": "2024-04-10T11:03:50.842365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run the training sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91cd66de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:03:50.884742Z",
     "iopub.status.busy": "2024-04-10T11:03:50.884431Z",
     "iopub.status.idle": "2024-04-10T11:15:51.630421Z",
     "shell.execute_reply": "2024-04-10T11:15:51.629502Z"
    },
    "papermill": {
     "duration": 720.76279,
     "end_time": "2024-04-10T11:15:51.632285",
     "exception": false,
     "start_time": "2024-04-10T11:03:50.869495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 736ms/step - loss: 1.7209 - sparse_categorical_accuracy: 0.5241\n",
      "Epoch 2/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.6869 - sparse_categorical_accuracy: 0.5313\n",
      "Epoch 3/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.6175 - sparse_categorical_accuracy: 0.5417\n",
      "Epoch 4/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.5770 - sparse_categorical_accuracy: 0.5509\n",
      "Epoch 5/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.5537 - sparse_categorical_accuracy: 0.5552\n",
      "Epoch 6/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - loss: 1.5304 - sparse_categorical_accuracy: 0.5568\n",
      "Epoch 7/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.5028 - sparse_categorical_accuracy: 0.5630\n",
      "Epoch 8/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.4733 - sparse_categorical_accuracy: 0.5682\n",
      "Epoch 9/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.4444 - sparse_categorical_accuracy: 0.5745\n",
      "Epoch 10/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.4025 - sparse_categorical_accuracy: 0.5873\n",
      "Epoch 11/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.3607 - sparse_categorical_accuracy: 0.5960\n",
      "Epoch 12/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.3163 - sparse_categorical_accuracy: 0.6079\n",
      "Epoch 13/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.2684 - sparse_categorical_accuracy: 0.6198\n",
      "Epoch 14/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.2149 - sparse_categorical_accuracy: 0.6325\n",
      "Epoch 15/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 729ms/step - loss: 1.1585 - sparse_categorical_accuracy: 0.6451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7eca14080970>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_causal_lm.preprocessor.sequence_length = Config.sequence_length \n",
    "\n",
    "# Compile the model with loss, optimizer, and metric\n",
    "gemma_causal_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=8e-5),\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "gemma_causal_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc28f7",
   "metadata": {
    "papermill": {
     "duration": 0.085379,
     "end_time": "2024-04-10T11:15:51.804637",
     "exception": false,
     "start_time": "2024-04-10T11:15:51.719258",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f1345cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:15:51.977365Z",
     "iopub.status.busy": "2024-04-10T11:15:51.977004Z",
     "iopub.status.idle": "2024-04-10T11:15:51.981770Z",
     "shell.execute_reply": "2024-04-10T11:15:51.980507Z"
    },
    "papermill": {
     "duration": 0.093645,
     "end_time": "2024-04-10T11:15:51.984012",
     "exception": false,
     "start_time": "2024-04-10T11:15:51.890367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_qa = GemmaQA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd91d81d",
   "metadata": {
    "papermill": {
     "duration": 0.086078,
     "end_time": "2024-04-10T11:15:52.157636",
     "exception": false,
     "start_time": "2024-04-10T11:15:52.071558",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f46ce615",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:15:52.332956Z",
     "iopub.status.busy": "2024-04-10T11:15:52.332124Z",
     "iopub.status.idle": "2024-04-10T11:16:11.640729Z",
     "shell.execute_reply": "2024-04-10T11:16:11.639713Z"
    },
    "papermill": {
     "duration": 19.398877,
     "end_time": "2024-04-10T11:16:11.642931",
     "exception": false,
     "start_time": "2024-04-10T11:15:52.244054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-competition\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What are the different types of competitions available on Kaggle?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Types of Competitions\n",
       "\n",
       "There are two main types of competitions on Kaggle:\n",
       "\n",
       "- **Data science challenges**: These are typically data science challenges where the goal is to build a model that outperforms a provided baseline. Examples include building a model to predict the likelihood of a fraudulent transaction or building a model to predict the likelihood of a heart attack.\n",
       "\n",
       "- **Classification challenges**: These are typically classification challenges where the goal is to build a model that outperforms a provided baseline. Examples include building a model to classify emails as spam or ham or building a model to classify images as cats or dogs.\n",
       "\n",
       "## How Are the Kaggle Competitions Ranked?\n",
       "\n",
       "The rankings on Kaggle are based on a combination of two metrics:\n",
       "\n",
       "- **Submissions**: The total number of submissions made by a user.\n",
       "\n",
       "- **Leaderboard Rank**: The rank of a user on the leaderboard of a given competition.\n",
       "\n",
       "The leaderboard rank is calculated by taking the median rank of the top 100 users on the leaderboard. The median rank is calculated by first sorting the leaderboard by leaderboard rank and then taking the middle value of the leaderboard.\n",
       "\n",
       "The combination of these two metrics is designed to encourage users to make high-quality submissions and to reward users who are consistently at the top of the leaderboard."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[0]\n",
    "gemma_qa.query(row.Category,row.Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c6b906",
   "metadata": {
    "papermill": {
     "duration": 0.086627,
     "end_time": "2024-04-10T11:16:11.818481",
     "exception": false,
     "start_time": "2024-04-10T11:16:11.731854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35063e29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:16:11.993694Z",
     "iopub.status.busy": "2024-04-10T11:16:11.992936Z",
     "iopub.status.idle": "2024-04-10T11:16:25.397097Z",
     "shell.execute_reply": "2024-04-10T11:16:25.396229Z"
    },
    "papermill": {
     "duration": 13.494327,
     "end_time": "2024-04-10T11:16:25.399244",
     "exception": false,
     "start_time": "2024-04-10T11:16:11.904917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-tpu\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "How to load and save model on TPU?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Overview\n",
       "\n",
       "TPU models are saved in SavedModel format. To load a model from SavedModel, you can use the `tpu.Model.from_saved_model` method.\n",
       "\n",
       "To save a model, you can use the `tpu.Model.save_main_session()` method.\n",
       "\n",
       "## Example\n",
       "\n",
       "```python\n",
       "# Load a model from SavedModel\n",
       "model = tpu.Model.from_saved_model(tpu.saved_model_fn)\n",
       "```\n",
       "\n",
       "## TPU Model Format\n",
       "\n",
       "TPU models are saved in SavedModel format. To load a model from SavedModel, you can use the tpu.Model.from_saved_model method.\n",
       "\n",
       "To save a model, you can use the tpu.Model.save_main_session() method.\n",
       "\n",
       "## TPU Model Format Overview\n",
       "\n",
       "The SavedModel format is a model archive that contains all the information needed to run a model on TPU. It is a zip archive with the following structure:\n",
       "\n",
       "- `model.pb` is the TensorFlow graph that describes the model.\n",
       "- `checkpoint.index` is a binary file that contains the metadata for the checkpoints in the model.\n",
       "- `config.json` is a JSON file that contains the configuration for the model.\n",
       "- `variables.add_file_io_ops.json` is a JSON file that contains the metadata for the file I/O ops in the model.\n",
       "- `variables.add_file_io_ops.pb` is a binary file that contains the code for the file I/O ops in the model.\n",
       "- `variables.add_op_for_each_checkpoint.index` is a binary file that contains the metadata for the checkpoints in the model.\n",
       "- `variables.add_op_for_each_checkpoint.json` is a JSON file that contains the metadata for the checkpoints in the model.\n",
       "- `variables.add_op_for_each_checkpoint.pb` is a binary file that contains the code for the checkpoints in the model.\n",
       "- `variables.add_op_for_each_checkpoint.summary.tsv` is a tab-separated-values file that contains the summary of the checkpoints in the model.\n",
       "- `variables.add_op_for_each_checkpoint.summary.tsv` is a tab-separated-values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[15]\n",
    "gemma_qa.query(row.Category,row.Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2935841",
   "metadata": {
    "papermill": {
     "duration": 0.086686,
     "end_time": "2024-04-10T11:16:25.574120",
     "exception": false,
     "start_time": "2024-04-10T11:16:25.487434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8f1b8a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:16:25.748854Z",
     "iopub.status.busy": "2024-04-10T11:16:25.748211Z",
     "iopub.status.idle": "2024-04-10T11:16:39.044711Z",
     "shell.execute_reply": "2024-04-10T11:16:39.043763Z"
    },
    "papermill": {
     "duration": 13.386025,
     "end_time": "2024-04-10T11:16:39.046831",
     "exception": false,
     "start_time": "2024-04-10T11:16:25.660806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-noteboook\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What are the different types of notebooks available on Kaggle?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Notebooks\n",
       "\n",
       "Kaggle Notebooks are interactive markdown documents that allow users to run code and see the results. Notebooks are a key part of the Kaggle experience, and are used by the community to share data science workflows, discuss techniques, and explore new ideas.\n",
       "\n",
       "Kaggle Notebooks are hosted on nbviewer.com. You can access them from any web browser, or from the Kaggle mobile app.\n",
       "\n",
       "You can create a Notebook from the Kaggle website or from the command line using the kaggle notebook command.\n",
       "\n",
       "## Versions\n",
       "\n",
       "Each Notebook has multiple versions. Each version is a snapshot of the Notebook at a specific point in time. You can view and compare the versions of a Notebook to see how it has changed over time.\n",
       "\n",
       "You can access the version history of a Notebook from the Notebook editor.\n",
       "\n",
       "You can create a new version of a Notebook from the Notebook editor.\n",
       "\n",
       "You can delete a Notebook version from the Notebook editor.\n",
       "\n",
       "## Versions and Sharing\n",
       "\n",
       "Each Notebook version has a URL that looks like this:\n",
       "\n",
       "```\n",
       "https://www.kaggle.com/kylegordon/kaggle-noteboook-versions?version=1666666\n",
       "```\n",
       "\n",
       "You can share a Notebook version URL with others, and they will be prompted to view the Notebook as a guest.\n",
       "\n",
       "You can also create a Notebook version from a URL that someone else has shared with you.\n",
       "\n",
       "## Versions and Permissions\n",
       "\n",
       "Each Notebook version has a URL that looks like this:\n",
       "\n",
       "```\n",
       "https://www.kaggle.com/kylegordon/kaggle-noteboook-versions?version=1666666\n",
       "```\n",
       "\n",
       "You can view and compare the versions of a Notebook to see how it has changed over time.\n",
       "\n",
       "You can access the version history of a Notebook from the Notebook editor.\n",
       "\n",
       "You can create a new version of a Notebook from the Notebook editor.\n",
       "\n",
       "You can delete a Notebook version from the Notebook editor.\n",
       "\n",
       "You can also delete a Notebook version from the Versions tab of the Notebook editor.\n",
       "\n",
       "## Versions and Privacy\n",
       "\n",
       "Each Notebook version has a URL that looks like this:\n",
       "\n",
       "```\n",
       "https://www.kaggle.com/kylegordon/kaggle-noteboook-versions?version=1666666\n",
       "```\n",
       "\n",
       "You can share a Notebook version URL with others"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[25]\n",
    "gemma_qa.query(row.Category,row.Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2708bd06",
   "metadata": {
    "papermill": {
     "duration": 0.089138,
     "end_time": "2024-04-10T11:16:39.224104",
     "exception": false,
     "start_time": "2024-04-10T11:16:39.134966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Not seen question(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "408b58fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:16:39.401571Z",
     "iopub.status.busy": "2024-04-10T11:16:39.401168Z",
     "iopub.status.idle": "2024-04-10T11:16:52.914290Z",
     "shell.execute_reply": "2024-04-10T11:16:52.913345Z"
    },
    "papermill": {
     "duration": 13.604821,
     "end_time": "2024-04-10T11:16:52.916517",
     "exception": false,
     "start_time": "2024-04-10T11:16:39.311696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-notebook\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "How to run a notebook?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Installation\n",
       "\n",
       "You can run notebooks on Kaggle without installing anything, but if you want to make use of the community-built add-ons marketplace or you have a team of users that you want to collaborate with on a notebook, then you should consider creating a notebook on Kaggle.\n",
       "\n",
       "To install Kaggle and Notebook features in your own conda environment, run:\n",
       "\n",
       "```\n",
       "kaggle datasets login-or-create <USERNAME>\n",
       "kaggle kernels --help\n",
       "```\n",
       "\n",
       "For a full list of command line arguments, run `kaggle kernels help <command` from the Kaggle command line.\n",
       "\n",
       "## Run Locally\n",
       "\n",
       "If you are running locally, you can run the notebook locally by running the following command from the notebook directory:\n",
       "\n",
       "```python\n",
       "!kaggle run --kernels /kaggle/notebooks/tutorial-saving-and-loading-models: Cells 1-X\n",
       "```\n",
       "\n",
       "The command will run the notebook from the first cell to the most recently executed cell. If you want to run a specific cell, you can specify the cell number after the `--kernels` flag.\n",
       "\n",
       "If you want to run the notebook from scratch, you can run the notebook locally by running the following command from the notebook directory:\n",
       "\n",
       "```python\n",
       "kaggle run --kernels <USERNAME>/notebooks/tutorial-saving-and-loading-models:latest\n",
       "```\n",
       "\n",
       "The command will run the notebook from the first cell to the most recently executed cell.\n",
       "\n",
       "If you want to run a specific cell, you can specify the cell number after the `--kernels` flag.\n",
       "\n",
       "If you want to run the notebook from scratch, you can run the notebook locally by running the following command from the notebook directory:\n",
       "\n",
       "```python\n",
       "kaggle run --kernels <USERNAME>/notebooks/tutorial-saving-and-loading-models:latest\n",
       "```\n",
       "\n",
       "## Run on Kaggle\n",
       "\n",
       "If you are running on Kaggle, you can run the notebook on Kaggle by clicking the “Run Notebook” button in the top right corner of the notebook editor.\n",
       "\n",
       "If you are running locally, you can upload the notebook to Kaggle and run the notebook locally by running the following command from the notebook directory:\n",
       "\n",
       "```python\n",
       "kaggle runnotebooks <USERNAME>/notebooks/tutorial-saving-and-loading-models:latest\n",
       "```\n",
       "\n",
       "If you are running on Kaggle, you can run the notebook"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category = \"notebook\"\n",
    "question = \"How to run a notebook?\"\n",
    "gemma_qa.query(category,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f9dfa2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:16:53.096409Z",
     "iopub.status.busy": "2024-04-10T11:16:53.096029Z",
     "iopub.status.idle": "2024-04-10T11:17:06.580691Z",
     "shell.execute_reply": "2024-04-10T11:17:06.579744Z"
    },
    "papermill": {
     "duration": 13.575585,
     "end_time": "2024-04-10T11:17:06.582639",
     "exception": false,
     "start_time": "2024-04-10T11:16:53.007054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-discussions\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "How to create a discussion topic?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Creating a Discussion\n",
       "\n",
       "To create a new discussion topic, click on the \"Discussions\" tab and click on the \"New Discussion\" button.\n",
       "\n",
       "You can create a new discussion by either uploading a dataset or by pasting a URL. If you upload a dataset, the discussion will be associated with the dataset. If you paste a URL, the discussion will be associated with the notebook that URL points to.\n",
       "\n",
       "If you upload a dataset, the discussion will be associated with the dataset. If you paste a URL, the discussion will be associated with the notebook that URL points to.\n",
       "\n",
       "If you are creating a discussion associated with a dataset, the discussion will be public by default. If you are creating a discussion associated with a notebook, the discussion will be private by default.\n",
       "\n",
       "If you want to make the discussion public, click on the \"Privacy\" dropdown and select \"Public\".\n",
       "\n",
       "If you want to make the discussion private, click on the \"Privacy\" dropdown and select \"Private\".\n",
       "\n",
       "If you want to make the discussion a community discussion, click on the \"Privacy\" dropdown and select \"Community\".\n",
       "\n",
       "If you want to make the discussion a team discussion, click on the \"Team\" dropdown and select \"Team\".\n",
       "\n",
       "If you want to make the discussion a user-to-user conversation, click on the \"Conversation\" dropdown and select \"User to User\".\n",
       "\n",
       "If you want to make the discussion a notebook discussion, click on the \"Notebook\" dropdown and select \"Notebook\".\n",
       "\n",
       "If you want to create a discussion associated with a dataset, click on the \"Dataset\" dropdown and select the dataset you want to associate the discussion with.\n",
       "\n",
       "If you want to create a discussion associated with a notebook, click on the \"Notebook\" dropdown and select the notebook you want to associate the discussion with.\n",
       "\n",
       "If you are creating a discussion associated with a dataset, you will be presented with a list of all the datasets associated with the selected folder.\n",
       "\n",
       "If you are creating a discussion associated with a notebook, you will be presented with a list of all the notebooks associated with the selected folder.\n",
       "\n",
       "If you are creating a discussion associated with a dataset, you will be presented with a list of all the users who have access to the dataset.\n",
       "\n",
       "If you are creating a discussion associated with a notebook, you will be presented with a list of all the users who have access to the notebook.\n",
       "\n",
       "If you are creating"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category = \"discussions\"\n",
    "question = \"How to create a discussion topic?\"\n",
    "gemma_qa.query(category,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68ab06e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:17:06.761300Z",
     "iopub.status.busy": "2024-04-10T11:17:06.760519Z",
     "iopub.status.idle": "2024-04-10T11:17:20.247833Z",
     "shell.execute_reply": "2024-04-10T11:17:20.246791Z"
    },
    "papermill": {
     "duration": 13.577438,
     "end_time": "2024-04-10T11:17:20.249787",
     "exception": false,
     "start_time": "2024-04-10T11:17:06.672349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-competitions\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is a code competition?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Code competitions are a type of competition where participants are given a dataset and a set of tools to build machine learning models. The goal is to build the best model for a given task. Code competitions are a popular format for Kaggle Datasets because they encourage participants to use the tools and techniques available on Kaggle.\n",
       "\n",
       "Code competitions are a great way to learn new tools and techniques, and to see how they can be applied to a real-world problem. They also provide a structured way to evaluate and improve your model, as you'll be competing against other participants who are using the same dataset and tools.\n",
       "\n",
       "There are many benefits to participating in code competitions. First, they're a great way to learn how to use a dataset and a task to drive model development. Second, they're a great way to improve your model's performance. Third, they're a great way to meet other data scientists and machine learning enthusiasts. Fourth, they're a lot of fun!\n",
       "\n",
       "## Types of Code Competitions\n",
       "\n",
       "There are two main types of code competitions on Kaggle: public and private.\n",
       "\n",
       "Public competitions are open to anyone and everyone. They're a great way to see what other people are working on in the community, and to learn from others' experiences. Public competitions are a great way to get exposure and recognition for your work.\n",
       "\n",
       "Private competitions are invitation-only and typically invitees are part of a larger community or organization. They're a great way to collaborate with others who share your interest in the same topic. Private competitions are a great way to get feedback on your work from experts in the field.\n",
       "\n",
       "## How to Participate in a Code Competition\n",
       "\n",
       "To participate in a code competition, you'll need to create a competition notebook. This notebook will contain all of the code you use to build your model. You can then submit your notebook to the competition.\n",
       "\n",
       "The first step is to find a competition that interests you. You can browse all of the current competitions on Kaggle Datasets by visiting the [code competitions page](https://www.kaggle.com/competitions/code/overview).\n",
       "\n",
       "Once you've found a competition you'd like to enter, click on the \"Participate\" button. This will take you to a page where you can sign up for the competition.\n",
       "\n",
       "The next step is to familiarize yourself with the dataset. This is the collection"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category = \"competitions\"\n",
    "question = \"What is a code competition?\"\n",
    "gemma_qa.query(category,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1380d22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:17:20.430896Z",
     "iopub.status.busy": "2024-04-10T11:17:20.430035Z",
     "iopub.status.idle": "2024-04-10T11:17:33.807645Z",
     "shell.execute_reply": "2024-04-10T11:17:33.806726Z"
    },
    "papermill": {
     "duration": 13.469964,
     "end_time": "2024-04-10T11:17:33.809626",
     "exception": false,
     "start_time": "2024-04-10T11:17:20.339662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='blue'>Category:</font>**\n",
       "kaggle-datasets\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What are the steps to create a Kaggle dataset?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "## Introduction\n",
       "\n",
       "Kaggle Datasets is a repository of public datasets that are created by members of the Kaggle community. Datasets are used by the Kaggle community for a variety of purposes, including training machine learning models, evaluating models, and sharing data.\n",
       "\n",
       "Kaggle Datasets is a great resource for anyone working with data.\n",
       "\n",
       "## Getting Started\n",
       "\n",
       "To get started creating a new dataset, navigate to the \"Create Dataset\" button in the left-hand navigation bar.\n",
       "\n",
       "You can also access the \"Create Dataset\" menu from your profile dropdown.\n",
       "\n",
       "You will be prompted to enter a name for your dataset, a description, and a license.\n",
       "\n",
       "## Dataset Metadata\n",
       "\n",
       "The metadata is the information about your dataset that will be displayed in the search results.\n",
       "\n",
       "The metadata is what people will see when they search for your dataset.\n",
       "\n",
       "## Metadata Fields\n",
       "\n",
       "The following fields are required:\n",
       "\n",
       "- **Name**: The name of your dataset. This is what people will see when they search for your dataset.\n",
       "- **Description**: A short description of your dataset. This description will be displayed in the search results.\n",
       "- **Tags**: Tags are keywords that describe your dataset. You can add as many as you want.\n",
       "- **License**: The license of your dataset.\n",
       "- **Data files**: The files that make up your dataset.\n",
       "- **Acknowledgements**: A list of people or organizations that contributed to the creation of your dataset.\n",
       "- **Metadata fields**: A list of the fields in your dataset.\n",
       "- **Schema**: A diagram of the structure of your dataset.\n",
       "- **Acknowledgements**: A list of people or organizations that contributed to the creation of your dataset.\n",
       "- **Metadata fields**: A list of the fields in your dataset.\n",
       "- **Schema**: A diagram of the structure of your dataset.\n",
       "- **Data files**: The files that make up your dataset.\n",
       "- **Versions**: The versions of your dataset that are currently published.\n",
       "\n",
       "## Metadata Best Practices\n",
       "\n",
       "- **Name**: Make sure the name of your dataset is descriptive and easy to remember.\n",
       "- **Description**: Write a concise description of your dataset that includes the purpose, format, and size of the data.\n",
       "- **Tags**: Use keywords that are relevant to your dataset.\n",
       "- **License**: Choose a license that is appropriate for your dataset.\n",
       "- **Data files**: Upload a copy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category = \"datasets\"\n",
    "question = \"What are the steps to create a Kaggle dataset?\"\n",
    "gemma_qa.query(category,question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562878e8",
   "metadata": {
    "papermill": {
     "duration": 0.087818,
     "end_time": "2024-04-10T11:17:33.988095",
     "exception": false,
     "start_time": "2024-04-10T11:17:33.900277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc6a49c",
   "metadata": {
    "papermill": {
     "duration": 0.089247,
     "end_time": "2024-04-10T11:17:34.166731",
     "exception": false,
     "start_time": "2024-04-10T11:17:34.077484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We demonstated how to fine-tune a Gemma model using LoRA.   \n",
    "We also created a class to run queries to the Gemma model and tested it with some examples from the existing training data but also with some new, not seen questions."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7669720,
     "sourceId": 64148,
     "sourceType": "competition"
    },
    {
     "datasetId": 4484051,
     "sourceId": 7711309,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 5171,
     "sourceId": 11371,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 939.704639,
   "end_time": "2024-04-10T11:17:38.129538",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-10T11:01:58.424899",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
