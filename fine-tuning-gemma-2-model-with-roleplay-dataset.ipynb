{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9430417,"sourceType":"datasetVersion","datasetId":5729234},{"sourceId":85984,"sourceType":"modelInstanceVersion","modelInstanceId":72244,"modelId":78150}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-with-roleplay-dataset?scriptVersionId=197291626\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<center><h1>Fine-tuning Gemma 2 model using LoRA and Keras</h1></center>\n\n<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n\n\n# Introduction\n\nThis notebook will demonstrate three things:\n\n1. How to fine-tune Gemma model using LoRA\n2. Creation of a specialised class to query about Kaggle features\n3. Some results of querying about various topics while instructing the model to adopt a certain persona, from the ones included in the data used for fine tuning.\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# What is Gemma 2?\n\nGemma is a collection of lightweight, advanced open models developed by Google, leveraging the same research and technology behind the Gemini models. These models are text-to-text, decoder-only large language models available in English, with open weights provided for both pre-trained and instruction-tuned versions. Gemma models excel in a range of text generation tasks, such as question answering, summarization, and reasoning. Their compact size allows for deployment in resource-constrained environments like laptops, desktops, or personal cloud infrastructure, making state-of-the-art AI models more accessible and encouraging innovation for all. \n\nGemma 2 represent the 2nd generation of Gemma models. These models were trained on a dataset of text data that includes a wide variety of sources. The **27B** model was trained with **13 trillion** tokens, the **9B** model was trained with **8 trillion tokens**, and **2B** model was trained with **2 trillion** tokens. Here is a summary of their key components: \n* **Web Documents**: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content.\n* **Code**: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions.\n* **Mathematics**: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.\n\nTo learn more about Gemma 2, follow this link: [Gemma 2 Model Card](https://www.kaggle.com/models/google/gemma-2).\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# What is LoRA?  \n\n**LoRA** stands for **Low-Rank Adaptation**. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to **LoRA** paper, this number decreases **10,000 times**, and the computational resources size decreases 3 times. ","metadata":{}},{"cell_type":"markdown","source":"# How we proceed?\n\nFor fine-tunning with LoRA, we will follow the steps:\n\n1. Install prerequisites\n2. Load and process the data for fine-tuning\n3. Initialize the code for Gemma causal language model (Gemma Causal LM)\n4. Perform fine-tuning so that the model will learn the various persona and be able to perform in each role.\n5. Test the fine-tunned model with questions from the data used for fine-tuning and with aditional questions","metadata":{}},{"cell_type":"markdown","source":"# Prerequisites\n\n\n## Install packages\n\nWe start by installing `keras-nlp` and `keras` packages.","metadata":{}},{"cell_type":"code","source":"# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n!pip install -q -U keras-nlp\n!pip install -q -U keras>=3\n!pip install -q -U kagglehub --upgrade","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-19T11:44:03.69845Z","iopub.execute_input":"2024-09-19T11:44:03.698912Z","iopub.status.idle":"2024-09-19T11:44:33.723223Z","shell.execute_reply.started":"2024-09-19T11:44:03.698866Z","shell.execute_reply":"2024-09-19T11:44:33.721997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import packages\n\nNow we can import the packages we just installed. We will also install `os`, so that we can set the environment variables needed for keras backend. We will use `jax` as `KERAS_BACKEND`.\n\nBecause we want to publish the Model from the Notebook, we also include `kagglehub` and import secrets from `Kaggle App`.","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\nos.environ[\"JAX_PLATFORMS\"] = \"\"\nimport keras\nimport keras_nlp\nimport kagglehub\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\ntqdm.pandas() # progress bar for pandas\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, Markdown","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-19T11:44:33.725555Z","iopub.execute_input":"2024-09-19T11:44:33.725956Z","iopub.status.idle":"2024-09-19T11:44:47.315173Z","shell.execute_reply.started":"2024-09-19T11:44:33.72591Z","shell.execute_reply":"2024-09-19T11:44:47.314187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initialize user secrets\n\nWe initialize user secrets, so that we can publish the model using `kagglehub`.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nos.environ[\"KAGGLE_USERNAME\"] = user_secrets.get_secret(\"kaggle_username\")\nos.environ[\"KAGGLE_KEY\"] = user_secrets.get_secret(\"kaggle_key\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configurations\n\n\nWe use a `Config` class to group the information needed to control the fine-tuning process:\n* random seed \n* dataset path\n* preset - name of pretrained Gemma 2\n* sequence length - this is the maximum size of input sequence for training\n* batch size - size of the input batch in training, x 2 as two GPUs\n* lora rank - rank for LoRA, higher means more trainable parameters \n* learning rate used in the train\n* epochs - number of epochs for train","metadata":{}},{"cell_type":"code","source":"class Config:\n    seed = 42\n    dataset_path = \"/kaggle/input/roleplay-snapshot/roleplay.csv\"\n    preset = \"/kaggle/input/gemma2/keras/gemma2_2b_en/1\" # name of pretrained Gemma 2\n    sequence_length = 512 # max size of input sequence for training\n    batch_size = 1 # size of the input batch in training\n    lora_rank = 4 # rank for LoRA, higher means more trainable parameters\n    learning_rate=8e-5 # learning rate used in train\n    epochs = 15 # number of epochs to train","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:44:47.316664Z","iopub.execute_input":"2024-09-19T11:44:47.31762Z","iopub.status.idle":"2024-09-19T11:44:47.323127Z","shell.execute_reply.started":"2024-09-19T11:44:47.317572Z","shell.execute_reply":"2024-09-19T11:44:47.32209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set a random seed for results reproducibility.","metadata":{}},{"cell_type":"code","source":"keras.utils.set_random_seed(Config.seed)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:44:47.326084Z","iopub.execute_input":"2024-09-19T11:44:47.326477Z","iopub.status.idle":"2024-09-19T11:44:47.367384Z","shell.execute_reply.started":"2024-09-19T11:44:47.326435Z","shell.execute_reply":"2024-09-19T11:44:47.366514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the data\n\n\nWe load the data we will use for fine-tunining.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(f\"{Config.dataset_path}\", sep=\";\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:44:47.368563Z","iopub.execute_input":"2024-09-19T11:44:47.368974Z","iopub.status.idle":"2024-09-19T11:44:47.403754Z","shell.execute_reply.started":"2024-09-19T11:44:47.368941Z","shell.execute_reply":"2024-09-19T11:44:47.402707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the total number of rows in this dataset.","metadata":{}},{"cell_type":"code","source":"df.shape, df.columns","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:44:47.404841Z","iopub.execute_input":"2024-09-19T11:44:47.405136Z","iopub.status.idle":"2024-09-19T11:44:47.41105Z","shell.execute_reply.started":"2024-09-19T11:44:47.405105Z","shell.execute_reply":"2024-09-19T11:44:47.410122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess the data\n\nWe will preprocess the data so that, from the sequences in the `text` column, we extract the `<|system|>` prompt and the pairs of {`<|user|>`, `<|assistant|>`} to form triplets of {`<|system|>`, `<|user|>`, `<|assistant|>`}  for each entry in the data for fine-tuning.","metadata":{}},{"cell_type":"code","source":"import re\ndef extract_dialogue_components(text):\n    # Extract the system prompt\n    system_prompt = re.search(r\"<\\|system\\|>.*?</s>\", text, re.DOTALL).group(0)\n    \n    # Extract user and assistant dialogues\n    dialogue_pairs = re.findall(r\"(<\\|user\\|>.*?</s>\\s*<\\|assistant\\|>.*?</s>)\", text, re.DOTALL)\n    \n    return system_prompt, dialogue_pairs","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:44:47.412241Z","iopub.execute_input":"2024-09-19T11:44:47.412603Z","iopub.status.idle":"2024-09-19T11:44:47.420113Z","shell.execute_reply.started":"2024-09-19T11:44:47.412571Z","shell.execute_reply":"2024-09-19T11:44:47.419234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We process the data. We will only include in the data for fine-tuning the model the rows that fits in the max length as configured.","metadata":{}},{"cell_type":"code","source":"data = []\nfor row in df.iterrows():\n    text = row[1][\"text\"]\n    try:\n        system_prompt, dialogue_pairs = extract_dialogue_components(text)\n        for pair in dialogue_pairs:\n            prompt_sample = f\"{system_prompt}\\n\\n{pair}\"\n            data.append(prompt_sample)\n    except Exception as ex:\n        print(ex, row[0])","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:44:47.421481Z","iopub.execute_input":"2024-09-19T11:44:47.421862Z","iopub.status.idle":"2024-09-19T11:44:47.432878Z","shell.execute_reply.started":"2024-09-19T11:44:47.421821Z","shell.execute_reply":"2024-09-19T11:44:47.431955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Template utility function\n\n\nWe use this function to reformat the output of our queries, so that it is more user friendly.\n\nWe replace and highlight the initial special tokens with more human-readable text (Instruction, Question, Answer).","metadata":{}},{"cell_type":"code","source":"def colorize_text(text):\n    for word, formatted_word, color in zip([\"<|system|>:\", \"<|user|>:\", \"<|assistant|>:\"], \n                                           [\"Instruction:\", \"Question:\", \"Answer:\"],\n                                           [\"blue\", \"red\", \"green\"]):\n        text = text.replace(f\"\\n\\n{word}\", f\"\\n\\n**<font color='{color}'>{formatted_word}</font>**\")\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:44:47.455191Z","iopub.execute_input":"2024-09-19T11:44:47.455592Z","iopub.status.idle":"2024-09-19T11:44:47.467595Z","shell.execute_reply.started":"2024-09-19T11:44:47.45555Z","shell.execute_reply":"2024-09-19T11:44:47.466764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Specialized class to query Gemma\n\n\nWe define a specialized class to query Gemma. But first, we need to initialize an object of GemmaCausalLM class.","metadata":{}},{"cell_type":"markdown","source":"## Initialize the code for Gemma Causal LM","metadata":{}},{"cell_type":"code","source":"gemma_causal_lm = keras_nlp.models.GemmaCausalLM.from_preset(Config.preset)\ngemma_causal_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:44:47.468596Z","iopub.execute_input":"2024-09-19T11:44:47.468904Z","iopub.status.idle":"2024-09-19T11:45:48.520843Z","shell.execute_reply.started":"2024-09-19T11:44:47.468873Z","shell.execute_reply":"2024-09-19T11:45:48.519934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the specialized class\n\nHere we define the special class `GemmaQA`. \nin the `__init__` we pass the `GemmaCausalLM` object created before.\nThe `query` member function uses `GemmaCausalLM` member function `generate` to generate the answer, based on a prompt that includes the category and the question.","metadata":{}},{"cell_type":"code","source":"template = \"\\n\\n<|system|>:\\n{instruct}\\n\\n<|user|>:\\n{question}\\n\\n<|assistant|>:\\n{answer}\"\nclass GemmaQA:\n    def __init__(self, max_length=512):\n        self.max_length = max_length\n        self.prompt = template\n        self.gemma_causal_lm = gemma_causal_lm\n        \n    def query(self, instruct, question):\n        response = self.gemma_causal_lm.generate(\n            self.prompt.format(\n                instruct=instruct,\n                question=question,\n                answer=\"\"), \n            max_length=self.max_length)\n        display(Markdown(colorize_text(response)))\n        ","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:45:48.52295Z","iopub.execute_input":"2024-09-19T11:45:48.523258Z","iopub.status.idle":"2024-09-19T11:45:48.529359Z","shell.execute_reply.started":"2024-09-19T11:45:48.523225Z","shell.execute_reply":"2024-09-19T11:45:48.528397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gemma preprocessor\n\n\nThis preprocessing layer will take in batches of strings, and return outputs in a ```(x, y, sample_weight)``` format, where the y label is the next token id in the x sequence.\n\nFrom the code below, we can see that, after the preprocessor, the data shape is ```(num_samples, sequence_length)```.","metadata":{}},{"cell_type":"code","source":"x, y, sample_weight = gemma_causal_lm.preprocessor(data[0:2])","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:45:48.530614Z","iopub.execute_input":"2024-09-19T11:45:48.530983Z","iopub.status.idle":"2024-09-19T11:45:49.381245Z","shell.execute_reply.started":"2024-09-19T11:45:48.530941Z","shell.execute_reply":"2024-09-19T11:45:49.380459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x, y)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:45:49.382393Z","iopub.execute_input":"2024-09-19T11:45:49.3827Z","iopub.status.idle":"2024-09-19T11:45:49.451097Z","shell.execute_reply.started":"2024-09-19T11:45:49.382668Z","shell.execute_reply":"2024-09-19T11:45:49.450139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Perform fine-tuning with LoRA","metadata":{}},{"cell_type":"markdown","source":"## Enable LoRA for the model\n\nLoRA rank is setting the number of trainable parameters. A larger rank will result in a larger number of parameters to train.","metadata":{}},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to the lora_rank as set in Config (4).\ngemma_causal_lm.backbone.enable_lora(rank=Config.lora_rank)\ngemma_causal_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:45:49.452157Z","iopub.execute_input":"2024-09-19T11:45:49.452501Z","iopub.status.idle":"2024-09-19T11:45:49.751731Z","shell.execute_reply.started":"2024-09-19T11:45:49.452459Z","shell.execute_reply":"2024-09-19T11:45:49.750818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that only a small part of the parameters are trainable. 2.6 billions parameters total, and only 2.9 Millions parameters trainable.","metadata":{}},{"cell_type":"markdown","source":"## Run the training sequence\n\nWe set the `sequence_length` for the `GemmaCausalLM` (from configuration, will be 512).\nWe compile the model, with the loss, optimizer and metric.\nFor the metric, it is used `SparseCategoricalAccuracy`. This metric calculates how often predictions match integer labels.","metadata":{}},{"cell_type":"code","source":"#set sequence length cf. config (896)\ngemma_causal_lm.preprocessor.sequence_length = Config.sequence_length \n\n# Compile the model with loss, optimizer, and metric\ngemma_causal_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.Adam(learning_rate=Config.learning_rate),\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\n# Train model\ngemma_causal_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T11:45:49.753003Z","iopub.execute_input":"2024-09-19T11:45:49.754077Z","iopub.status.idle":"2024-09-19T11:46:42.454702Z","shell.execute_reply.started":"2024-09-19T11:45:49.754024Z","shell.execute_reply":"2024-09-19T11:46:42.453347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We obtained a rather good accuracy after the 15 steps of fine-tuning.","metadata":{}},{"cell_type":"markdown","source":"# Test the fine-tuned model\n\nWe instantiate an object of class GemmaQA. Because `gemma_causal_lm` was fine-tuned using LoRA, `gemma_qa` defined here will use the fine-tuned model.","metadata":{}},{"cell_type":"code","source":"gemma_qa = GemmaQA()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T10:02:46.936328Z","iopub.execute_input":"2024-09-19T10:02:46.937217Z","iopub.status.idle":"2024-09-19T10:02:46.941227Z","shell.execute_reply.started":"2024-09-19T10:02:46.937185Z","shell.execute_reply":"2024-09-19T10:02:46.940172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For start, we are testing the model with some of the data from the training set itself.","metadata":{}},{"cell_type":"markdown","source":"## Sample 1","metadata":{}},{"cell_type":"code","source":"gemma_qa = GemmaQA(max_length=96)\ninstruct = \"Sherlock the renowned detective from Baker Street is known for his astute logical reasoning disguise ability and use of forensic science to solve perplexing crimes\"\nquestion = \"What's Sherlock secret to solving crimes?\"\ngemma_qa.query(instruct, question)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T10:27:56.846614Z","iopub.execute_input":"2024-09-19T10:27:56.846983Z","iopub.status.idle":"2024-09-19T10:27:58.462669Z","shell.execute_reply.started":"2024-09-19T10:27:56.846956Z","shell.execute_reply":"2024-09-19T10:27:58.461478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Not seen question(s)","metadata":{}},{"cell_type":"code","source":"gemma_qa = GemmaQA(max_length=128)\ninstruct = \"Sherlock the renowned detective from Baker Street is known for his astute logical reasoning disguise ability and use of forensic science to solve perplexing crimes\"\nquestion = \"How is able Sherlock to be so succesfull in solving crimes?\"\ngemma_qa.query(instruct, question)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T10:18:46.571924Z","iopub.execute_input":"2024-09-19T10:18:46.572637Z","iopub.status.idle":"2024-09-19T10:18:49.228106Z","shell.execute_reply.started":"2024-09-19T10:18:46.572597Z","shell.execute_reply":"2024-09-19T10:18:49.227225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"instruct = df.description.values[1]\nquestion = \"How is Serena Williams play against Simona Halep?\"\n\ngemma_qa.query(instruct,question)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T10:28:18.799297Z","iopub.execute_input":"2024-09-19T10:28:18.799994Z","iopub.status.idle":"2024-09-19T10:28:20.314088Z","shell.execute_reply.started":"2024-09-19T10:28:18.799964Z","shell.execute_reply":"2024-09-19T10:28:20.313165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gemma_qa = GemmaQA(max_length=256)\ninstruct = df.description.values[2]\nquestion = \"How will Beethoven solve the equation system x + 1 = 2?\"\n\ngemma_qa.query(instruct,question)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T10:28:29.239143Z","iopub.execute_input":"2024-09-19T10:28:29.240116Z","iopub.status.idle":"2024-09-19T10:28:32.708429Z","shell.execute_reply.started":"2024-09-19T10:28:29.240081Z","shell.execute_reply":"2024-09-19T10:28:32.707448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gemma_qa = GemmaQA(max_length=90)\ninstruct = df.description.values[10]\nquestion = \"How will Michael Jordan knitt a sweater?\"\n\ngemma_qa.query(instruct,question)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T10:28:38.734959Z","iopub.execute_input":"2024-09-19T10:28:38.735688Z","iopub.status.idle":"2024-09-19T10:28:40.125974Z","shell.execute_reply.started":"2024-09-19T10:28:38.735654Z","shell.execute_reply":"2024-09-19T10:28:40.124987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save the model","metadata":{}},{"cell_type":"code","source":"preset_dir = \".\\gemma2_2b_en_roleplay\"\ngemma_causal_lm.save_to_preset(preset_dir)","metadata":{"execution":{"iopub.status.busy":"2024-08-22T08:00:56.381358Z","iopub.status.idle":"2024-08-22T08:00:56.381681Z","shell.execute_reply.started":"2024-08-22T08:00:56.381521Z","shell.execute_reply":"2024-08-22T08:00:56.381535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Publish Model on Kaggle as a Kaggle Model\n\nWe are publishing now the saved model as a Kaggle Model.","metadata":{}},{"cell_type":"code","source":"kaggle_username = os.environ[\"KAGGLE_USERNAME\"]\n\nkaggle_uri = f\"kaggle://{kaggle_username}/gemma2_2b_en_roleplay/keras/gemma2_2b_en_roleplay\"\nkeras_nlp.upload_preset(kaggle_uri, preset_dir)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions\n\n","metadata":{}},{"cell_type":"markdown","source":"We demonstated how to fine-tune a **Gemma 2** model using LoRA.   \nWe also created a class to run queries to the **Gemma 2** model and tested it with some examples from the existing training data but also with some new, not seen questions.   ","metadata":{}}]}